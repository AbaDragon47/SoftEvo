{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# research\n",
    "# https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "\n",
    "# implementation\n",
    "# https://www.kaggle.com/code/auxeno/ddpg-rl\n",
    "\n",
    "### This was made in google colab, so matplotlib might not work on this\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive_output, FloatSlider, HBox, VBox\n",
    "import ipywidgets as widgets\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic actor net\n",
    "# state => action\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), # arbitrary middle layers\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh() # normalize to (-1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic net evaluates potential of actions (the return of each action) taken by actor net. Used to assess action quality\n",
    "# (state, action) => return\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim), # takes (state, action) tuple\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1) # collapse to scalar return value\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU noise generator class, used to add OU noise to actions\n",
    "class OUNoise:\n",
    "    def __init__(self, size, mu=0, sigma=0.1, theta=0.15):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    # Reset noise to mean\n",
    "    def reset(self):\n",
    "        self.state = self.mu.copy()\n",
    "\n",
    "    # Returns next generated value\n",
    "    def sample(self):\n",
    "        dx = (self.theta * (self.mu - self.state)) + (self.sigma * np.random.randn(self.size))\n",
    "        self.state += dx\n",
    "        return self.state.copy() # Return copy to prevent outside class edits from changing behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Ignore (plotting noise)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "def plot_ou_noise(mu, sigma, theta):\n",
    "    ou_noise = OUNoise(size=1, mu=mu, sigma=sigma, theta=theta)\n",
    "    data = [ou_noise.sample()[0] for _ in range(1000)]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(data, lw=2, c='#636EFA')\n",
    "    plt.title('Ornstein-Uhlenbeck Noise')\n",
    "    plt.xlabel('Time step')\n",
    "    plt.ylabel('Noise value')\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Sliders\n",
    "mu_slider = FloatSlider(value=0, min=-1, max=1, step=0.1, description='Mu:')\n",
    "sigma_slider = FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01, description='Sigma:')\n",
    "theta_slider = FloatSlider(value=0.15, min=0.001, max=0.25, step=0.001, description='Theta:')\n",
    "\n",
    "# Interactive plot\n",
    "out = interactive_output(plot_ou_noise, {'mu': mu_slider, 'sigma': sigma_slider, 'theta': theta_slider})\n",
    "ui = HBox([mu_slider, sigma_slider, theta_slider])\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer of experience tuples D (see artcle)\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, num_steps=1, gamma=0.99):\n",
    "        self.buffer = deque(maxlen=capacity) # Deque buffer for effecient popping, pushing, and iterators from both sides\n",
    "        self.num_steps = num_steps\n",
    "        self.gamma = gamma\n",
    "        self.n_step_buffer = deque(maxlen=num_steps)\n",
    "\n",
    "    # Add transition tuple of type D to buffer and handle n_step logic (if needed)\n",
    "    def add(self, transition):\n",
    "        # transition = (s, a, r, s', terminal_state, ?)\n",
    "        assert len(transition) == 6\n",
    "\n",
    "        if self.num_steps == 1:\n",
    "            state, action, reward, next_state, terminated, _ = transition\n",
    "            self.buffer.append((state, action, reward, next_state, terminated)) # Append without last element of tuple to conform to D\n",
    "        else:\n",
    "            self.n_step_buffer.append(transition)\n",
    "\n",
    "            # Calculate n_step reward\n",
    "            _, _, _, final_state, final_termination, final_truncation = transition\n",
    "            n_step_reward = 0\n",
    "\n",
    "            for _, _, reward, _, _, _ in reversed(self.n_step_buffer):\n",
    "                n_step_reward = (n_step_reward * self.gamma) + reward\n",
    "            state, action, _, _, _, _ = self.n_step_buffer[0]\n",
    "\n",
    "            # If n-step buffer is full, append to main buffer\n",
    "            if len(self.n_step_buffer) == self.num_steps:\n",
    "                self.buffer.append((state, action, n_step_reward, final_state, final_termination))\n",
    "\n",
    "            # If the state is terminal, clear the n-step buffer\n",
    "            if final_termination or final_truncation:\n",
    "                self.n_step_buffer.clear()\n",
    "\n",
    "    # Get sample batch of exerpiences for learner to learn from\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "class DDPG:\n",
    "    def __init__(self, config):\n",
    "        self.device = config['device']\n",
    "        self.env = gym.make(config['env_name'])\n",
    "        state_dim = np.prod(self.env.observation_space.shape)\n",
    "        action_dim = np.prod(self.env.action_space.shape)\n",
    "        self.online_actor = Actor(state_dim, action_dim, config['hidden_dim']).to(self.device)\n",
    "        self.target_actor = Actor(state_dim, action_dim, config['hidden_dim']).to(self.device)\n",
    "        self.online_critic = Critic(state_dim, action_dim, config['hidden_dim']).to(self.device)\n",
    "        self.target_critic = Critic(state_dim, action_dim, config['hidden_dim']).to(self.device)\n",
    "        self.soft_update(self.online_actor, self.target_actor, 1.)\n",
    "        self.soft_update(self.online_critic, self.target_critic, 1.)\n",
    "        self.optimizer_actor = torch.optim.Adam(self.online_actor.parameters(), lr=config['lr'])\n",
    "        self.optimizer_critic = torch.optim.Adam(self.online_critic.parameters(), lr=config['lr'])\n",
    "        self.buffer = ReplayBuffer(config['buffer_capacity'], config['num_steps'], config['gamma'])\n",
    "        self.noise_generator = OUNoise(size=action_dim, mu=config['ou_mu'], \n",
    "                                                      sigma=config['ou_sigma'], theta=config['ou_theta'])\n",
    "        self.config = config\n",
    "\n",
    "    # DDPG action selection\n",
    "    def select_action(self, state, noise=None):\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            action = self.online_actor(state_tensor).squeeze(0).detach().cpu().numpy()\n",
    "            if noise:\n",
    "                return np.clip(action + noise, a_min=-1, a_max=1)\n",
    "            return action\n",
    "\n",
    "    # Copies the parameters from an online to target network, tau controls how fully the weights are copied.\n",
    "    def soft_update(self, online, target, tau):\n",
    "        for online_param, target_param in zip(online.parameters(), target.parameters()):\n",
    "            target_param.data.copy_(tau * online_param.data + (1. - tau) * target_param.data)\n",
    "\n",
    "    # Perform a single learning step\n",
    "    def learn(self):\n",
    "        # Sample and preprocess experience data\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.config['batch_size'])\n",
    "        states      = torch.tensor(np.array(states),      dtype=torch.float32, device=self.device)\n",
    "        actions     = torch.tensor(np.array(actions),     dtype=torch.float32, device=self.device)\n",
    "        rewards     = torch.tensor(np.array(rewards),     dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=self.device)\n",
    "        dones       = torch.tensor(np.array(dones),       dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "\n",
    "        # Critic loss\n",
    "        current_action_q = self.online_critic(torch.cat((states, actions), dim=1))\n",
    "        with torch.no_grad():\n",
    "            next_state_q = self.target_critic(torch.cat((next_states, self.online_actor(next_states)), dim=1))\n",
    "            target_q = rewards + self.config['gamma'] ** self.config['num_steps'] * (1. - dones) * next_state_q    \n",
    "        critic_loss = F.mse_loss(current_action_q, target_q)\n",
    "\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.online_critic.parameters(), self.config['grad_norm_clip'])\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        # Actor loss\n",
    "        current_action_q = self.online_critic(torch.cat((states, self.online_actor(states)), dim=1))\n",
    "        actor_loss = -(current_action_q).mean()\n",
    "\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.online_actor.parameters(), self.config['grad_norm_clip'])\n",
    "        self.optimizer_actor.step()\n",
    "\n",
    "    # Trains agent for a given number of steps according to given configuration\n",
    "    def train(self):\n",
    "        if self.config['verbose']: print(\"Training agent\\n\")\n",
    "        \n",
    "        # Logging information\n",
    "        logs = {'episode_count': 0, 'episodic_reward': 0., 'episode_rewards': [], 'start_time': time.time()}\n",
    "        \n",
    "        # Reset episode\n",
    "        state, _ = self.env.reset()\n",
    "        \n",
    "        # Main training loop\n",
    "        for step in range(1, self.config['total_steps'] + 1):\n",
    "            # Get action and step in envrionment\n",
    "            noise = self.noise_generator.sample()\n",
    "            action = self.config['action_scale'] * self.select_action(state, noise)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            \n",
    "            # Update logs\n",
    "            logs['episodic_reward'] += reward\n",
    "            \n",
    "            # Push experience to buffer\n",
    "            self.buffer.add((state, action, reward, next_state, terminated, truncated))\n",
    "\n",
    "            # Reset environment and noise\n",
    "            if terminated or truncated:\n",
    "                state, _ = self.env.reset()\n",
    "                self.noise_generator.reset()\n",
    "                \n",
    "                # Update logs\n",
    "                logs['episode_count'] += 1\n",
    "                logs['episode_rewards'].append(logs['episodic_reward'])\n",
    "                logs['episodic_reward'] = 0.\n",
    "            else:\n",
    "                state = next_state\n",
    "            \n",
    "            # Perform learning step\n",
    "            if len(self.buffer) > self.config['batch_size'] and step >= self.config['learning_starts']:\n",
    "                self.learn()\n",
    "            \n",
    "            # Update target networks\n",
    "            self.soft_update(self.online_critic, self.target_critic, self.config['tau'])\n",
    "            self.soft_update(self.online_actor, self.target_actor, self.config['tau'])\n",
    "                \n",
    "            # If mean of last 20 rewards exceed target, end training\n",
    "            if len(logs['episode_rewards']) > 0 and np.mean(logs['episode_rewards'][-20:]) >= self.config['target_reward']:\n",
    "                break\n",
    "            \n",
    "            # Print training info if verbose\n",
    "            if self.config['verbose'] and step % 100 == 0 and len(logs['episode_rewards']) > 0:\n",
    "                print(f\"\\r--- {100 * step / self.config['total_steps']:.1f}%\" \n",
    "                      f\"\\t Step: {step:,}\"\n",
    "                      f\"\\t Mean Reward: {np.mean(logs['episode_rewards'][-20:]):.2f}\"\n",
    "                      f\"\\t Episode: {logs['episode_count']:,}\"\n",
    "                      f\"\\t Duration: {time.time() - logs['start_time']:,.1f}s  ---\", end='')\n",
    "                if step % 1000 == 0:\n",
    "                    print()\n",
    "                    \n",
    "        # Training ended\n",
    "        if self.config['verbose']: print(\"\\n\\nTraining done\")\n",
    "        logs['end_time'] = time.time()\n",
    "        logs['duration'] = logs['end_time'] - logs['start_time']\n",
    "        return logs\n",
    "\n",
    "\n",
    "### DDPG Config ###\n",
    "ddpg_config = {\n",
    "    'env_name'       : 'Pendulum-v1',  # Environment name\n",
    "    'device'         :   'cpu',  # Device DQN runs on\n",
    "    'total_steps'    :  100000,  # Total training steps\n",
    "    'target_reward'  :    -200,  # Target reward to stop training at when reached\n",
    "    'action_scale' :        2.,  # Gym pendulum's action range is from -2 to +2 (Why OpenAI?)\n",
    "    'gamma'          :    0.99,  # Discount Factor \n",
    "    'lr'             :    1e-4,  # Learning rate\n",
    "    'hidden_dim'     :     256,  # Number of neurons in hidden layers\n",
    "    'batch_size'     :      64,  # Batch size used by learner\n",
    "    'buffer_capacity':  100000,  # Maximum replay buffer capacity\n",
    "    'tau'            :   0.001,  # Soft target network update interpolation coefficient\n",
    "    'ou_mu'          :      0.,  # OU noise mean\n",
    "    'ou_sigma'       :     0.2,  # OU noise sdev\n",
    "    'ou_theta'       :    0.15,  # OU noise reversion rate\n",
    "    'learning_starts':     512,  # Begin learning after performing this many steps\n",
    "    'num_steps'      :       3,  # Number of steps to unroll Bellman equation by\n",
    "    'grad_norm_clip' :      40,  # Global gradient clipping value\n",
    "    'verbose'        :    True,  # Verbose printing\n",
    "}\n",
    "\n",
    "\n",
    "### Train Agent ###\n",
    "ddpg = DDPG(ddpg_config)\n",
    "logs = ddpg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(logs, window=5):\n",
    "    rewards = logs['episode_rewards']\n",
    "    moving_avg_rewards = [np.mean(rewards[max(0, i-window):i+1]) for i in range(len(rewards))]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards, label='Reward per Episode', c='#636EFA')\n",
    "    plt.plot(moving_avg_rewards, label=f'{window}-Episode Moving Average', color='#636EFA', ls='--', alpha=0.5)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Episodic Reward')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(logs, window=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
