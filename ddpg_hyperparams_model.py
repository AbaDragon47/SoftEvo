# -*- coding: utf-8 -*-
"""DDPG Hyperparams Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oEZ_9346IzBu0idlH0Ib21RYz5RQc2Yt
"""

# research
# https://spinningup.openai.com/en/latest/algorithms/ddpg.html

# implementation
# https://www.kaggle.com/code/auxeno/ddpg-rl

from collections import deque
# from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F



# Deterministic actor net
# state => action
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(Actor, self).__init__()
        hidden_dim = int(hidden_dim)
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),  # arbitrary middle layers
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()  # normalize to (-1, 1)
        )

    def forward(self, x):
        return self.network(x)


# Critic net evaluates potential of actions (the return of each action) taken by actor net. Used to assess action quality
# (state, action) => return
class Critic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(Critic, self).__init__()
        hidden_dim = int(hidden_dim)
        self.network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),  # takes (state, action) tuple
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)  # collapse to scalar return value
        )

    def forward(self, x):
        return self.network(x)


# OU noise generator class, used to add OU noise to actions
class OUNoise:
    def __init__(self, size, mu=0, sigma=0.1, theta=0.15):
        self.mu = mu * np.ones(size)
        self.sigma = sigma
        self.theta = theta
        self.size = size
        self.reset()

    # Reset noise to mean
    def reset(self):
        self.state = self.mu.copy()

    # Returns next generated value
    def sample(self):
        dx = (self.theta * (self.mu - self.state)) + (self.sigma * np.random.randn(self.size))
        self.state += dx
        return self.state.copy()  # Return copy to prevent outside class edits from changing behavior


# !!! Ignore (plotting noise)
plt.style.use('fivethirtyeight')


def plot_ou_noise(mu, sigma, theta):
    ou_noise = OUNoise(size=1, mu=mu, sigma=sigma, theta=theta)
    data = [ou_noise.sample()[0] for _ in range(1000)]

    plt.figure(figsize=(10, 5))
    plt.plot(data, lw=2, c='#636EFA')
    plt.title('Ornstein-Uhlenbeck Noise')
    plt.xlabel('Time step')
    plt.ylabel('Noise value')
    plt.ylim(-2, 2)
    plt.grid(True)
    plt.show()


# # Sliders
# mu_slider = FloatSlider(value=0, min=-1, max=1, step=0.1, description='Mu:')
# sigma_slider = FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01, description='Sigma:')
# theta_slider = FloatSlider(value=0.15, min=0.001, max=0.25, step=0.001, description='Theta:')


# Interactive plot
# out = interactive_output(plot_ou_noise, {'mu': mu_slider, 'sigma': sigma_slider, 'theta': theta_slider})
# ui = HBox([mu_slider, sigma_slider, theta_slider])
# display(ui, out)

# Buffer of experience tuples D (see artcle)
class ReplayBuffer:
    def __init__(self, capacity, num_steps=1, gamma=0.99):
        capacity = int(capacity)
        num_steps = int(num_steps)

        self.buffer = deque(
            maxlen=capacity)  # Deque buffer for effecient popping, pushing, and iterators from both sides
        self.num_steps = num_steps
        self.gamma = gamma
        self.n_step_buffer = deque(maxlen=num_steps)

    # Add transition tuple of type D to buffer and handle n_step logic (if needed)
    def add(self, transition):
        # transition = (s, a, r, s', terminal_state, ?)
        assert len(transition) == 6

        if self.num_steps == 1:
            state, action, reward, next_state, terminated, _ = transition
            self.buffer.append(
                (state, action, reward, next_state, terminated))  # Append without last element of tuple to conform to D
        else:
            self.n_step_buffer.append(transition)

            # Calculate n_step reward
            _, _, _, final_state, final_termination, final_truncation = transition
            n_step_reward = 0

            for _, _, reward, _, _, _ in reversed(self.n_step_buffer):
                n_step_reward = (n_step_reward * self.gamma) + reward
            state, action, _, _, _, _ = self.n_step_buffer[0]

            # If n-step buffer is full, append to main buffer
            if len(self.n_step_buffer) == self.num_steps:
                self.buffer.append((state, action, n_step_reward, final_state, final_termination))

            # If the state is terminal, clear the n-step buffer
            if final_termination or final_truncation:
                self.n_step_buffer.clear()

    # Get sample batch of exerpiences for learner to learn from
    def sample(self, batch_size):
        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))
        return states, actions, rewards, next_states, dones

    def __len__(self):
        return len(self.buffer)


# Commented out IPython magic to ensure Python compatibility.
# %pip install gymnasium
import numpy as np
import torch
import gymnasium as gym
import time
import math
import sys
import numpy as np
from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.exception import UnityWorkerInUseException
from mlagents_envs.base_env import ActionTuple

def init_unity_env(env_path, show_visuals=True):
    worker_id = 0
    while worker_id <= 64:
        try:
            env = UnityEnvironment(file_name=env_path, worker_id=worker_id, no_graphics=not show_visuals)
            env.reset()
            return env
        except UnityWorkerInUseException:
            worker_id += 1
    sys.exit("All worker IDs are in use. Please ensure no other Unity environments are running.")

env = init_unity_env('/home/ani/SoftEvo/Build/main.x86_64')
behavior_name = list(env.behavior_specs.keys())[0]
spec = env.behavior_specs[behavior_name]

class DDPG:
    def __init__(self, config):
        self.device = config['device']
        # self.env = init_unity_env(config['env_name'])
        #
        # behavior_name = list(self.env.behavior_specs.keys())[0]
        # spec = self.env.behavior_specs[behavior_name]

        # state_dim = np.prod(self.env.observation_space.shape)
        # action_dim = np.prod(self.env.action_space.shape)

        self.env = env

        state_dim = spec.observation_specs[0].shape[0]
        action_dim = spec.action_spec.continuous_size

        self.online_actor = Actor(state_dim, action_dim, config['hidden_dim']).to(self.device)
        self.target_actor = Actor(state_dim, action_dim, config['hidden_dim']).to(self.device)
        self.online_critic = Critic(state_dim, action_dim, config['hidden_dim']).to(self.device)
        self.target_critic = Critic(state_dim, action_dim, config['hidden_dim']).to(self.device)
        self.soft_update(self.online_actor, self.target_actor, 1.)
        self.soft_update(self.online_critic, self.target_critic, 1.)
        self.optimizer_actor = torch.optim.Adam(self.online_actor.parameters(), lr=config['lr'])
        self.optimizer_critic = torch.optim.Adam(self.online_critic.parameters(), lr=config['lr'])
        self.buffer = ReplayBuffer(config['buffer_capacity'], config['num_steps'], config['gamma'])
        self.noise_generator = OUNoise(size=action_dim, mu=config['ou_mu'],
                                       sigma=config['ou_sigma'], theta=config['ou_theta'])
        self.config = config

    # DDPG action selection
    def select_action(self, state, noise=None):
        with torch.no_grad():
            state_tensor = torch.tensor(state, device=self.device).unsqueeze(0)
            action = self.online_actor(state_tensor).squeeze(0).detach().cpu().numpy()
            if noise is not None:
                return np.clip(action + noise, a_min=-1, a_max=1)
            return action

    # Copies the parameters from an online to target network, tau controls how fully the weights are copied.
    def soft_update(self, online, target, tau):
        for online_param, target_param in zip(online.parameters(), target.parameters()):
            target_param.data.copy_(tau * online_param.data + (1. - tau) * target_param.data)

    # Perform a single learning step
    def learn(self):
        # Sample and preprocess experience data
        states, actions, rewards, next_states, dones = self.buffer.sample(self.config['batch_size'])
        states = torch.tensor(np.array(states), dtype=torch.float32, device=self.device)
        actions = torch.tensor(np.array(actions), dtype=torch.float32, device=self.device)
        rewards = torch.tensor(np.array(rewards), dtype=torch.float32, device=self.device).unsqueeze(1)
        next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=self.device)
        dones = torch.tensor(np.array(dones), dtype=torch.float32, device=self.device).unsqueeze(1)

        # Critic loss
        current_action_q = self.online_critic(torch.cat((states, actions), dim=1))
        with torch.no_grad():
            next_state_q = self.target_critic(torch.cat((next_states, self.online_actor(next_states)), dim=1))
            target_q = rewards + self.config['gamma'] ** self.config['num_steps'] * (1. - dones) * next_state_q
        critic_loss = F.mse_loss(current_action_q, target_q)

        self.optimizer_critic.zero_grad()
        critic_loss.backward()
        nn.utils.clip_grad_norm_(self.online_critic.parameters(), self.config['grad_norm_clip'])
        self.optimizer_critic.step()

        # Actor loss
        current_action_q = self.online_critic(torch.cat((states, self.online_actor(states)), dim=1))
        actor_loss = -(current_action_q).mean()

        self.optimizer_actor.zero_grad()
        actor_loss.backward()
        nn.utils.clip_grad_norm_(self.online_actor.parameters(), self.config['grad_norm_clip'])
        self.optimizer_actor.step()

    # Trains agent for a given number of steps according to given configuration
    def train(self):
        if self.config['verbose']: print("Training agent\n")

        # Logging information
        logs = {'episode_count': 0, 'episodic_reward': 0.0, 'episode_rewards': [], 'start_time': time.time()}

        self.env.reset()
        def get_state():
            behavior_name = list(self.env.behavior_specs.keys())[0]
            decision, terminal = self.env.get_steps(behavior_name)

            return decision.obs[0][0]

        state = get_state()

        # Main training loop
        for step in range(1, int(self.config['total_steps']) + 1):
            # Get action and step in envrionment
            noise = self.noise_generator.sample()
            action = self.config['action_scale'] * self.select_action(state, noise)
            action_tuple = ActionTuple(continuous=np.array([action]))

            self.env.set_actions(behavior_name, action_tuple)
            self.env.step()

            decision, terminal = self.env.get_steps(behavior_name)

            next_state, reward, terminated, truncated, _ = None, None, None, None, None

            if len(terminal) > 0:  # Check if any agents are in a terminal state
                next_state = terminal.obs[0][0]  # Final state if terminated
                reward = terminal.reward[0]
                terminated = True
            else:
                next_state = decision.obs[0][0]  # Next state if not terminated
                reward = decision.reward[0]
                terminated = False
            truncated = terminated

            # Update logs
            logs['episodic_reward'] += reward
            if math.isnan(logs['episodic_reward']):

                exit(1)

            # Push experience to buffer
            self.buffer.add((state, action, reward, next_state, terminated, truncated))

            # Reset environment and noise
            if terminated or truncated:
                self.env.reset()
                state = get_state()
                self.noise_generator.reset()

                # Update logs
                logs['episode_count'] += 1
                logs['episode_rewards'].append(logs['episodic_reward'])
                asdlfjaslfkjasflksfsld = 'episode_rewards'
                print(
                    f'Episode rewards: {f"nan -> {logs[asdlfjaslfkjasflksfsld]}" if math.isnan(logs[asdlfjaslfkjasflksfsld][-1]) else logs[asdlfjaslfkjasflksfsld][-1]}')  # logs['episode_rewards'][-1]
                logs['episodic_reward'] = 0.0
            else:
                state = next_state

            # Perform learning step
            if len(self.buffer) > self.config['batch_size'] and step >= self.config['learning_starts']:
                self.learn()

            # Update target networks
            self.soft_update(self.online_critic, self.target_critic, self.config['tau'])
            self.soft_update(self.online_actor, self.target_actor, self.config['tau'])

            # If mean of last 20 rewards exceed target, end training
            if len(logs['episode_rewards']) > 0 and np.nanmean(logs['episode_rewards'][-20:]) >= self.config[
                'target_reward']:
                break

            # if math.isnan(np.mean(logs['episode_rewards'][-20:])):
            #     print(logs['episode_rewards'])print(logs['episode_rewards'])
            #     print(np.mean(logs['episode_rewards'][-20:]))
            #     breakpoint()

            # print(len(logs['episode_rewards']))

            # print(logs['episode_rewards'][-20:])
            # print(np.mean(logs['episode_rewards'][-20:]))
            # Print training info if verbose
            if self.config['verbose'] and step % 100 == 0 and len(logs['episode_rewards']) > 0:
                print(f"\r--- {100 * step / self.config['total_steps']:.1f}%"
                      f"\t Step: {step:,}"
                      f"\t Mean Reward: {np.nanmean(logs['episode_rewards'][-20:])}"
                      f"\t Episode: {logs['episode_count']:,}"
                      f"\t Duration: {time.time() - logs['start_time']:,.1f}s  ---", end='\n')
                if step % 1000 == 0:
                    print()

        # Training ended
        if self.config['verbose']: print("\n\nTraining done")
        logs['end_time'] = time.time()
        logs['duration'] = logs['end_time'] - logs['start_time']
        return logs

    # def train(self):
    #     if self.config['verbose']:
    #         print("Training agent\n")
    #
    #     logs = {'episode_count': 0, 'episodic_reward': 0.0, 'episode_rewards': [], 'start_time': time.time()}
    #
    #     # Unity-specific initialization of the environment
    #     env.reset()  # Be sure this resets your Unity environment
    #     behavior_name = list(env.behavior_specs.keys())[0]
    #
    #     for step in range(1, self.config['total_steps'] + 1):
    #         # Sample noise and get action
    #         noise = self.noise_generator.sample()
    #         action = self.config['action_scale'] * self.select_action(state, noise)
    #
    #         # Send action to Unity ML-Agents environment
    #         env.set_actions(behavior_name, ActionTuple(continuous=np.array([action])))
    #         env.step()
    #
    #         # Get the next observations and rewards
    #         decision_steps, terminal_steps = env.get_steps(behavior_name)
    #         if len(terminal_steps.agent_id) > 0:  # Check if the episode has ended
    #             next_state = terminal_steps.obs[0][0]
    #             reward = terminal_steps.reward[0]
    #             terminated = True
    #         else:
    #             next_state = decision_steps.obs[0][0]
    #             reward = decision_steps.reward[0]
    #             terminated = False
    #
    #         logs['episodic_reward'] += reward
    #         self.buffer.add((state, action, reward, next_state, terminated, False))
    #
    #         if terminated:
    #             state = env.reset()  # Reset the environment after termination
    #             self.noise_generator.reset()
    #
    #             logs['episode_count'] += 1
    #             logs['episode_rewards'].append(logs['episodic_reward'])
    #             logs['episodic_reward'] = 0.0
    #         else:
    #             state = next_state
    #
    #         # Learning step if conditions are met
    #         if len(self.buffer) > self.config['batch_size'] and step >= self.config['learning_starts']:
    #             self.learn()
    #
    #         # Update target networks
    #         self.soft_update(self.online_critic, self.target_critic, self.config['tau'])
    #         self.soft_update(self.online_actor, self.target_actor, self.config['tau'])
    #
    #         # Check for early stopping if target reward is reached
    #         if np.nanmean(logs['episode_rewards'][-20:]) >= self.config['target_reward']:
    #             break
    #
    #         # Print training info
    #         if self.config['verbose'] and step % 100 == 0:
    #             print(f"\r--- {100 * step / self.config['total_steps']:.1f}%"
    #                   f"\t Step: {step:,}"
    #                   f"\t Mean Reward: {np.nanmean(logs['episode_rewards'][-20:])}"
    #                   f"\t Episode: {logs['episode_count']:,}"
    #                   f"\t Duration: {time.time() - logs['start_time']:,.1f}s  ---", end='\n')
    #
    #     logs['end_time'] = time.time()
    #     logs['duration'] = logs['end_time'] - logs['start_time']
    #     return logs


# ### DDPG Config ###
# ddpg_config = {
#     'env_name'       : 'Pendulum-v1',  # Environment name
#     'device'         :   'cpu',  # Device DQN runs on
#     'total_steps'    :  100000,  # Total training steps
#     'target_reward'  :    -200,  # Target reward to stop training at when reached
#     'action_scale' :        2.,  # Gym pendulum's action range is from -2 to +2 (Why OpenAI?)
#     'gamma'          :    0.99,  # Discount Factor
#     'lr'             :    1e-4,  # Learning rate
#     'hidden_dim'     :     256,  # Number of neurons in hidden layers
#     'batch_size'     :      64,  # Batch size used by learner
#     'buffer_capacity':  100000,  # Maximum replay buffer capacity
#     'tau'            :   0.001,  # Soft target network update interpolation coefficient
#     'ou_mu'          :      0.,  # OU noise mean
#     'ou_sigma'       :     0.2,  # OU noise sdev
#     'ou_theta'       :    0.15,  # OU noise reversion rate
#     'learning_starts':     512,  # Begin learning after performing this many steps
#     'num_steps'      :       3,  # Number of steps to unroll Bellman equation by
#     'grad_norm_clip' :      40,  # Global gradient clipping value
#     'verbose'        :    True,  # Verbose printing
# }


# ### Train Agent ###
# ddpg = DDPG(ddpg_config)
# logs = ddpg.train()

def plot_rewards(logs, window=5):
    rewards = logs['episode_rewards']
    moving_avg_rewards = [np.mean(rewards[max(0, i - window):i + 1]) for i in range(len(rewards))]

    plt.figure(figsize=(10, 5))
    plt.plot(rewards, label='Reward per Episode', c='#636EFA')
    plt.plot(moving_avg_rewards, label=f'{window}-Episode Moving Average', color='#636EFA', ls='--', alpha=0.5)
    plt.xlabel('Episodes')
    plt.ylabel('Reward')
    plt.title('Episodic Reward')
    plt.legend()
    plt.grid(True)
    plt.show()


# print(logs)
# plot_rewards(logs, window=5)

population_size = 10
mutation_rate = 0.01
num_generations = 5

optimized_params = 'total_steps gamma lr hidden_dim batch_size buffer_capacity tau ou_mu ou_sigma ou_theta learning_starts num_steps grad_norm_clip'.split(
    ' ')


def generate_params_from_noise_value(ou_noise_millions, ou_noise_thousands, ou_noise_hundreds, ou_noise_tens,
                                     ou_noise_digit, ou_noise_decimal):
    d = {
        'env_name': '/home/ani/SoftEvo/Build/main.x86_64',  # Environment name
        'device': 'cpu',  # Device DQN runs on
        'total_steps': round(ou_noise_millions.sample()[0]),  # Total training steps
        'target_reward': -200,  # Target reward to stop training at when reached
        'action_scale': 2.,  # Gym pendulum's action range is from -2 to +2 (Why OpenAI?)
        'gamma': abs(ou_noise_decimal.sample()[0]),  # Discount Factor
        'lr': int(abs(ou_noise_decimal.sample()[0])),  # Learning rate
        'hidden_dim': int(round(ou_noise_hundreds.sample()[0])),  # Number of neurons in hidden layers
        'batch_size': int(round(ou_noise_tens.sample()[0])),  # Batch size used by learner
        'buffer_capacity': int(round(ou_noise_millions.sample()[0])),  # Maximum replay buffer capacity
        'tau': ou_noise_decimal.sample()[0],  # Soft target network update interpolation coefficient
        'ou_mu': ou_noise_decimal.sample()[0],  # OU noise mean
        'ou_sigma': ou_noise_decimal.sample()[0],  # OU noise sdev
        'ou_theta': ou_noise_decimal.sample()[0],  # OU noise reversion rate
        'learning_starts': ou_noise_hundreds.sample()[0],  # Begin learning after performing this many steps
        'num_steps': int(round(ou_noise_digit.sample()[0])),  # Number of steps to unroll Bellman equation by
        'grad_norm_clip': ou_noise_tens.sample()[0],  # Global gradient clipping value
        'verbose': True,  # Verbose printing
    }
    if d['num_steps'] == 0:
        d['num_steps'] = 1
    return d


import random


def init_population(population_size):
    def generate_individual():
        def get_noise(place):
            return OUNoise(size=1, mu=int(random.random() * place))

        ou_noise_millions = get_noise(10_000_000)
        ou_noise_thousands = get_noise(10_000)
        ou_noise_hundreds = get_noise(1000)
        ou_noise_tens = get_noise(100)
        ou_noise_digit = get_noise(10)
        ou_noise_decimal = get_noise(1)

        params = generate_params_from_noise_value(ou_noise_millions, ou_noise_thousands, ou_noise_hundreds,
                                                  ou_noise_tens, ou_noise_digit, ou_noise_decimal)
        return params, DDPG(params)

    result = [generate_individual() for _ in range(population_size)]

    return result


def default_params():
    return {
        'env_name': '/home/ani/SoftEvo/Build/main.x86_64',  # Environment name
        'device': 'cpu',  # Device DQN runs on
        'target_reward': -200,  # Target reward to stop training at when reached
        'action_scale': 2.,  # Gym pendulum's action range is from -2 to +2 (Why OpenAI?)
        'verbose': True,  # Verbose printing
    }


from copy import deepcopy


def model_crossover(parent1, parent2, crossover_rate=0.5):
    """
    Performs crossover between two neural networks.

    Args:
        parent1 (nn.Module): The first parent network.
        parent2 (nn.Module): The second parent network.
        crossover_rate (float): The probability of performing crossover for each weight.

    Returns:
        nn.Module: The offspring network.
    """

    offspring = deepcopy(parent1)  # Create a new network with the same architecture
    offspring.load_state_dict(parent1.state_dict())  # Initialize with parent1's weights
    for (name1, param1), (name2, param2) in zip(parent1.named_parameters(), parent2.named_parameters()):
        if torch.rand(1) < crossover_rate:
            # Perform crossover for this weight
            mask = torch.rand_like(param1) < 0.5
            offspring.state_dict()[name1].data = torch.where(mask, param1, param2)
    return offspring


def crossover(parent1: DDPG, parent2: DDPG | None, mutation_rate: float) -> list[tuple[dict[str, str], DDPG]]:
    num_children = random.randint(1, 5)
    children: list[tuple[dict[str, str], DDPG]] = []
    for _ in range(num_children):
        params = default_params()
        for param in optimized_params:
            params[param] = (parent1 if random.randint(0, 1) == 1 or parent2 is None else parent2).config[param]

            if random.random() < mutation_rate:
                hyperparam_value_context = 10 * (len(str(params[param])) - 1)
                params[param] += (random.random() * 2 * hyperparam_value_context) - (hyperparam_value_context)
        children.append((params, DDPG(params)))
    return children


def filthy_crossover(parent1: DDPG, parent2: DDPG | None, mutation_rate: float) -> list[tuple[dict[str, str], DDPG]]:
    num_children = random.randint(1, 5)
    children: list[tuple[dict[str, str], DDPG]] = []
    for _ in range(num_children):
        params = default_params()
        for param in optimized_params:
            params[param] = (parent1 if random.randint(0, 1) == 1 or parent2 is None else parent2).config[param]

            if random.random() < mutation_rate:
                hyperparam_value_context = 10 * (len(str(params[param])) - 1)
                params[param] += (random.random() * 2 * hyperparam_value_context) - (hyperparam_value_context)
        children.append((params, DDPG(params)))
        children[-1][1].online_actor = model_crossover(parent1.online_actor, parent2.online_actor)
        children[-1][1].online_critic = model_crossover(parent1.online_critic, parent2.online_critic)
        children[-1][1].optimizer_actor = model_crossover(parent1.optimizer_actor, parent2.optimizer_actor)
        children[-1][1].optimizer_critic = model_crossover(parent1.optimizer_critic, parent2.optimizer_critic)
    return children


def evolve(population_size, mutation_rate, num_generations, filthy=False):
    population = init_population(population_size)

    def handler(params, individual):
        logs = individual.train()
        return params, individual, logs

    for generation in range(num_generations):
        print(f'Generation {generation + 1}')
        best_accuracy = float('-inf')
        best_individual = None, None

        # print(population)
        # with ThreadPoolExecutor() as executor:
        #     futures = [executor.submit(handler, params, individual) for params, individual in population]
        #     for future in as_completed(futures):
        #         params, individual, logs = future.result()
        #         if logs['episode_rewards'][-1] > best_accuracy:
        #             best_accuracy = logs['episode_rewards'][-1]
        #             best_individual = params, individual
        #
        # print(f'Best reward in generation {generation + 1}: {best_accuracy}')
        # print(f'Best individual params: {best_individual[0]}')

        for params, individual in population:
            logs = individual.train()
            if logs['episode_rewards'][-1] > best_accuracy:
                best_accuracy = logs['episode_rewards'][-1]
                best_individual = params, individual



        next_generation = []
        selected = population[:population_size // 2]  # Select last half of population to reproduce

        # Reproduction
        for i in range(0, len(selected), 2):
            _, parent1 = selected[i]
            _, parent2 = selected[i + 1] if i + 1 < len(selected) - 1 else (None, None)
            children = filthy_crossover(parent1, parent2, mutation_rate) if filthy else crossover(parent1, parent2,
                                                                                                  mutation_rate)
            next_generation.extend(children)

        population = next_generation


evolve(10, .01, 20)
