{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27df6f0-ab51-4aef-b87c-15b745ee30de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  1\n",
      "Generation 1 | Episode 20 | Avg Reward 9.7\n",
      "Generation 1 | Episode 40 | Avg Reward 10.9\n",
      "Generation 1 | Episode 60 | Avg Reward 16.1\n",
      "Generation 1 | Episode 80 | Avg Reward 48.0\n",
      "Generation 1 | Episode 100 | Avg Reward 85.0\n",
      "Generation 1 | Episode 120 | Avg Reward 156.7\n",
      "Generation 1 | Episode 140 | Avg Reward 148.8\n",
      "Generation 1 | Episode 160 | Avg Reward 334.0\n",
      "Generation 1 | Episode 180 | Avg Reward 423.9\n",
      "Generation 1 | Episode 200 | Avg Reward 255.0\n",
      "Generation 1 Average Reward: 148.8\n",
      "Generation:  2\n",
      "Generation 2 | Episode 20 | Avg Reward 82.5\n",
      "Generation 2 | Episode 40 | Avg Reward 166.3\n",
      "Generation 2 | Episode 60 | Avg Reward 448.9\n",
      "Generation 2 | Episode 80 | Avg Reward 826.0\n",
      "Generation 2 | Episode 100 | Avg Reward 836.5\n",
      "Generation 2 | Episode 120 | Avg Reward 577.6\n",
      "Generation 2 | Episode 140 | Avg Reward 183.6\n",
      "Generation 2 | Episode 160 | Avg Reward 458.4\n",
      "Generation 2 | Episode 180 | Avg Reward 308.3\n",
      "Generation 2 | Episode 200 | Avg Reward 467.7\n",
      "Generation 2 Average Reward: 435.6\n",
      "Generation:  3\n",
      "Generation 3 | Episode 20 | Avg Reward 221.2\n",
      "Generation 3 | Episode 40 | Avg Reward 458.6\n",
      "Generation 3 | Episode 60 | Avg Reward 935.9\n",
      "Generation 3 | Episode 80 | Avg Reward 668.6\n",
      "Generation 3 | Episode 100 | Avg Reward 156.2\n",
      "Generation 3 | Episode 120 | Avg Reward 418.3\n",
      "Generation 3 | Episode 140 | Avg Reward 874.5\n",
      "Generation 3 | Episode 160 | Avg Reward 969.2\n",
      "Generation 3 | Episode 180 | Avg Reward 969.8\n",
      "Generation 3 | Episode 200 | Avg Reward 750.5\n",
      "Generation 3 Average Reward: 642.3\n",
      "Generation:  4\n",
      "Generation 4 | Episode 20 | Avg Reward 655.7\n",
      "Generation 4 | Episode 40 | Avg Reward 639.5\n",
      "Generation 4 | Episode 60 | Avg Reward 476.9\n",
      "Generation 4 | Episode 80 | Avg Reward 188.1\n",
      "Generation 4 | Episode 100 | Avg Reward 217.0\n",
      "Generation 4 | Episode 120 | Avg Reward 136.0\n",
      "Generation 4 | Episode 140 | Avg Reward 124.5\n",
      "Generation 4 | Episode 160 | Avg Reward 563.2\n",
      "Generation 4 | Episode 180 | Avg Reward 1000.0\n",
      "Generation 4 | Episode 200 | Avg Reward 1000.0\n",
      "Generation 4 Average Reward: 500.1\n",
      "Generation:  5\n",
      "Generation 5 | Episode 20 | Avg Reward 965.0\n",
      "Generation 5 | Episode 40 | Avg Reward 979.4\n",
      "Generation 5 | Episode 60 | Avg Reward 208.7\n",
      "Generation 5 | Episode 80 | Avg Reward 9.2\n",
      "Generation 5 | Episode 100 | Avg Reward 9.2\n",
      "Generation 5 | Episode 120 | Avg Reward 9.7\n",
      "Generation 5 | Episode 140 | Avg Reward 9.2\n",
      "Generation 5 | Episode 160 | Avg Reward 9.5\n",
      "Generation 5 | Episode 180 | Avg Reward 13.9\n",
      "Generation 5 | Episode 200 | Avg Reward 370.7\n",
      "Generation 5 Average Reward: 258.4\n",
      "Generation:  6\n",
      "Generation 6 | Episode 20 | Avg Reward 785.4\n",
      "Generation 6 | Episode 40 | Avg Reward 779.7\n",
      "Generation 6 | Episode 60 | Avg Reward 186.9\n",
      "Generation 6 | Episode 80 | Avg Reward 89.9\n",
      "Generation 6 | Episode 100 | Avg Reward 73.0\n",
      "Generation 6 | Episode 120 | Avg Reward 89.8\n",
      "Generation 6 | Episode 140 | Avg Reward 59.9\n",
      "Generation 6 | Episode 160 | Avg Reward 11.1\n",
      "Generation 6 | Episode 180 | Avg Reward 10.8\n",
      "Generation 6 | Episode 200 | Avg Reward 10.8\n",
      "Generation 6 Average Reward: 209.7\n",
      "Generation:  7\n",
      "Generation 7 | Episode 20 | Avg Reward 15.1\n",
      "Generation 7 | Episode 40 | Avg Reward 122.6\n",
      "Generation 7 | Episode 60 | Avg Reward 25.4\n",
      "Generation 7 | Episode 80 | Avg Reward 12.7\n",
      "Generation 7 | Episode 100 | Avg Reward 11.5\n",
      "Generation 7 | Episode 120 | Avg Reward 14.3\n",
      "Generation 7 | Episode 140 | Avg Reward 52.5\n",
      "Generation 7 | Episode 160 | Avg Reward 157.1\n",
      "Generation 7 | Episode 180 | Avg Reward 201.4\n",
      "Generation 7 | Episode 200 | Avg Reward 351.8\n",
      "Generation 7 Average Reward: 96.4\n",
      "Generation:  8\n",
      "Generation 8 | Episode 20 | Avg Reward 829.1\n",
      "Generation 8 | Episode 40 | Avg Reward 445.2\n",
      "Generation 8 | Episode 60 | Avg Reward 133.5\n",
      "Generation 8 | Episode 80 | Avg Reward 99.7\n",
      "Generation 8 | Episode 100 | Avg Reward 138.4\n",
      "Generation 8 | Episode 120 | Avg Reward 170.2\n",
      "Generation 8 | Episode 140 | Avg Reward 217.6\n",
      "Generation 8 | Episode 160 | Avg Reward 135.6\n",
      "Generation 8 | Episode 180 | Avg Reward 138.2\n",
      "Generation 8 | Episode 200 | Avg Reward 567.9\n",
      "Generation 8 Average Reward: 287.5\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import random\n",
    "\n",
    "sns.set()\n",
    "\n",
    "class ActorCriticNN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,action_dim))\n",
    "\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1))\n",
    "\n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "\n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value\n",
    "\n",
    "#trainer\n",
    "class PPOTrainer():\n",
    "    def __init__(self, actor_critic, ppo_clip_val = 0.2, target_kl_div = 0.01, max_policy_train_iters = 80, value_train_iters=80, \n",
    "                policy_lr = 3e-4, value_lr = 1e-2):\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_val = ppo_clip_val\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.value_train_iters = value_train_iters\n",
    "    \n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr = policy_lr)\n",
    "    \n",
    "        value_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr = value_lr)\n",
    "\n",
    "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "\n",
    "        for _ in range(self.max_policy_train_iters):\n",
    "                \n",
    "            self.policy_optim.zero_grad()\n",
    "    \n",
    "            new_logits = self.ac.policy(obs)\n",
    "            new_logits = Categorical(logits = new_logits)\n",
    "            new_log_probs = new_logits.log_prob(acts)\n",
    "    \n",
    "            policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            clipped_ratio = policy_ratio.clamp(1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "            clipped_loss = clipped_ratio * gaes\n",
    "            full_loss = policy_ratio * gaes\n",
    "            policy_loss = -torch.min(full_loss, clipped_loss).mean()\n",
    "    \n",
    "            policy_loss.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "            kl_div = (old_log_probs - new_log_probs).mean()\n",
    "            if kl_div >= self.target_kl_div:\n",
    "                break\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "\n",
    "        for _ in range (self.value_train_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "    \n",
    "            values = self.ac.value(obs)\n",
    "            value_loss = (returns-values) ** 2\n",
    "            value_loss = value_loss.mean()\n",
    "    \n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()\n",
    "\n",
    "def create_population(pop_size, state_dim, action_dim):\n",
    "    return [ActorCriticNN(state_dim, action_dim) for _ in range(pop_size)]\n",
    "\n",
    "def evaluate_population(population, env, max_steps=100):\n",
    "    fitness_scores = []\n",
    "    for policy in population:\n",
    "        total_reward = 0\n",
    "        obs, _ = env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "            logits, _ = policy(obs_tensor)\n",
    "            act_dist = Categorical(logits=logits)\n",
    "            action = act_dist.sample()\n",
    "            next_obs, reward, done, _, __ = env.step(action.item())\n",
    "            total_reward += reward\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n",
    "        fitness_scores.append(total_reward)\n",
    "    return fitness_scores\n",
    "\n",
    "def select_parents(population, fitness_scores, num_parents):\n",
    "   # Get indices that would sort the fitness scores in descending order\n",
    "    sorted_indices = np.argsort(fitness_scores)[::-1]  # Reverse for descending order\n",
    "    \n",
    "    # Select the best parents based on the sorted indices\n",
    "    parents = np.array(population)[sorted_indices][:num_parents]\n",
    "    return parents.tolist()\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    child = ActorCriticNN(state_dim, action_dim)\n",
    "    for param1, param2, param_child in zip(parent1.parameters(), parent2.parameters(), child.parameters()):\n",
    "        param_child.data = (param1.data + param2.data) / 2  # Average weights\n",
    "    return child\n",
    "\n",
    "def mutate(policy, mutation_rate=0.01):\n",
    "    for param in policy.parameters():\n",
    "        if random.random() < mutation_rate:\n",
    "            param.data += torch.randn(param.size()) * 0.1  # Add small random noise\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Return discounted rewards based on the given rewards and the gamma param.\n",
    "    \"\"\"\n",
    "\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma = 0.99, decay = 0.97):\n",
    "    \"\"\"\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/abs/1506.02438\n",
    "    \"\"\"\n",
    "\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
    "\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "    return np.array(gaes[::-1])\n",
    "\n",
    "def rollout(model, env, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Performs a single rollout, sampling an action and collecting data.\n",
    "    Returns training data in the shape (n_steps, observation_shape) and the cumulative reward.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the data storage (obs, act, reward, values, act_log_probs)\n",
    "    train_data = [[], [], [], [], []]  \n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    #print(\"Environment reset, initial observation:\", obs)  # Check if env.reset() works\n",
    "\n",
    "    ep_reward = 0\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        #Convert observation to a PyTorch tensor and ensure correct shape\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "        logits, val = model(obs)\n",
    "\n",
    "        # Create action distribution and sample an action\n",
    "        act_dist = Categorical(logits=logits)\n",
    "        act = act_dist.sample()\n",
    "        act_log_prob = act_dist.log_prob(act).item()\n",
    "\n",
    "        act, val = act.item(), val.item()\n",
    "\n",
    "        # Take action in the environment\n",
    "        next_obs, reward, done, _, __ = env.step(act)\n",
    "\n",
    "        # Store the data for training\n",
    "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
    "            train_data[i].append(item)\n",
    "\n",
    "        #print(f\"Action taken: {act.item()}, Reward received: {reward}, Done: {done}\")\n",
    "\n",
    "        # Update observation and cumulative reward\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "\n",
    "        # Break if the episode ends\n",
    "        if done:\n",
    "            #print(f\"Episode ended after {step+1} steps with cumulative reward: {ep_reward}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "    \n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3])\n",
    "    \n",
    "    return train_data, ep_reward\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Define the environment and parameters\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "n_episodes = 200\n",
    "print_freq = 20\n",
    "pop_size = 30\n",
    "num_parents = 10\n",
    "num_generations = 8\n",
    "\n",
    "# Initialize the population\n",
    "population = create_population(pop_size, state_dim, action_dim)\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    # Evaluate the current population\n",
    "    fitness_scores = evaluate_population(population, env)\n",
    "    \n",
    "    # Select the best parents\n",
    "    parents = select_parents(population, fitness_scores, num_parents)\n",
    "\n",
    "    next_generation = []\n",
    "    \n",
    "    # Print the generation number\n",
    "    print(\"Generation: \", generation + 1)\n",
    "\n",
    "    # Create a selective parents list for crossover\n",
    "    selective_parents = [parents[0], parents[1], parents[2], parents[3], parents[4]]\n",
    "\n",
    "    best_select = []\n",
    "    \n",
    "    for i in range(num_parents):\n",
    "        parent1, parent2 = np.random.choice(selective_parents, 2, replace=False)\n",
    "        child = crossover(parent1, parent2)\n",
    "        mutate(child)\n",
    "        best_select.append(child)\n",
    "\n",
    "    for i in range(pop_size):\n",
    "        next_generation.append(population[i])\n",
    "\n",
    "    # Evaluate fitness of the new generation\n",
    "    next_generation_fitness = evaluate_population(next_generation, env)\n",
    "    \n",
    "    # Sort the next generation based on fitness scores (best to worst)\n",
    "    next_generation = [x for _, x in sorted(zip(next_generation_fitness, next_generation), key=lambda pair: pair[0], reverse=True)]\n",
    "\n",
    "    index = 0\n",
    "    for i in range ((pop_size-10),pop_size):\n",
    "        next_generation[i] = best_select[index]\n",
    "        index = index + 1\n",
    "    \n",
    "    population = next_generation\n",
    "\n",
    "    # Optionally fine-tune the best policy with PPO after EA\n",
    "    best_policy = parents[0]  # Select the best policy for further training with PPO\n",
    "    ppo = PPOTrainer(best_policy, policy_lr=3e-4, value_lr=1e-3, target_kl_div=0.02, max_policy_train_iters=40, value_train_iters=40)\n",
    "\n",
    "    ep_rewards = []\n",
    "    for episode_idx in range(n_episodes):\n",
    "        # Perform rollout\n",
    "        train_data, reward = rollout(best_policy, env)\n",
    "        ep_rewards.append(reward)\n",
    "\n",
    "        permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "        obs = torch.tensor(train_data[0][permute_idxs], dtype=torch.float32)\n",
    "        act = torch.tensor(train_data[1][permute_idxs], dtype=torch.int32)\n",
    "        gaes = torch.tensor(train_data[3][permute_idxs], dtype=torch.float32)\n",
    "        act_log_probs = torch.tensor(train_data[4][permute_idxs], dtype=torch.float32)\n",
    "\n",
    "        # Value Data\n",
    "        returns = discount_rewards(train_data[2])[permute_idxs]\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        # Train Policy\n",
    "        ppo.train_policy(obs, act, act_log_probs, gaes)\n",
    "        ppo.train_value(obs, returns)\n",
    "\n",
    "        # Print average reward every 'print_freq' episodes\n",
    "        if (episode_idx + 1) % print_freq == 0:\n",
    "            avg_reward = np.mean(ep_rewards[-print_freq:])  # Calculate the average of the last 'print_freq' rewards\n",
    "            print('Generation {} | Episode {} | Avg Reward {:.1f}'.format(\n",
    "                generation + 1, episode_idx + 1, avg_reward))\n",
    "\n",
    "    # Calculate and print the overall average reward for this generation\n",
    "    generation_avg_reward = np.mean(ep_rewards)  # Calculate average for all episodes in this generation\n",
    "    print(\"Generation {} Average Reward: {:.1f}\".format(generation + 1, generation_avg_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d4229-9567-4287-b2d3-375145f2360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three Things I tried\n",
    "\"\"\"\n",
    "First I tried random parents from the parents array and using them to create a child and constructing the new generation as the offspring\n",
    "\n",
    "Second I tried choosing the best 5-6 parents and then using their children to populate the next generation\n",
    "\n",
    "Third I used the best 5-6 parents to make 10 children, then I copied the current population into next_gen and then I ordered next gen in\n",
    "order of best policy to worst, and then I replaced the last 10 with the 10 children that I produced\n",
    "\n",
    "\"\"\"\n",
    "# I NEED TO TEST THIS SOMEWHERE WHERE IT DOESNT OVERFIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2430982-5c3c-4083-8649-c5c1f7d219c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  1\n",
      "Generation 1 | Episode 20 | Avg Reward 50.2\n",
      "Generation 1 | Episode 40 | Avg Reward 21.0\n",
      "Generation 1 | Episode 60 | Avg Reward 10.6\n",
      "Generation 1 | Episode 80 | Avg Reward 12.9\n",
      "Generation 1 | Episode 100 | Avg Reward 54.9\n",
      "Generation 1 | Episode 120 | Avg Reward 63.8\n",
      "Generation 1 | Episode 140 | Avg Reward 84.0\n",
      "Generation 1 | Episode 160 | Avg Reward 167.8\n",
      "Generation 1 | Episode 180 | Avg Reward 121.3\n",
      "Generation 1 | Episode 200 | Avg Reward 130.2\n",
      "Generation 1 Average Reward: 71.7\n",
      "Generation:  2\n",
      "Generation 2 | Episode 20 | Avg Reward 22.8\n",
      "Generation 2 | Episode 40 | Avg Reward 15.4\n",
      "Generation 2 | Episode 60 | Avg Reward 34.6\n",
      "Generation 2 | Episode 80 | Avg Reward 50.5\n",
      "Generation 2 | Episode 100 | Avg Reward 79.1\n",
      "Generation 2 | Episode 120 | Avg Reward 69.7\n",
      "Generation 2 | Episode 140 | Avg Reward 95.9\n",
      "Generation 2 | Episode 160 | Avg Reward 89.0\n",
      "Generation 2 | Episode 180 | Avg Reward 25.0\n",
      "Generation 2 | Episode 200 | Avg Reward 9.3\n",
      "Generation 2 Average Reward: 49.1\n",
      "Generation:  3\n",
      "Generation 3 | Episode 20 | Avg Reward 10.4\n",
      "Generation 3 | Episode 40 | Avg Reward 17.3\n",
      "Generation 3 | Episode 60 | Avg Reward 41.2\n",
      "Generation 3 | Episode 80 | Avg Reward 45.9\n",
      "Generation 3 | Episode 100 | Avg Reward 82.5\n",
      "Generation 3 | Episode 120 | Avg Reward 71.0\n",
      "Generation 3 | Episode 140 | Avg Reward 104.9\n",
      "Generation 3 | Episode 160 | Avg Reward 69.2\n",
      "Generation 3 | Episode 180 | Avg Reward 41.4\n",
      "Generation 3 | Episode 200 | Avg Reward 87.8\n",
      "Generation 3 Average Reward: 57.2\n",
      "Generation:  4\n",
      "Generation 4 | Episode 20 | Avg Reward 85.2\n",
      "Generation 4 | Episode 40 | Avg Reward 11.3\n",
      "Generation 4 | Episode 60 | Avg Reward 9.3\n",
      "Generation 4 | Episode 80 | Avg Reward 9.3\n",
      "Generation 4 | Episode 100 | Avg Reward 9.4\n",
      "Generation 4 | Episode 120 | Avg Reward 12.1\n",
      "Generation 4 | Episode 140 | Avg Reward 18.4\n",
      "Generation 4 | Episode 160 | Avg Reward 154.7\n",
      "Generation 4 | Episode 180 | Avg Reward 144.4\n",
      "Generation 4 | Episode 200 | Avg Reward 139.4\n",
      "Generation 4 Average Reward: 59.4\n",
      "Generation:  5\n",
      "Generation 5 | Episode 20 | Avg Reward 241.1\n",
      "Generation 5 | Episode 40 | Avg Reward 266.6\n",
      "Generation 5 | Episode 60 | Avg Reward 535.3\n",
      "Generation 5 | Episode 80 | Avg Reward 439.8\n",
      "Generation 5 | Episode 100 | Avg Reward 469.5\n",
      "Generation 5 | Episode 120 | Avg Reward 121.5\n",
      "Generation 5 | Episode 140 | Avg Reward 31.6\n",
      "Generation 5 | Episode 160 | Avg Reward 24.7\n",
      "Generation 5 | Episode 180 | Avg Reward 9.8\n",
      "Generation 5 | Episode 200 | Avg Reward 31.6\n",
      "Generation 5 Average Reward: 217.1\n",
      "Generation:  6\n",
      "Generation 6 | Episode 20 | Avg Reward 117.8\n",
      "Generation 6 | Episode 40 | Avg Reward 90.2\n",
      "Generation 6 | Episode 60 | Avg Reward 102.0\n",
      "Generation 6 | Episode 80 | Avg Reward 70.2\n",
      "Generation 6 | Episode 100 | Avg Reward 257.2\n",
      "Generation 6 | Episode 120 | Avg Reward 249.7\n",
      "Generation 6 | Episode 140 | Avg Reward 224.6\n",
      "Generation 6 | Episode 160 | Avg Reward 252.6\n",
      "Generation 6 | Episode 180 | Avg Reward 609.5\n",
      "Generation 6 | Episode 200 | Avg Reward 1000.0\n",
      "Generation 6 Average Reward: 297.4\n",
      "Generation:  7\n",
      "Generation 7 | Episode 20 | Avg Reward 48.1\n",
      "Generation 7 | Episode 40 | Avg Reward 85.0\n",
      "Generation 7 | Episode 60 | Avg Reward 100.5\n",
      "Generation 7 | Episode 80 | Avg Reward 140.6\n",
      "Generation 7 | Episode 100 | Avg Reward 113.2\n",
      "Generation 7 | Episode 120 | Avg Reward 69.9\n",
      "Generation 7 | Episode 140 | Avg Reward 65.2\n",
      "Generation 7 | Episode 160 | Avg Reward 58.2\n",
      "Generation 7 | Episode 180 | Avg Reward 43.4\n",
      "Generation 7 | Episode 200 | Avg Reward 93.1\n",
      "Generation 7 Average Reward: 81.7\n",
      "Generation:  8\n",
      "Generation 8 | Episode 20 | Avg Reward 669.0\n",
      "Generation 8 | Episode 40 | Avg Reward 840.7\n",
      "Generation 8 | Episode 60 | Avg Reward 9.2\n",
      "Generation 8 | Episode 80 | Avg Reward 9.3\n",
      "Generation 8 | Episode 100 | Avg Reward 9.5\n",
      "Generation 8 | Episode 120 | Avg Reward 9.3\n",
      "Generation 8 | Episode 140 | Avg Reward 9.3\n",
      "Generation 8 | Episode 160 | Avg Reward 10.7\n",
      "Generation 8 | Episode 180 | Avg Reward 9.2\n",
      "Generation 8 | Episode 200 | Avg Reward 48.4\n",
      "Generation 8 Average Reward: 162.5\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import random\n",
    "\n",
    "sns.set()\n",
    "\n",
    "class ActorCriticNN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,action_dim))\n",
    "\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1))\n",
    "\n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "\n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value\n",
    "\n",
    "#trainer\n",
    "class PPOTrainer():\n",
    "    def __init__(self, actor_critic, ppo_clip_val, target_kl_div, policy_lr, value_lr, max_policy_train_iters = 80, value_train_iters = 80):\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_val = ppo_clip_val\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.value_train_iters = value_train_iters\n",
    "    \n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr = policy_lr)\n",
    "    \n",
    "        value_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr = value_lr)\n",
    "\n",
    "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "\n",
    "        for _ in range(self.max_policy_train_iters):\n",
    "                \n",
    "            self.policy_optim.zero_grad()\n",
    "    \n",
    "            new_logits = self.ac.policy(obs)\n",
    "            new_logits = Categorical(logits = new_logits)\n",
    "            new_log_probs = new_logits.log_prob(acts)\n",
    "    \n",
    "            policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            clipped_ratio = policy_ratio.clamp(1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "            clipped_loss = clipped_ratio * gaes\n",
    "            full_loss = policy_ratio * gaes\n",
    "            policy_loss = -torch.min(full_loss, clipped_loss).mean()\n",
    "    \n",
    "            policy_loss.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "            kl_div = (old_log_probs - new_log_probs).mean()\n",
    "            if kl_div >= self.target_kl_div:\n",
    "                break\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "\n",
    "        for _ in range (self.value_train_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "    \n",
    "            values = self.ac.value(obs)\n",
    "            value_loss = (returns-values) ** 2\n",
    "            value_loss = value_loss.mean()\n",
    "    \n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()\n",
    "\n",
    "class ActorCriticChromosome:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        # Actor-Critic neural network\n",
    "        self.model = ActorCriticNN(state_dim, action_dim)\n",
    "\n",
    "        #Hyperparameters\n",
    "        self.policy_lr = random.uniform(1e-5, 1e-2)  # Expanded range\n",
    "        self.value_lr = random.uniform(1e-5, 1e-2)  # Expanded range\n",
    "        self.ppo_clip_val = random.uniform(0.1, 0.4)  # Expanded range\n",
    "        self.target_kl_div = random.uniform(0.001, 0.05)\n",
    "\n",
    "def create_population(pop_size, state_dim, action_dim):\n",
    "    return [ActorCriticChromosome(state_dim, action_dim) for _ in range(pop_size)]\n",
    "\n",
    "\"\"\"\n",
    "When initializing PPOTrainer, you're passing the individual's hyperparameters (policy_lr, value_lr, etc.) \n",
    "to ensure the evaluation process reflects the effect of those hyperparameters.\n",
    "You're sampling actions from the model and accumulating rewards to calculate the fitness score.\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_population(population, env, max_steps=100):\n",
    "    fitness_scores = []\n",
    "    for individual in population:\n",
    "        total_reward = 0\n",
    "\n",
    "        # Initialize PPOTrainer with individual's hyperparameters\n",
    "        ppo = PPOTrainer(individual.model,\n",
    "                         ppo_clip_val=individual.ppo_clip_val,\n",
    "                         target_kl_div=individual.target_kl_div,\n",
    "                         policy_lr=individual.policy_lr,\n",
    "                         value_lr=individual.value_lr)\n",
    "\n",
    "        # Reset the environment and evaluate the model\n",
    "        obs, _ = env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "            logits, _ = individual.model(obs_tensor)  # Use the model from the individual\n",
    "            act_dist = Categorical(logits=logits)\n",
    "            action = act_dist.sample()\n",
    "            next_obs, reward, done, _, __ = env.step(action.item())\n",
    "            total_reward += reward\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n",
    "        fitness_scores.append(total_reward)\n",
    "    \n",
    "    return fitness_scores\n",
    "\n",
    "def select_parents(population, fitness_scores, num_parents):\n",
    "   # Get indices that would sort the fitness scores in descending order\n",
    "    sorted_indices = np.argsort(fitness_scores)[::-1]  # Reverse for descending order\n",
    "    \n",
    "    # Select the best parents based on the sorted indices\n",
    "    parents = np.array(population)[sorted_indices][:num_parents]\n",
    "    return parents.tolist()\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    child = ActorCriticChromosome(state_dim, action_dim)\n",
    "    for param1, param2, param_child in zip(parent1.model.parameters(), parent2.model.parameters(), child.model.parameters()):\n",
    "        param_child.data = (param1.data + param2.data) / 2  # Average weights\n",
    "\n",
    "    # Crossover for hyperparameters\n",
    "    child.policy_lr = (parent1.policy_lr + parent2.policy_lr) / 2\n",
    "    child.value_lr = (parent1.value_lr + parent2.value_lr) / 2\n",
    "    child.ppo_clip_val = (parent1.ppo_clip_val + parent2.ppo_clip_val) / 2\n",
    "    child.target_kl_div = (parent1.target_kl_div + parent2.target_kl_div) / 2\n",
    "    \n",
    "    return child\n",
    "\n",
    "def mutate(individual, mutation_rate=0.01):\n",
    "    for param in individual.model.parameters():\n",
    "        if random.random() < mutation_rate:\n",
    "            param.data += torch.randn(param.size()) * 0.1  # Add small random noise\n",
    "\n",
    "    # Mutate hyperparameters\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.policy_lr += random.uniform(-1e-5, 1e-5)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.value_lr += random.uniform(-1e-4, 1e-4)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.ppo_clip_val += random.uniform(-0.01, 0.01)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.target_kl_div += random.uniform(-0.001, 0.001)\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Return discounted rewards based on the given rewards and the gamma param.\n",
    "    \"\"\"\n",
    "\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma = 0.99, decay = 0.97):\n",
    "    \"\"\"\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/abs/1506.02438\n",
    "    \"\"\"\n",
    "\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
    "\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "    return np.array(gaes[::-1])\n",
    "\n",
    "def rollout(model, env, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Performs a single rollout, sampling an action and collecting data.\n",
    "    Returns training data in the shape (n_steps, observation_shape) and the cumulative reward.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the data storage (obs, act, reward, values, act_log_probs)\n",
    "    train_data = [[], [], [], [], []]  \n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    #print(\"Environment reset, initial observation:\", obs)  # Check if env.reset() works\n",
    "\n",
    "    ep_reward = 0\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        #Convert observation to a PyTorch tensor and ensure correct shape\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "        logits, val = model(obs)\n",
    "\n",
    "        # Create action distribution and sample an action\n",
    "        act_dist = Categorical(logits=logits)\n",
    "        act = act_dist.sample()\n",
    "        act_log_prob = act_dist.log_prob(act).item()\n",
    "\n",
    "        act, val = act.item(), val.item()\n",
    "\n",
    "        # Take action in the environment\n",
    "        next_obs, reward, done, _, __ = env.step(act)\n",
    "\n",
    "        # Store the data for training\n",
    "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
    "            train_data[i].append(item)\n",
    "\n",
    "        #print(f\"Action taken: {act.item()}, Reward received: {reward}, Done: {done}\")\n",
    "\n",
    "        # Update observation and cumulative reward\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "\n",
    "        # Break if the episode ends\n",
    "        if done:\n",
    "            #print(f\"Episode ended after {step+1} steps with cumulative reward: {ep_reward}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "    \n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3])\n",
    "    \n",
    "    return train_data, ep_reward\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Define the environment and parameters\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "n_episodes = 200\n",
    "print_freq = 20\n",
    "pop_size = 30\n",
    "num_parents = 10\n",
    "num_generations = 8\n",
    "\n",
    "# Initialize the population\n",
    "population = create_population(pop_size, state_dim, action_dim)\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    # Evaluate the current population\n",
    "    fitness_scores = evaluate_population(population, env)\n",
    "    \n",
    "    # Select the best parents\n",
    "    parents = select_parents(population, fitness_scores, num_parents)\n",
    "\n",
    "    next_generation = []\n",
    "    \n",
    "    # Print the generation number\n",
    "    print(\"Generation: \", generation + 1)\n",
    "\n",
    "    # Create a selective parents list for crossover\n",
    "    selective_parents = [parents[0], parents[1], parents[2], parents[3], parents[4]]\n",
    "\n",
    "    best_select = []\n",
    "    \n",
    "    for i in range(num_parents):\n",
    "        parent1, parent2 = np.random.choice(selective_parents, 2, replace=False)\n",
    "        child = crossover(parent1, parent2)\n",
    "        mutate(child)\n",
    "        best_select.append(child)\n",
    "\n",
    "    for i in range(pop_size):\n",
    "        next_generation.append(population[i])\n",
    "\n",
    "    # Evaluate fitness of the new generation\n",
    "    next_generation_fitness = evaluate_population(next_generation, env)\n",
    "    \n",
    "    # Sort the next generation based on fitness scores (best to worst)\n",
    "    next_generation = [x for _, x in sorted(zip(next_generation_fitness, next_generation), key=lambda pair: pair[0], reverse=True)]\n",
    "\n",
    "    index = 0\n",
    "    for i in range ((pop_size-10),pop_size):\n",
    "        next_generation[i] = best_select[index]\n",
    "        index = index + 1\n",
    "    \n",
    "    population = next_generation\n",
    "\n",
    "    # Optionally fine-tune the best policy with PPO after EA\n",
    "    best_policy = parents[0]  # Select the best policy for further training with PPO\n",
    "    ppo = PPOTrainer(best_policy.model, ppo_clip_val = best_policy.ppo_clip_val, target_kl_div=best_policy.target_kl_div, \n",
    "                     policy_lr=best_policy.policy_lr, value_lr=best_policy.value_lr)\n",
    "\n",
    "    ep_rewards = []\n",
    "    for episode_idx in range(n_episodes):\n",
    "        # Perform rollout\n",
    "        train_data, reward = rollout(best_policy.model, env)\n",
    "        ep_rewards.append(reward)\n",
    "\n",
    "        permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "        obs = torch.tensor(train_data[0][permute_idxs], dtype=torch.float32)\n",
    "        act = torch.tensor(train_data[1][permute_idxs], dtype=torch.int32)\n",
    "        gaes = torch.tensor(train_data[3][permute_idxs], dtype=torch.float32)\n",
    "        act_log_probs = torch.tensor(train_data[4][permute_idxs], dtype=torch.float32)\n",
    "\n",
    "        # Value Data\n",
    "        returns = discount_rewards(train_data[2])[permute_idxs]\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        # Train Policy\n",
    "        ppo.train_policy(obs, act, act_log_probs, gaes)\n",
    "        ppo.train_value(obs, returns)\n",
    "\n",
    "        # Print average reward every 'print_freq' episodes\n",
    "        if (episode_idx + 1) % print_freq == 0:\n",
    "            avg_reward = np.mean(ep_rewards[-print_freq:])  # Calculate the average of the last 'print_freq' rewards\n",
    "            print('Generation {} | Episode {} | Avg Reward {:.1f}'.format(\n",
    "                generation + 1, episode_idx + 1, avg_reward))\n",
    "\n",
    "    # Calculate and print the overall average reward for this generation\n",
    "    generation_avg_reward = np.mean(ep_rewards)  # Calculate average for all episodes in this generation\n",
    "    print(\"Generation {} Average Reward: {:.1f}\".format(generation + 1, generation_avg_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681ec8e-4785-4b24-870d-97fb01189205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
