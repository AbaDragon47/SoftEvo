{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27df6f0-ab51-4aef-b87c-15b745ee30de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  1\n",
      "Generation 1 | Episode 20 | Avg Reward 28.6\n",
      "Generation 1 | Episode 40 | Avg Reward 97.4\n",
      "Generation 1 | Episode 60 | Avg Reward 136.2\n",
      "Generation 1 | Episode 80 | Avg Reward 156.3\n",
      "Generation 1 | Episode 100 | Avg Reward 304.3\n",
      "Generation 1 | Episode 120 | Avg Reward 178.1\n",
      "Generation 1 | Episode 140 | Avg Reward 160.0\n",
      "Generation 1 | Episode 160 | Avg Reward 161.4\n",
      "Generation 1 | Episode 180 | Avg Reward 365.6\n",
      "Generation 1 | Episode 200 | Avg Reward 180.2\n",
      "Generation 1 Average Reward: 176.8\n",
      "Generation:  2\n",
      "Generation 2 | Episode 20 | Avg Reward 81.0\n",
      "Generation 2 | Episode 40 | Avg Reward 88.7\n",
      "Generation 2 | Episode 60 | Avg Reward 96.5\n",
      "Generation 2 | Episode 80 | Avg Reward 152.0\n",
      "Generation 2 | Episode 100 | Avg Reward 270.3\n",
      "Generation 2 | Episode 120 | Avg Reward 124.8\n",
      "Generation 2 | Episode 140 | Avg Reward 136.1\n",
      "Generation 2 | Episode 160 | Avg Reward 202.6\n",
      "Generation 2 | Episode 180 | Avg Reward 180.4\n",
      "Generation 2 | Episode 200 | Avg Reward 234.1\n",
      "Generation 2 Average Reward: 156.6\n",
      "Generation:  3\n",
      "Generation 3 | Episode 20 | Avg Reward 328.2\n",
      "Generation 3 | Episode 40 | Avg Reward 562.0\n",
      "Generation 3 | Episode 60 | Avg Reward 341.5\n",
      "Generation 3 | Episode 80 | Avg Reward 404.9\n",
      "Generation 3 | Episode 100 | Avg Reward 368.9\n",
      "Generation 3 | Episode 120 | Avg Reward 355.4\n",
      "Generation 3 | Episode 140 | Avg Reward 380.4\n",
      "Generation 3 | Episode 160 | Avg Reward 381.1\n",
      "Generation 3 | Episode 180 | Avg Reward 616.8\n",
      "Generation 3 | Episode 200 | Avg Reward 1000.0\n",
      "Generation 3 Average Reward: 473.9\n",
      "Generation:  4\n",
      "Generation 4 | Episode 20 | Avg Reward 928.9\n",
      "Generation 4 | Episode 40 | Avg Reward 1000.0\n",
      "Generation 4 | Episode 60 | Avg Reward 865.1\n",
      "Generation 4 | Episode 80 | Avg Reward 673.4\n",
      "Generation 4 | Episode 100 | Avg Reward 547.4\n",
      "Generation 4 | Episode 120 | Avg Reward 762.8\n",
      "Generation 4 | Episode 140 | Avg Reward 1000.0\n",
      "Generation 4 | Episode 160 | Avg Reward 916.0\n",
      "Generation 4 | Episode 180 | Avg Reward 195.9\n",
      "Generation 4 | Episode 200 | Avg Reward 79.8\n",
      "Generation 4 Average Reward: 696.9\n",
      "Generation:  5\n",
      "Generation 5 | Episode 20 | Avg Reward 275.1\n",
      "Generation 5 | Episode 40 | Avg Reward 673.2\n",
      "Generation 5 | Episode 60 | Avg Reward 567.1\n",
      "Generation 5 | Episode 80 | Avg Reward 479.7\n",
      "Generation 5 | Episode 100 | Avg Reward 213.8\n",
      "Generation 5 | Episode 120 | Avg Reward 736.1\n",
      "Generation 5 | Episode 140 | Avg Reward 956.5\n",
      "Generation 5 | Episode 160 | Avg Reward 974.8\n",
      "Generation 5 | Episode 180 | Avg Reward 982.2\n",
      "Generation 5 | Episode 200 | Avg Reward 827.0\n",
      "Generation 5 Average Reward: 668.6\n",
      "Generation:  6\n",
      "Generation 6 | Episode 20 | Avg Reward 220.3\n",
      "Generation 6 | Episode 40 | Avg Reward 337.5\n",
      "Generation 6 | Episode 60 | Avg Reward 675.6\n",
      "Generation 6 | Episode 80 | Avg Reward 314.9\n",
      "Generation 6 | Episode 100 | Avg Reward 878.4\n",
      "Generation 6 | Episode 120 | Avg Reward 995.1\n",
      "Generation 6 | Episode 140 | Avg Reward 988.0\n",
      "Generation 6 | Episode 160 | Avg Reward 882.1\n",
      "Generation 6 | Episode 180 | Avg Reward 943.0\n",
      "Generation 6 | Episode 200 | Avg Reward 896.9\n",
      "Generation 6 Average Reward: 713.2\n",
      "Generation:  7\n",
      "Generation 7 | Episode 20 | Avg Reward 596.1\n",
      "Generation 7 | Episode 40 | Avg Reward 924.2\n",
      "Generation 7 | Episode 60 | Avg Reward 910.8\n",
      "Generation 7 | Episode 80 | Avg Reward 803.5\n",
      "Generation 7 | Episode 100 | Avg Reward 969.2\n",
      "Generation 7 | Episode 120 | Avg Reward 1000.0\n",
      "Generation 7 | Episode 140 | Avg Reward 599.5\n",
      "Generation 7 | Episode 160 | Avg Reward 516.6\n",
      "Generation 7 | Episode 180 | Avg Reward 886.9\n",
      "Generation 7 | Episode 200 | Avg Reward 263.2\n",
      "Generation 7 Average Reward: 747.0\n",
      "Generation:  8\n",
      "Generation 8 | Episode 20 | Avg Reward 318.7\n",
      "Generation 8 | Episode 40 | Avg Reward 331.9\n",
      "Generation 8 | Episode 60 | Avg Reward 315.2\n",
      "Generation 8 | Episode 80 | Avg Reward 449.9\n",
      "Generation 8 | Episode 100 | Avg Reward 565.6\n",
      "Generation 8 | Episode 120 | Avg Reward 511.1\n",
      "Generation 8 | Episode 140 | Avg Reward 652.4\n",
      "Generation 8 | Episode 160 | Avg Reward 482.8\n",
      "Generation 8 | Episode 180 | Avg Reward 270.4\n",
      "Generation 8 | Episode 200 | Avg Reward 393.4\n",
      "Generation 8 Average Reward: 429.1\n",
      "Generation:  9\n",
      "Generation 9 | Episode 20 | Avg Reward 177.1\n",
      "Generation 9 | Episode 40 | Avg Reward 259.6\n",
      "Generation 9 | Episode 60 | Avg Reward 535.0\n",
      "Generation 9 | Episode 80 | Avg Reward 497.9\n",
      "Generation 9 | Episode 100 | Avg Reward 841.5\n",
      "Generation 9 | Episode 120 | Avg Reward 695.7\n",
      "Generation 9 | Episode 140 | Avg Reward 882.2\n",
      "Generation 9 | Episode 160 | Avg Reward 910.0\n",
      "Generation 9 | Episode 180 | Avg Reward 856.5\n",
      "Generation 9 | Episode 200 | Avg Reward 991.4\n",
      "Generation 9 Average Reward: 664.7\n",
      "Generation:  10\n",
      "Generation 10 | Episode 20 | Avg Reward 247.1\n",
      "Generation 10 | Episode 40 | Avg Reward 535.2\n",
      "Generation 10 | Episode 60 | Avg Reward 529.4\n",
      "Generation 10 | Episode 80 | Avg Reward 863.0\n",
      "Generation 10 | Episode 100 | Avg Reward 668.6\n",
      "Generation 10 | Episode 120 | Avg Reward 707.6\n",
      "Generation 10 | Episode 140 | Avg Reward 972.8\n",
      "Generation 10 | Episode 160 | Avg Reward 863.6\n",
      "Generation 10 | Episode 180 | Avg Reward 783.9\n",
      "Generation 10 | Episode 200 | Avg Reward 991.1\n",
      "Generation 10 Average Reward: 716.2\n",
      "Generation:  11\n",
      "Generation 11 | Episode 20 | Avg Reward 641.4\n",
      "Generation 11 | Episode 40 | Avg Reward 868.0\n",
      "Generation 11 | Episode 60 | Avg Reward 948.0\n",
      "Generation 11 | Episode 80 | Avg Reward 855.2\n",
      "Generation 11 | Episode 100 | Avg Reward 936.4\n",
      "Generation 11 | Episode 120 | Avg Reward 890.8\n",
      "Generation 11 | Episode 140 | Avg Reward 986.5\n",
      "Generation 11 | Episode 160 | Avg Reward 816.4\n",
      "Generation 11 | Episode 180 | Avg Reward 1000.0\n",
      "Generation 11 | Episode 200 | Avg Reward 968.5\n",
      "Generation 11 Average Reward: 891.1\n",
      "Generation:  12\n",
      "Generation 12 | Episode 20 | Avg Reward 626.3\n",
      "Generation 12 | Episode 40 | Avg Reward 639.6\n",
      "Generation 12 | Episode 60 | Avg Reward 961.1\n",
      "Generation 12 | Episode 80 | Avg Reward 936.1\n",
      "Generation 12 | Episode 100 | Avg Reward 646.2\n",
      "Generation 12 | Episode 120 | Avg Reward 732.6\n",
      "Generation 12 | Episode 140 | Avg Reward 1000.0\n",
      "Generation 12 | Episode 160 | Avg Reward 889.9\n",
      "Generation 12 | Episode 180 | Avg Reward 987.8\n",
      "Generation 12 | Episode 200 | Avg Reward 985.8\n",
      "Generation 12 Average Reward: 840.5\n",
      "Generation:  13\n",
      "Generation 13 | Episode 20 | Avg Reward 814.5\n",
      "Generation 13 | Episode 40 | Avg Reward 830.4\n",
      "Generation 13 | Episode 60 | Avg Reward 911.4\n",
      "Generation 13 | Episode 80 | Avg Reward 937.0\n",
      "Generation 13 | Episode 100 | Avg Reward 921.1\n",
      "Generation 13 | Episode 120 | Avg Reward 888.1\n",
      "Generation 13 | Episode 140 | Avg Reward 948.0\n",
      "Generation 13 | Episode 160 | Avg Reward 849.5\n",
      "Generation 13 | Episode 180 | Avg Reward 837.2\n",
      "Generation 13 | Episode 200 | Avg Reward 920.0\n",
      "Generation 13 Average Reward: 885.7\n",
      "Generation:  14\n",
      "Generation 14 | Episode 20 | Avg Reward 872.4\n",
      "Generation 14 | Episode 40 | Avg Reward 857.9\n",
      "Generation 14 | Episode 60 | Avg Reward 987.2\n",
      "Generation 14 | Episode 80 | Avg Reward 801.7\n",
      "Generation 14 | Episode 100 | Avg Reward 623.7\n",
      "Generation 14 | Episode 120 | Avg Reward 804.1\n",
      "Generation 14 | Episode 140 | Avg Reward 813.9\n",
      "Generation 14 | Episode 160 | Avg Reward 638.9\n",
      "Generation 14 | Episode 180 | Avg Reward 904.6\n",
      "Generation 14 | Episode 200 | Avg Reward 958.5\n",
      "Generation 14 Average Reward: 826.3\n",
      "Generation:  15\n",
      "Generation 15 | Episode 20 | Avg Reward 561.0\n",
      "Generation 15 | Episode 40 | Avg Reward 389.3\n",
      "Generation 15 | Episode 60 | Avg Reward 864.2\n",
      "Generation 15 | Episode 80 | Avg Reward 835.4\n",
      "Generation 15 | Episode 100 | Avg Reward 916.6\n",
      "Generation 15 | Episode 120 | Avg Reward 957.8\n",
      "Generation 15 | Episode 140 | Avg Reward 982.7\n",
      "Generation 15 | Episode 160 | Avg Reward 1000.0\n",
      "Generation 15 | Episode 180 | Avg Reward 997.6\n",
      "Generation 15 | Episode 200 | Avg Reward 326.7\n",
      "Generation 15 Average Reward: 783.1\n",
      "Generation:  16\n",
      "Generation 16 | Episode 20 | Avg Reward 403.6\n",
      "Generation 16 | Episode 40 | Avg Reward 787.1\n",
      "Generation 16 | Episode 60 | Avg Reward 915.4\n",
      "Generation 16 | Episode 80 | Avg Reward 910.0\n",
      "Generation 16 | Episode 100 | Avg Reward 922.2\n",
      "Generation 16 | Episode 120 | Avg Reward 956.4\n",
      "Generation 16 | Episode 140 | Avg Reward 751.6\n",
      "Generation 16 | Episode 160 | Avg Reward 975.1\n",
      "Generation 16 | Episode 180 | Avg Reward 970.9\n",
      "Generation 16 | Episode 200 | Avg Reward 914.9\n",
      "Generation 16 Average Reward: 850.7\n",
      "Generation:  17\n",
      "Generation 17 | Episode 20 | Avg Reward 870.0\n",
      "Generation 17 | Episode 40 | Avg Reward 967.8\n",
      "Generation 17 | Episode 60 | Avg Reward 940.2\n",
      "Generation 17 | Episode 80 | Avg Reward 924.9\n",
      "Generation 17 | Episode 100 | Avg Reward 777.9\n",
      "Generation 17 | Episode 120 | Avg Reward 818.9\n",
      "Generation 17 | Episode 140 | Avg Reward 494.6\n",
      "Generation 17 | Episode 160 | Avg Reward 630.6\n",
      "Generation 17 | Episode 180 | Avg Reward 834.1\n",
      "Generation 17 | Episode 200 | Avg Reward 649.1\n",
      "Generation 17 Average Reward: 790.8\n",
      "Generation:  18\n",
      "Generation 18 | Episode 20 | Avg Reward 730.9\n",
      "Generation 18 | Episode 40 | Avg Reward 848.2\n",
      "Generation 18 | Episode 60 | Avg Reward 946.3\n",
      "Generation 18 | Episode 80 | Avg Reward 934.2\n",
      "Generation 18 | Episode 100 | Avg Reward 754.8\n",
      "Generation 18 | Episode 120 | Avg Reward 958.9\n",
      "Generation 18 | Episode 140 | Avg Reward 941.5\n",
      "Generation 18 | Episode 160 | Avg Reward 933.4\n",
      "Generation 18 | Episode 180 | Avg Reward 933.1\n",
      "Generation 18 | Episode 200 | Avg Reward 1000.0\n",
      "Generation 18 Average Reward: 898.1\n",
      "Generation:  19\n",
      "Generation 19 | Episode 20 | Avg Reward 392.1\n",
      "Generation 19 | Episode 40 | Avg Reward 912.0\n",
      "Generation 19 | Episode 60 | Avg Reward 968.9\n",
      "Generation 19 | Episode 80 | Avg Reward 951.9\n",
      "Generation 19 | Episode 100 | Avg Reward 913.2\n",
      "Generation 19 | Episode 120 | Avg Reward 857.5\n",
      "Generation 19 | Episode 140 | Avg Reward 1000.0\n",
      "Generation 19 | Episode 160 | Avg Reward 517.6\n",
      "Generation 19 | Episode 180 | Avg Reward 58.7\n",
      "Generation 19 | Episode 200 | Avg Reward 223.6\n",
      "Generation 19 Average Reward: 679.6\n",
      "Generation:  20\n",
      "Generation 20 | Episode 20 | Avg Reward 934.1\n",
      "Generation 20 | Episode 40 | Avg Reward 830.9\n",
      "Generation 20 | Episode 60 | Avg Reward 888.1\n",
      "Generation 20 | Episode 80 | Avg Reward 733.2\n",
      "Generation 20 | Episode 100 | Avg Reward 815.8\n",
      "Generation 20 | Episode 120 | Avg Reward 1000.0\n",
      "Generation 20 | Episode 140 | Avg Reward 876.8\n",
      "Generation 20 | Episode 160 | Avg Reward 71.0\n",
      "Generation 20 | Episode 180 | Avg Reward 9.2\n",
      "Generation 20 | Episode 200 | Avg Reward 9.2\n",
      "Generation 20 Average Reward: 616.8\n",
      "Generation:  21\n",
      "Generation 21 | Episode 20 | Avg Reward 854.1\n",
      "Generation 21 | Episode 40 | Avg Reward 1000.0\n",
      "Generation 21 | Episode 60 | Avg Reward 945.8\n",
      "Generation 21 | Episode 80 | Avg Reward 912.2\n",
      "Generation 21 | Episode 100 | Avg Reward 949.8\n",
      "Generation 21 | Episode 120 | Avg Reward 970.6\n",
      "Generation 21 | Episode 140 | Avg Reward 782.6\n",
      "Generation 21 | Episode 160 | Avg Reward 964.5\n",
      "Generation 21 | Episode 180 | Avg Reward 967.1\n",
      "Generation 21 | Episode 200 | Avg Reward 891.1\n",
      "Generation 21 Average Reward: 923.8\n",
      "Generation:  22\n",
      "Generation 22 | Episode 20 | Avg Reward 367.6\n",
      "Generation 22 | Episode 40 | Avg Reward 845.9\n",
      "Generation 22 | Episode 60 | Avg Reward 808.5\n",
      "Generation 22 | Episode 80 | Avg Reward 1000.0\n",
      "Generation 22 | Episode 100 | Avg Reward 863.9\n",
      "Generation 22 | Episode 120 | Avg Reward 952.0\n",
      "Generation 22 | Episode 140 | Avg Reward 847.0\n",
      "Generation 22 | Episode 160 | Avg Reward 962.5\n",
      "Generation 22 | Episode 180 | Avg Reward 1000.0\n",
      "Generation 22 | Episode 200 | Avg Reward 1000.0\n",
      "Generation 22 Average Reward: 864.7\n",
      "Generation:  23\n",
      "Generation 23 | Episode 20 | Avg Reward 416.5\n",
      "Generation 23 | Episode 40 | Avg Reward 815.0\n",
      "Generation 23 | Episode 60 | Avg Reward 1000.0\n",
      "Generation 23 | Episode 80 | Avg Reward 952.1\n",
      "Generation 23 | Episode 100 | Avg Reward 947.0\n",
      "Generation 23 | Episode 120 | Avg Reward 860.5\n",
      "Generation 23 | Episode 140 | Avg Reward 860.4\n",
      "Generation 23 | Episode 160 | Avg Reward 680.2\n",
      "Generation 23 | Episode 180 | Avg Reward 415.6\n",
      "Generation 23 | Episode 200 | Avg Reward 12.1\n",
      "Generation 23 Average Reward: 695.9\n",
      "Generation:  24\n",
      "Generation 24 | Episode 20 | Avg Reward 756.5\n",
      "Generation 24 | Episode 40 | Avg Reward 659.0\n",
      "Generation 24 | Episode 60 | Avg Reward 901.9\n",
      "Generation 24 | Episode 80 | Avg Reward 871.3\n",
      "Generation 24 | Episode 100 | Avg Reward 918.3\n",
      "Generation 24 | Episode 120 | Avg Reward 947.5\n",
      "Generation 24 | Episode 140 | Avg Reward 995.2\n",
      "Generation 24 | Episode 160 | Avg Reward 937.2\n",
      "Generation 24 | Episode 180 | Avg Reward 858.8\n",
      "Generation 24 | Episode 200 | Avg Reward 948.0\n",
      "Generation 24 Average Reward: 879.4\n",
      "Generation:  25\n",
      "Generation 25 | Episode 20 | Avg Reward 655.7\n",
      "Generation 25 | Episode 40 | Avg Reward 817.7\n",
      "Generation 25 | Episode 60 | Avg Reward 962.8\n",
      "Generation 25 | Episode 80 | Avg Reward 978.5\n",
      "Generation 25 | Episode 100 | Avg Reward 990.5\n",
      "Generation 25 | Episode 120 | Avg Reward 948.8\n",
      "Generation 25 | Episode 140 | Avg Reward 404.3\n",
      "Generation 25 | Episode 160 | Avg Reward 12.3\n",
      "Generation 25 | Episode 180 | Avg Reward 21.8\n",
      "Generation 25 | Episode 200 | Avg Reward 32.0\n",
      "Generation 25 Average Reward: 582.4\n",
      "Generation:  26\n",
      "Generation 26 | Episode 20 | Avg Reward 1000.0\n",
      "Generation 26 | Episode 40 | Avg Reward 928.4\n",
      "Generation 26 | Episode 60 | Avg Reward 947.0\n",
      "Generation 26 | Episode 80 | Avg Reward 762.0\n",
      "Generation 26 | Episode 100 | Avg Reward 971.5\n",
      "Generation 26 | Episode 120 | Avg Reward 967.5\n",
      "Generation 26 | Episode 140 | Avg Reward 948.4\n",
      "Generation 26 | Episode 160 | Avg Reward 908.0\n",
      "Generation 26 | Episode 180 | Avg Reward 960.0\n",
      "Generation 26 | Episode 200 | Avg Reward 537.1\n",
      "Generation 26 Average Reward: 893.0\n",
      "Generation:  27\n",
      "Generation 27 | Episode 20 | Avg Reward 572.6\n",
      "Generation 27 | Episode 40 | Avg Reward 661.5\n",
      "Generation 27 | Episode 60 | Avg Reward 825.2\n",
      "Generation 27 | Episode 80 | Avg Reward 835.1\n",
      "Generation 27 | Episode 100 | Avg Reward 960.5\n",
      "Generation 27 | Episode 120 | Avg Reward 912.2\n",
      "Generation 27 | Episode 140 | Avg Reward 982.0\n",
      "Generation 27 | Episode 160 | Avg Reward 1000.0\n",
      "Generation 27 | Episode 180 | Avg Reward 1000.0\n",
      "Generation 27 | Episode 200 | Avg Reward 543.7\n",
      "Generation 27 Average Reward: 829.3\n",
      "Generation:  28\n",
      "Generation 28 | Episode 20 | Avg Reward 948.2\n",
      "Generation 28 | Episode 40 | Avg Reward 1000.0\n",
      "Generation 28 | Episode 60 | Avg Reward 1000.0\n",
      "Generation 28 | Episode 80 | Avg Reward 835.0\n",
      "Generation 28 | Episode 100 | Avg Reward 716.4\n",
      "Generation 28 | Episode 120 | Avg Reward 769.2\n",
      "Generation 28 | Episode 140 | Avg Reward 869.0\n",
      "Generation 28 | Episode 160 | Avg Reward 955.5\n",
      "Generation 28 | Episode 180 | Avg Reward 962.6\n",
      "Generation 28 | Episode 200 | Avg Reward 661.1\n",
      "Generation 28 Average Reward: 871.7\n",
      "Generation:  29\n",
      "Generation 29 | Episode 20 | Avg Reward 832.6\n",
      "Generation 29 | Episode 40 | Avg Reward 748.2\n",
      "Generation 29 | Episode 60 | Avg Reward 883.8\n",
      "Generation 29 | Episode 80 | Avg Reward 915.5\n",
      "Generation 29 | Episode 100 | Avg Reward 962.1\n",
      "Generation 29 | Episode 120 | Avg Reward 960.0\n",
      "Generation 29 | Episode 140 | Avg Reward 942.2\n",
      "Generation 29 | Episode 160 | Avg Reward 919.6\n",
      "Generation 29 | Episode 180 | Avg Reward 778.1\n",
      "Generation 29 | Episode 200 | Avg Reward 719.4\n",
      "Generation 29 Average Reward: 866.2\n",
      "Generation:  30\n",
      "Generation 30 | Episode 20 | Avg Reward 446.4\n",
      "Generation 30 | Episode 40 | Avg Reward 377.7\n",
      "Generation 30 | Episode 60 | Avg Reward 730.8\n",
      "Generation 30 | Episode 80 | Avg Reward 874.8\n",
      "Generation 30 | Episode 100 | Avg Reward 859.2\n",
      "Generation 30 | Episode 120 | Avg Reward 968.7\n",
      "Generation 30 | Episode 140 | Avg Reward 1000.0\n",
      "Generation 30 | Episode 160 | Avg Reward 289.5\n",
      "Generation 30 | Episode 180 | Avg Reward 16.4\n",
      "Generation 30 | Episode 200 | Avg Reward 110.4\n",
      "Generation 30 Average Reward: 567.4\n",
      "Generation:  31\n",
      "Generation 31 | Episode 20 | Avg Reward 927.8\n",
      "Generation 31 | Episode 40 | Avg Reward 1000.0\n",
      "Generation 31 | Episode 60 | Avg Reward 994.4\n",
      "Generation 31 | Episode 80 | Avg Reward 392.1\n",
      "Generation 31 | Episode 100 | Avg Reward 354.0\n",
      "Generation 31 | Episode 120 | Avg Reward 726.0\n",
      "Generation 31 | Episode 140 | Avg Reward 887.2\n",
      "Generation 31 | Episode 160 | Avg Reward 924.1\n",
      "Generation 31 | Episode 180 | Avg Reward 784.0\n",
      "Generation 31 | Episode 200 | Avg Reward 964.8\n",
      "Generation 31 Average Reward: 795.4\n",
      "Generation:  32\n",
      "Generation 32 | Episode 20 | Avg Reward 419.8\n",
      "Generation 32 | Episode 40 | Avg Reward 436.2\n",
      "Generation 32 | Episode 60 | Avg Reward 836.5\n",
      "Generation 32 | Episode 80 | Avg Reward 781.5\n",
      "Generation 32 | Episode 100 | Avg Reward 859.0\n",
      "Generation 32 | Episode 120 | Avg Reward 831.1\n",
      "Generation 32 | Episode 140 | Avg Reward 564.3\n",
      "Generation 32 | Episode 160 | Avg Reward 15.4\n",
      "Generation 32 | Episode 180 | Avg Reward 147.1\n",
      "Generation 32 | Episode 200 | Avg Reward 181.5\n",
      "Generation 32 Average Reward: 507.2\n",
      "Generation:  33\n",
      "Generation 33 | Episode 20 | Avg Reward 417.8\n",
      "Generation 33 | Episode 40 | Avg Reward 951.6\n",
      "Generation 33 | Episode 60 | Avg Reward 906.2\n",
      "Generation 33 | Episode 80 | Avg Reward 863.9\n",
      "Generation 33 | Episode 100 | Avg Reward 933.1\n",
      "Generation 33 | Episode 120 | Avg Reward 871.8\n",
      "Generation 33 | Episode 140 | Avg Reward 223.6\n",
      "Generation 33 | Episode 160 | Avg Reward 678.4\n",
      "Generation 33 | Episode 180 | Avg Reward 959.6\n",
      "Generation 33 | Episode 200 | Avg Reward 390.7\n",
      "Generation 33 Average Reward: 719.7\n",
      "Generation:  34\n",
      "Generation 34 | Episode 20 | Avg Reward 396.9\n",
      "Generation 34 | Episode 40 | Avg Reward 571.5\n",
      "Generation 34 | Episode 60 | Avg Reward 963.0\n",
      "Generation 34 | Episode 80 | Avg Reward 945.4\n",
      "Generation 34 | Episode 100 | Avg Reward 947.0\n",
      "Generation 34 | Episode 120 | Avg Reward 1000.0\n",
      "Generation 34 | Episode 140 | Avg Reward 1000.0\n",
      "Generation 34 | Episode 160 | Avg Reward 986.5\n",
      "Generation 34 | Episode 180 | Avg Reward 789.3\n",
      "Generation 34 | Episode 200 | Avg Reward 420.8\n",
      "Generation 34 Average Reward: 802.0\n",
      "Generation:  35\n",
      "Generation 35 | Episode 20 | Avg Reward 532.8\n",
      "Generation 35 | Episode 40 | Avg Reward 820.6\n",
      "Generation 35 | Episode 60 | Avg Reward 943.5\n",
      "Generation 35 | Episode 80 | Avg Reward 868.7\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import random\n",
    "\n",
    "sns.set()\n",
    "\n",
    "class ActorCriticNN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,action_dim))\n",
    "\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1))\n",
    "\n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "\n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value\n",
    "\n",
    "#trainer\n",
    "class PPOTrainer():\n",
    "    def __init__(self, actor_critic, ppo_clip_val = 0.2, target_kl_div = 0.01, max_policy_train_iters = 80, value_train_iters=80, \n",
    "                policy_lr = 3e-4, value_lr = 1e-2):\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_val = ppo_clip_val\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.value_train_iters = value_train_iters\n",
    "    \n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr = policy_lr)\n",
    "    \n",
    "        value_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr = value_lr)\n",
    "\n",
    "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "\n",
    "        for _ in range(self.max_policy_train_iters):\n",
    "                \n",
    "            self.policy_optim.zero_grad()\n",
    "    \n",
    "            new_logits = self.ac.policy(obs)\n",
    "            new_logits = Categorical(logits = new_logits)\n",
    "            new_log_probs = new_logits.log_prob(acts)\n",
    "    \n",
    "            policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            clipped_ratio = policy_ratio.clamp(1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "            clipped_loss = clipped_ratio * gaes\n",
    "            full_loss = policy_ratio * gaes\n",
    "            policy_loss = -torch.min(full_loss, clipped_loss).mean()\n",
    "    \n",
    "            policy_loss.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "            kl_div = (old_log_probs - new_log_probs).mean()\n",
    "            if kl_div >= self.target_kl_div:\n",
    "                break\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "\n",
    "        for _ in range (self.value_train_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "    \n",
    "            values = self.ac.value(obs)\n",
    "            value_loss = (returns-values) ** 2\n",
    "            value_loss = value_loss.mean()\n",
    "    \n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()\n",
    "\n",
    "def create_population(pop_size, state_dim, action_dim):\n",
    "    return [ActorCriticNN(state_dim, action_dim) for _ in range(pop_size)]\n",
    "\n",
    "def evaluate_population(population, env, max_steps=100):\n",
    "    fitness_scores = []\n",
    "    for policy in population:\n",
    "        total_reward = 0\n",
    "        obs, _ = env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "            logits, _ = policy(obs_tensor)\n",
    "            act_dist = Categorical(logits=logits)\n",
    "            action = act_dist.sample()\n",
    "            next_obs, reward, done, _, __ = env.step(action.item())\n",
    "            total_reward += reward\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n",
    "        fitness_scores.append(total_reward)\n",
    "    return fitness_scores\n",
    "\n",
    "def select_parents(population, fitness_scores, num_parents):\n",
    "   # Get indices that would sort the fitness scores in descending order\n",
    "    sorted_indices = np.argsort(fitness_scores)[::-1]  # Reverse for descending order\n",
    "    \n",
    "    # Select the best parents based on the sorted indices\n",
    "    parents = np.array(population)[sorted_indices][:num_parents]\n",
    "    return parents.tolist()\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    child = ActorCriticNN(state_dim, action_dim)\n",
    "    for param1, param2, param_child in zip(parent1.parameters(), parent2.parameters(), child.parameters()):\n",
    "        param_child.data = (param1.data + param2.data) / 2  # Average weights\n",
    "    return child\n",
    "\n",
    "def mutate(policy, mutation_rate=0.01):\n",
    "    for param in policy.parameters():\n",
    "        if random.random() < mutation_rate:\n",
    "            param.data += torch.randn(param.size()) * 0.1  # Add small random noise\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Return discounted rewards based on the given rewards and the gamma param.\n",
    "    \"\"\"\n",
    "\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma = 0.99, decay = 0.97):\n",
    "    \"\"\"\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/abs/1506.02438\n",
    "    \"\"\"\n",
    "\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
    "\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "    return np.array(gaes[::-1])\n",
    "\n",
    "def rollout(model, env, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Performs a single rollout, sampling an action and collecting data.\n",
    "    Returns training data in the shape (n_steps, observation_shape) and the cumulative reward.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the data storage (obs, act, reward, values, act_log_probs)\n",
    "    train_data = [[], [], [], [], []]  \n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    #print(\"Environment reset, initial observation:\", obs)  # Check if env.reset() works\n",
    "\n",
    "    ep_reward = 0\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        #Convert observation to a PyTorch tensor and ensure correct shape\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "        logits, val = model(obs)\n",
    "\n",
    "        # Create action distribution and sample an action\n",
    "        act_dist = Categorical(logits=logits)\n",
    "        act = act_dist.sample()\n",
    "        act_log_prob = act_dist.log_prob(act).item()\n",
    "\n",
    "        act, val = act.item(), val.item()\n",
    "\n",
    "        # Take action in the environment\n",
    "        next_obs, reward, done, _, __ = env.step(act)\n",
    "\n",
    "        # Store the data for training\n",
    "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
    "            train_data[i].append(item)\n",
    "\n",
    "        #print(f\"Action taken: {act.item()}, Reward received: {reward}, Done: {done}\")\n",
    "\n",
    "        # Update observation and cumulative reward\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "\n",
    "        # Break if the episode ends\n",
    "        if done:\n",
    "            #print(f\"Episode ended after {step+1} steps with cumulative reward: {ep_reward}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "    \n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3])\n",
    "    \n",
    "    return train_data, ep_reward\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Define the environment and parameters\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "n_episodes = 200\n",
    "print_freq = 20\n",
    "pop_size = 30\n",
    "num_parents = 10\n",
    "num_generations = 50\n",
    "\n",
    "# Initialize the population\n",
    "population = create_population(pop_size, state_dim, action_dim)\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    # Evaluate the current population\n",
    "    fitness_scores = evaluate_population(population, env)\n",
    "    \n",
    "    # Select the best parents\n",
    "    parents = select_parents(population, fitness_scores, num_parents)\n",
    "\n",
    "    next_generation = []\n",
    "    \n",
    "    # Print the generation number\n",
    "    print(\"Generation: \", generation + 1)\n",
    "\n",
    "    # Create a selective parents list for crossover\n",
    "    selective_parents = [parents[0], parents[1], parents[2], parents[3], parents[4]]\n",
    "\n",
    "    best_select = []\n",
    "    \n",
    "    for i in range(num_parents):\n",
    "        parent1, parent2 = np.random.choice(selective_parents, 2, replace=False)\n",
    "        child = crossover(parent1, parent2)\n",
    "        mutate(child)\n",
    "        best_select.append(child)\n",
    "\n",
    "    for i in range(pop_size):\n",
    "        next_generation.append(population[i])\n",
    "\n",
    "    # Evaluate fitness of the new generation\n",
    "    next_generation_fitness = evaluate_population(next_generation, env)\n",
    "    \n",
    "    # Sort the next generation based on fitness scores (best to worst)\n",
    "    next_generation = [x for _, x in sorted(zip(next_generation_fitness, next_generation), key=lambda pair: pair[0], reverse=True)]\n",
    "\n",
    "    index = 0\n",
    "    for i in range ((pop_size-10),pop_size):\n",
    "        next_generation[i] = best_select[index]\n",
    "        index = index + 1\n",
    "    \n",
    "    population = next_generation\n",
    "\n",
    "    # Optionally fine-tune the best policy with PPO after EA\n",
    "    best_policy = parents[0]  # Select the best policy for further training with PPO\n",
    "    ppo = PPOTrainer(best_policy, policy_lr=3e-4, value_lr=1e-3, target_kl_div=0.02, max_policy_train_iters=40, value_train_iters=40)\n",
    "\n",
    "    ep_rewards = []\n",
    "    for episode_idx in range(n_episodes):\n",
    "        # Perform rollout\n",
    "        train_data, reward = rollout(best_policy, env)\n",
    "        ep_rewards.append(reward)\n",
    "\n",
    "        permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "        obs = torch.tensor(train_data[0][permute_idxs], dtype=torch.float32)\n",
    "        act = torch.tensor(train_data[1][permute_idxs], dtype=torch.int32)\n",
    "        gaes = torch.tensor(train_data[3][permute_idxs], dtype=torch.float32)\n",
    "        act_log_probs = torch.tensor(train_data[4][permute_idxs], dtype=torch.float32)\n",
    "\n",
    "        # Value Data\n",
    "        returns = discount_rewards(train_data[2])[permute_idxs]\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        # Train Policy\n",
    "        ppo.train_policy(obs, act, act_log_probs, gaes)\n",
    "        ppo.train_value(obs, returns)\n",
    "\n",
    "        # Print average reward every 'print_freq' episodes\n",
    "        if (episode_idx + 1) % print_freq == 0:\n",
    "            avg_reward = np.mean(ep_rewards[-print_freq:])  # Calculate the average of the last 'print_freq' rewards\n",
    "            print('Generation {} | Episode {} | Avg Reward {:.1f}'.format(\n",
    "                generation + 1, episode_idx + 1, avg_reward))\n",
    "\n",
    "    # Calculate and print the overall average reward for this generation\n",
    "    generation_avg_reward = np.mean(ep_rewards)  # Calculate average for all episodes in this generation\n",
    "    print(\"Generation {} Average Reward: {:.1f}\".format(generation + 1, generation_avg_reward))\n",
    "\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d4229-9567-4287-b2d3-375145f2360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three Things I tried\n",
    "\"\"\"\n",
    "First I tried random parents from the parents array and using them to create a child and constructing the new generation as the offspring\n",
    "\n",
    "Second I tried choosing the best 5-6 parents and then using their children to populate the next generation\n",
    "\n",
    "Third I used the best 5-6 parents to make 10 children, then I copied the current population into next_gen and then I ordered next gen in\n",
    "order of best policy to worst, and then I replaced the last 10 with the 10 children that I produced\n",
    "\n",
    "\"\"\"\n",
    "# I NEED TO TEST THIS SOMEWHERE WHERE IT DOESNT OVERFIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2430982-5c3c-4083-8649-c5c1f7d219c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import random\n",
    "\n",
    "sns.set()\n",
    "\n",
    "class ActorCriticNN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,action_dim))\n",
    "\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1))\n",
    "\n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "\n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value\n",
    "\n",
    "#trainer\n",
    "class PPOTrainer():\n",
    "    def __init__(self, actor_critic, ppo_clip_val, target_kl_div, policy_lr, value_lr, max_policy_train_iters = 80, value_train_iters = 80):\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_val = ppo_clip_val\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.value_train_iters = value_train_iters\n",
    "    \n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr = policy_lr)\n",
    "    \n",
    "        value_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr = value_lr)\n",
    "\n",
    "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "\n",
    "        for _ in range(self.max_policy_train_iters):\n",
    "                \n",
    "            self.policy_optim.zero_grad()\n",
    "    \n",
    "            new_logits = self.ac.policy(obs)\n",
    "            new_logits = Categorical(logits = new_logits)\n",
    "            new_log_probs = new_logits.log_prob(acts)\n",
    "    \n",
    "            policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            clipped_ratio = policy_ratio.clamp(1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "            clipped_loss = clipped_ratio * gaes\n",
    "            full_loss = policy_ratio * gaes\n",
    "            policy_loss = -torch.min(full_loss, clipped_loss).mean()\n",
    "    \n",
    "            policy_loss.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "            kl_div = (old_log_probs - new_log_probs).mean()\n",
    "            if kl_div >= self.target_kl_div:\n",
    "                break\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "\n",
    "        for _ in range (self.value_train_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "    \n",
    "            values = self.ac.value(obs)\n",
    "            value_loss = (returns-values) ** 2\n",
    "            value_loss = value_loss.mean()\n",
    "    \n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()\n",
    "\n",
    "class ActorCriticChromosome:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        # Actor-Critic neural network\n",
    "        self.model = ActorCriticNN(state_dim, action_dim)\n",
    "\n",
    "        #Hyperparameters\n",
    "        self.policy_lr = random.uniform(1e-5, 1e-2)  # Expanded range\n",
    "        self.value_lr = random.uniform(1e-5, 1e-2)  # Expanded range\n",
    "        self.ppo_clip_val = random.uniform(0.1, 0.4)  # Expanded range\n",
    "        self.target_kl_div = random.uniform(0.001, 0.05)\n",
    "\n",
    "def create_population(pop_size, state_dim, action_dim):\n",
    "    return [ActorCriticChromosome(state_dim, action_dim) for _ in range(pop_size)]\n",
    "\n",
    "\"\"\"\n",
    "When initializing PPOTrainer, you're passing the individual's hyperparameters (policy_lr, value_lr, etc.) \n",
    "to ensure the evaluation process reflects the effect of those hyperparameters.\n",
    "You're sampling actions from the model and accumulating rewards to calculate the fitness score.\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_population(population, env, max_steps=100):\n",
    "    fitness_scores = []\n",
    "    for individual in population:\n",
    "        total_reward = 0\n",
    "\n",
    "        # Initialize PPOTrainer with individual's hyperparameters\n",
    "        ppo = PPOTrainer(individual.model,\n",
    "                         ppo_clip_val=individual.ppo_clip_val,\n",
    "                         target_kl_div=individual.target_kl_div,\n",
    "                         policy_lr=individual.policy_lr,\n",
    "                         value_lr=individual.value_lr)\n",
    "\n",
    "        # Reset the environment and evaluate the model\n",
    "        obs, _ = env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "            logits, _ = individual.model(obs_tensor)  # Use the model from the individual\n",
    "            act_dist = Categorical(logits=logits)\n",
    "            action = act_dist.sample()\n",
    "            next_obs, reward, done, _, __ = env.step(action.item())\n",
    "            total_reward += reward\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n",
    "        fitness_scores.append(total_reward)\n",
    "    \n",
    "    return fitness_scores\n",
    "\n",
    "def select_parents(population, fitness_scores, num_parents):\n",
    "   # Get indices that would sort the fitness scores in descending order\n",
    "    sorted_indices = np.argsort(fitness_scores)[::-1]  # Reverse for descending order\n",
    "    \n",
    "    # Select the best parents based on the sorted indices\n",
    "    parents = np.array(population)[sorted_indices][:num_parents]\n",
    "    return parents.tolist()\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    child = ActorCriticChromosome(state_dim, action_dim)\n",
    "    for param1, param2, param_child in zip(parent1.model.parameters(), parent2.model.parameters(), child.model.parameters()):\n",
    "        param_child.data = (param1.data + param2.data) / 2  # Average weights\n",
    "\n",
    "    # Crossover for hyperparameters\n",
    "    child.policy_lr = (parent1.policy_lr + parent2.policy_lr) / 2\n",
    "    child.value_lr = (parent1.value_lr + parent2.value_lr) / 2\n",
    "    child.ppo_clip_val = (parent1.ppo_clip_val + parent2.ppo_clip_val) / 2\n",
    "    child.target_kl_div = (parent1.target_kl_div + parent2.target_kl_div) / 2\n",
    "    \n",
    "    return child\n",
    "\n",
    "def mutate(individual, mutation_rate=0.01):\n",
    "    for param in individual.model.parameters():\n",
    "        if random.random() < mutation_rate:\n",
    "            param.data += torch.randn(param.size()) * 0.1  # Add small random noise\n",
    "    \n",
    "    # Mutate hyperparameters\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.policy_lr += random.uniform(-1e-5, 1e-5)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.value_lr += random.uniform(-1e-4, 1e-4)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.ppo_clip_val += random.uniform(-0.01, 0.01)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.target_kl_div += random.uniform(-0.001, 0.001)\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Return discounted rewards based on the given rewards and the gamma param.\n",
    "    \"\"\"\n",
    "\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma = 0.99, decay = 0.97):\n",
    "    \"\"\"\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/abs/1506.02438\n",
    "    \"\"\"\n",
    "\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
    "\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "    return np.array(gaes[::-1])\n",
    "\n",
    "def rollout(model, env, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Performs a single rollout, sampling an action and collecting data.\n",
    "    Returns training data in the shape (n_steps, observation_shape) and the cumulative reward.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the data storage (obs, act, reward, values, act_log_probs)\n",
    "    train_data = [[], [], [], [], []]  \n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    #print(\"Environment reset, initial observation:\", obs)  # Check if env.reset() works\n",
    "\n",
    "    ep_reward = 0\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        #Convert observation to a PyTorch tensor and ensure correct shape\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "        logits, val = model(obs)\n",
    "\n",
    "        # Create action distribution and sample an action\n",
    "        act_dist = Categorical(logits=logits)\n",
    "        act = act_dist.sample()\n",
    "        act_log_prob = act_dist.log_prob(act).item()\n",
    "\n",
    "        act, val = act.item(), val.item()\n",
    "\n",
    "        # Take action in the environment\n",
    "        next_obs, reward, done, _, __ = env.step(act)\n",
    "\n",
    "        # Store the data for training\n",
    "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
    "            train_data[i].append(item)\n",
    "\n",
    "        #print(f\"Action taken: {act.item()}, Reward received: {reward}, Done: {done}\")\n",
    "\n",
    "        # Update observation and cumulative reward\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "\n",
    "        # Break if the episode ends\n",
    "        if done:\n",
    "            #print(f\"Episode ended after {step+1} steps with cumulative reward: {ep_reward}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "    \n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3])\n",
    "    \n",
    "    return train_data, ep_reward\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Define the environment and parameters\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "n_episodes = 200\n",
    "print_freq = 20\n",
    "pop_size = 30\n",
    "num_parents = 10\n",
    "num_generations = 50\n",
    "\n",
    "# Initialize the population\n",
    "population = create_population(pop_size, state_dim, action_dim)\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    # Evaluate the current population\n",
    "    fitness_scores = evaluate_population(population, env)\n",
    "    \n",
    "    # Select the best parents\n",
    "    parents = select_parents(population, fitness_scores, num_parents)\n",
    "\n",
    "    next_generation = []\n",
    "    \n",
    "    # Print the generation number\n",
    "    print(\"Generation: \", generation + 1)\n",
    "\n",
    "    # Create a selective parents list for crossover\n",
    "    selective_parents = [parents[0], parents[1], parents[2], parents[3], parents[4]]\n",
    "\n",
    "    best_select = []\n",
    "    \n",
    "    for i in range(num_parents):\n",
    "        parent1, parent2 = np.random.choice(selective_parents, 2, replace=False)\n",
    "        child = crossover(parent1, parent2)\n",
    "        mutate(child)\n",
    "        best_select.append(child)\n",
    "\n",
    "    for i in range(pop_size):\n",
    "        next_generation.append(population[i])\n",
    "\n",
    "    # Evaluate fitness of the new generation\n",
    "    next_generation_fitness = evaluate_population(next_generation, env)\n",
    "    \n",
    "    # Sort the next generation based on fitness scores (best to worst)\n",
    "    next_generation = [x for _, x in sorted(zip(next_generation_fitness, next_generation), key=lambda pair: pair[0], reverse=True)]\n",
    "\n",
    "    index = 0\n",
    "    for i in range ((pop_size-10),pop_size):\n",
    "        next_generation[i] = best_select[index]\n",
    "        index = index + 1\n",
    "    \n",
    "    population = next_generation\n",
    "\n",
    "    # Optionally fine-tune the best policy with PPO after EA\n",
    "    best_policy = parents[0]  # Select the best policy for further training with PPO\n",
    "    ppo = PPOTrainer(best_policy.model, ppo_clip_val = best_policy.ppo_clip_val, target_kl_div=best_policy.target_kl_div, \n",
    "                     policy_lr=best_policy.policy_lr, value_lr=best_policy.value_lr)\n",
    "\n",
    "    ep_rewards = []\n",
    "    for episode_idx in range(n_episodes):\n",
    "        # Perform rollout\n",
    "        train_data, reward = rollout(best_policy.model, env)\n",
    "        ep_rewards.append(reward)\n",
    "\n",
    "        permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "        obs = torch.tensor(train_data[0][permute_idxs], dtype=torch.float32)\n",
    "        act = torch.tensor(train_data[1][permute_idxs], dtype=torch.int32)\n",
    "        gaes = torch.tensor(train_data[3][permute_idxs], dtype=torch.float32)\n",
    "        act_log_probs = torch.tensor(train_data[4][permute_idxs], dtype=torch.float32)\n",
    "\n",
    "        # Value Data\n",
    "        returns = discount_rewards(train_data[2])[permute_idxs]\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        # Train Policy\n",
    "        ppo.train_policy(obs, act, act_log_probs, gaes)\n",
    "        ppo.train_value(obs, returns)\n",
    "\n",
    "        # Print average reward every 'print_freq' episodes\n",
    "        if (episode_idx + 1) % print_freq == 0:\n",
    "            avg_reward = np.mean(ep_rewards[-print_freq:])  # Calculate the average of the last 'print_freq' rewards\n",
    "            print('Generation {} | Episode {} | Avg Reward {:.1f}'.format(\n",
    "                generation + 1, episode_idx + 1, avg_reward))\n",
    "\n",
    "    # Calculate and print the overall average reward for this generation\n",
    "    generation_avg_reward = np.mean(ep_rewards)  # Calculate average for all episodes in this generation\n",
    "    print(\"Generation {} Average Reward: {:.1f}\".format(generation + 1, generation_avg_reward))\n",
    "\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681ec8e-4785-4b24-870d-97fb01189205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  1\n",
      "Generation 1 | Episode 20 | Avg Reward -434.6\n",
      "Generation 1 | Episode 40 | Avg Reward -627.1\n",
      "Generation 1 | Episode 60 | Avg Reward -597.1\n",
      "Generation 1 | Episode 80 | Avg Reward -165.2\n",
      "Generation 1 | Episode 100 | Avg Reward -170.1\n",
      "Generation 1 | Episode 120 | Avg Reward -139.0\n",
      "Generation 1 | Episode 140 | Avg Reward -128.0\n",
      "Generation 1 | Episode 160 | Avg Reward -172.4\n",
      "Generation 1 | Episode 180 | Avg Reward -140.9\n",
      "Generation 1 | Episode 200 | Avg Reward -355.5\n",
      "Generation 1 Average Reward: -293.0\n",
      "Generation:  2\n",
      "Generation 2 | Episode 20 | Avg Reward -213.3\n",
      "Generation 2 | Episode 40 | Avg Reward -525.5\n",
      "Generation 2 | Episode 60 | Avg Reward -953.9\n",
      "Generation 2 | Episode 80 | Avg Reward -934.9\n",
      "Generation 2 | Episode 100 | Avg Reward -750.5\n",
      "Generation 2 | Episode 120 | Avg Reward -1027.6\n",
      "Generation 2 | Episode 140 | Avg Reward -300.9\n",
      "Generation 2 | Episode 160 | Avg Reward -123.1\n",
      "Generation 2 | Episode 180 | Avg Reward -703.5\n",
      "Generation 2 | Episode 200 | Avg Reward -796.1\n",
      "Generation 2 Average Reward: -632.9\n",
      "Generation:  3\n",
      "Generation 3 | Episode 20 | Avg Reward -157.7\n",
      "Generation 3 | Episode 40 | Avg Reward -663.3\n",
      "Generation 3 | Episode 60 | Avg Reward -684.1\n",
      "Generation 3 | Episode 80 | Avg Reward -776.9\n",
      "Generation 3 | Episode 100 | Avg Reward -839.4\n",
      "Generation 3 | Episode 120 | Avg Reward -1092.3\n",
      "Generation 3 | Episode 140 | Avg Reward -692.0\n",
      "Generation 3 | Episode 160 | Avg Reward -863.0\n",
      "Generation 3 | Episode 180 | Avg Reward -833.0\n",
      "Generation 3 | Episode 200 | Avg Reward -919.5\n",
      "Generation 3 Average Reward: -752.1\n",
      "Generation:  4\n",
      "Generation 4 | Episode 20 | Avg Reward -194.0\n",
      "Generation 4 | Episode 40 | Avg Reward -416.6\n",
      "Generation 4 | Episode 60 | Avg Reward -746.2\n",
      "Generation 4 | Episode 80 | Avg Reward -870.0\n",
      "Generation 4 | Episode 100 | Avg Reward -714.4\n",
      "Generation 4 | Episode 120 | Avg Reward -745.2\n",
      "Generation 4 | Episode 140 | Avg Reward -905.7\n",
      "Generation 4 | Episode 160 | Avg Reward -937.3\n",
      "Generation 4 | Episode 180 | Avg Reward -909.6\n",
      "Generation 4 | Episode 200 | Avg Reward -861.1\n",
      "Generation 4 Average Reward: -730.0\n",
      "Generation:  5\n",
      "Generation 5 | Episode 20 | Avg Reward -465.3\n",
      "Generation 5 | Episode 40 | Avg Reward -764.4\n",
      "Generation 5 | Episode 60 | Avg Reward -899.4\n",
      "Generation 5 | Episode 80 | Avg Reward -875.7\n",
      "Generation 5 | Episode 100 | Avg Reward -626.9\n",
      "Generation 5 | Episode 120 | Avg Reward -619.2\n",
      "Generation 5 | Episode 140 | Avg Reward -405.4\n",
      "Generation 5 | Episode 160 | Avg Reward -206.1\n",
      "Generation 5 | Episode 180 | Avg Reward -509.0\n",
      "Generation 5 | Episode 200 | Avg Reward -587.4\n",
      "Generation 5 Average Reward: -595.9\n",
      "Generation:  6\n",
      "Generation 6 | Episode 20 | Avg Reward -660.0\n",
      "Generation 6 | Episode 40 | Avg Reward -685.3\n",
      "Generation 6 | Episode 60 | Avg Reward -205.3\n",
      "Generation 6 | Episode 80 | Avg Reward -226.4\n",
      "Generation 6 | Episode 100 | Avg Reward -454.9\n",
      "Generation 6 | Episode 120 | Avg Reward -724.7\n",
      "Generation 6 | Episode 140 | Avg Reward -780.9\n",
      "Generation 6 | Episode 160 | Avg Reward -748.6\n",
      "Generation 6 | Episode 180 | Avg Reward -1007.1\n",
      "Generation 6 | Episode 200 | Avg Reward -843.9\n",
      "Generation 6 Average Reward: -633.7\n",
      "Generation:  7\n",
      "Generation 7 | Episode 20 | Avg Reward -639.8\n",
      "Generation 7 | Episode 40 | Avg Reward -570.1\n",
      "Generation 7 | Episode 60 | Avg Reward -484.5\n",
      "Generation 7 | Episode 80 | Avg Reward -638.7\n",
      "Generation 7 | Episode 100 | Avg Reward -560.3\n",
      "Generation 7 | Episode 120 | Avg Reward -544.8\n",
      "Generation 7 | Episode 140 | Avg Reward -582.2\n",
      "Generation 7 | Episode 160 | Avg Reward -604.1\n",
      "Generation 7 | Episode 180 | Avg Reward -528.9\n",
      "Generation 7 | Episode 200 | Avg Reward -610.0\n",
      "Generation 7 Average Reward: -576.3\n",
      "Generation:  8\n",
      "Generation 8 | Episode 20 | Avg Reward -168.7\n",
      "Generation 8 | Episode 40 | Avg Reward -318.3\n",
      "Generation 8 | Episode 60 | Avg Reward -533.8\n",
      "Generation 8 | Episode 80 | Avg Reward -575.6\n",
      "Generation 8 | Episode 100 | Avg Reward -628.0\n",
      "Generation 8 | Episode 120 | Avg Reward -585.8\n",
      "Generation 8 | Episode 140 | Avg Reward -592.0\n",
      "Generation 8 | Episode 160 | Avg Reward -534.2\n",
      "Generation 8 | Episode 180 | Avg Reward -599.3\n",
      "Generation 8 | Episode 200 | Avg Reward -584.5\n",
      "Generation 8 Average Reward: -512.0\n",
      "Generation:  9\n",
      "Generation 9 | Episode 20 | Avg Reward -320.9\n",
      "Generation 9 | Episode 40 | Avg Reward -163.9\n",
      "Generation 9 | Episode 60 | Avg Reward -144.3\n",
      "Generation 9 | Episode 80 | Avg Reward -179.8\n",
      "Generation 9 | Episode 100 | Avg Reward -131.2\n",
      "Generation 9 | Episode 120 | Avg Reward -409.2\n",
      "Generation 9 | Episode 140 | Avg Reward -126.6\n",
      "Generation 9 | Episode 160 | Avg Reward -294.0\n",
      "Generation 9 | Episode 180 | Avg Reward -540.8\n",
      "Generation 9 | Episode 200 | Avg Reward -556.4\n",
      "Generation 9 Average Reward: -286.7\n",
      "Generation:  10\n",
      "Generation 10 | Episode 20 | Avg Reward -597.7\n",
      "Generation 10 | Episode 40 | Avg Reward -573.5\n",
      "Generation 10 | Episode 60 | Avg Reward -590.7\n",
      "Generation 10 | Episode 80 | Avg Reward -629.2\n",
      "Generation 10 | Episode 100 | Avg Reward -622.5\n",
      "Generation 10 | Episode 120 | Avg Reward -486.0\n",
      "Generation 10 | Episode 140 | Avg Reward -147.6\n",
      "Generation 10 | Episode 160 | Avg Reward -165.5\n",
      "Generation 10 | Episode 180 | Avg Reward -564.4\n",
      "Generation 10 | Episode 200 | Avg Reward -601.3\n",
      "Generation 10 Average Reward: -497.8\n",
      "Generation:  11\n",
      "Generation 11 | Episode 20 | Avg Reward -152.8\n",
      "Generation 11 | Episode 40 | Avg Reward -553.3\n",
      "Generation 11 | Episode 60 | Avg Reward -576.4\n",
      "Generation 11 | Episode 80 | Avg Reward -568.5\n",
      "Generation 11 | Episode 100 | Avg Reward -722.3\n",
      "Generation 11 | Episode 120 | Avg Reward -825.5\n",
      "Generation 11 | Episode 140 | Avg Reward -736.9\n",
      "Generation 11 | Episode 160 | Avg Reward -735.8\n",
      "Generation 11 | Episode 180 | Avg Reward -852.5\n",
      "Generation 11 | Episode 200 | Avg Reward -682.5\n",
      "Generation 11 Average Reward: -640.7\n",
      "Generation:  12\n",
      "Generation 12 | Episode 20 | Avg Reward -504.0\n",
      "Generation 12 | Episode 40 | Avg Reward -703.3\n",
      "Generation 12 | Episode 60 | Avg Reward -389.2\n",
      "Generation 12 | Episode 80 | Avg Reward -279.3\n",
      "Generation 12 | Episode 100 | Avg Reward -668.2\n",
      "Generation 12 | Episode 120 | Avg Reward -359.3\n",
      "Generation 12 | Episode 140 | Avg Reward -403.7\n",
      "Generation 12 | Episode 160 | Avg Reward -792.2\n",
      "Generation 12 | Episode 180 | Avg Reward -869.6\n",
      "Generation 12 | Episode 200 | Avg Reward -918.0\n",
      "Generation 12 Average Reward: -588.7\n",
      "Generation:  13\n",
      "Generation 13 | Episode 20 | Avg Reward -247.8\n",
      "Generation 13 | Episode 40 | Avg Reward -751.5\n",
      "Generation 13 | Episode 60 | Avg Reward -526.3\n",
      "Generation 13 | Episode 80 | Avg Reward -220.7\n",
      "Generation 13 | Episode 100 | Avg Reward -208.8\n",
      "Generation 13 | Episode 120 | Avg Reward -121.7\n",
      "Generation 13 | Episode 140 | Avg Reward -136.9\n",
      "Generation 13 | Episode 160 | Avg Reward -141.6\n",
      "Generation 13 | Episode 180 | Avg Reward -146.0\n",
      "Generation 13 | Episode 200 | Avg Reward -136.0\n",
      "Generation 13 Average Reward: -263.7\n",
      "Generation:  14\n",
      "Generation 14 | Episode 20 | Avg Reward -132.6\n",
      "Generation 14 | Episode 40 | Avg Reward -130.2\n",
      "Generation 14 | Episode 60 | Avg Reward -425.9\n",
      "Generation 14 | Episode 80 | Avg Reward -871.5\n",
      "Generation 14 | Episode 100 | Avg Reward -882.5\n",
      "Generation 14 | Episode 120 | Avg Reward -858.2\n",
      "Generation 14 | Episode 140 | Avg Reward -994.1\n",
      "Generation 14 | Episode 160 | Avg Reward -697.8\n",
      "Generation 14 | Episode 180 | Avg Reward -734.6\n",
      "Generation 14 | Episode 200 | Avg Reward -890.9\n",
      "Generation 14 Average Reward: -661.8\n",
      "Generation:  15\n",
      "Generation 15 | Episode 20 | Avg Reward -718.1\n",
      "Generation 15 | Episode 40 | Avg Reward -791.1\n",
      "Generation 15 | Episode 60 | Avg Reward -510.0\n",
      "Generation 15 | Episode 80 | Avg Reward -571.4\n",
      "Generation 15 | Episode 100 | Avg Reward -765.9\n",
      "Generation 15 | Episode 120 | Avg Reward -743.2\n",
      "Generation 15 | Episode 140 | Avg Reward -348.3\n",
      "Generation 15 | Episode 160 | Avg Reward -224.4\n",
      "Generation 15 | Episode 180 | Avg Reward -243.2\n",
      "Generation 15 | Episode 200 | Avg Reward -155.5\n",
      "Generation 15 Average Reward: -507.1\n",
      "Generation:  16\n",
      "Generation 16 | Episode 20 | Avg Reward -145.2\n",
      "Generation 16 | Episode 40 | Avg Reward -205.7\n",
      "Generation 16 | Episode 60 | Avg Reward -522.1\n",
      "Generation 16 | Episode 80 | Avg Reward -399.0\n",
      "Generation 16 | Episode 100 | Avg Reward -705.4\n",
      "Generation 16 | Episode 120 | Avg Reward -879.6\n",
      "Generation 16 | Episode 140 | Avg Reward -654.1\n",
      "Generation 16 | Episode 160 | Avg Reward -764.5\n",
      "Generation 16 | Episode 180 | Avg Reward -549.0\n",
      "Generation 16 | Episode 200 | Avg Reward -597.7\n",
      "Generation 16 Average Reward: -542.2\n",
      "Generation:  17\n",
      "Generation 17 | Episode 20 | Avg Reward -530.6\n",
      "Generation 17 | Episode 40 | Avg Reward -526.2\n",
      "Generation 17 | Episode 60 | Avg Reward -535.0\n",
      "Generation 17 | Episode 80 | Avg Reward -617.6\n",
      "Generation 17 | Episode 100 | Avg Reward -498.9\n",
      "Generation 17 | Episode 120 | Avg Reward -567.3\n",
      "Generation 17 | Episode 140 | Avg Reward -598.5\n",
      "Generation 17 | Episode 160 | Avg Reward -556.2\n",
      "Generation 17 | Episode 180 | Avg Reward -523.8\n",
      "Generation 17 | Episode 200 | Avg Reward -305.0\n",
      "Generation 17 Average Reward: -525.9\n",
      "Generation:  18\n",
      "Generation 18 | Episode 20 | Avg Reward -248.1\n",
      "Generation 18 | Episode 40 | Avg Reward -213.9\n",
      "Generation 18 | Episode 60 | Avg Reward -136.2\n",
      "Generation 18 | Episode 80 | Avg Reward -215.7\n",
      "Generation 18 | Episode 100 | Avg Reward -164.1\n",
      "Generation 18 | Episode 120 | Avg Reward -215.1\n",
      "Generation 18 | Episode 140 | Avg Reward -480.5\n",
      "Generation 18 | Episode 160 | Avg Reward -600.6\n",
      "Generation 18 | Episode 180 | Avg Reward -506.1\n",
      "Generation 18 | Episode 200 | Avg Reward -554.2\n",
      "Generation 18 Average Reward: -333.4\n",
      "Generation:  19\n",
      "Generation 19 | Episode 20 | Avg Reward -171.6\n",
      "Generation 19 | Episode 40 | Avg Reward -157.8\n",
      "Generation 19 | Episode 60 | Avg Reward -459.6\n",
      "Generation 19 | Episode 80 | Avg Reward -685.1\n",
      "Generation 19 | Episode 100 | Avg Reward -647.6\n",
      "Generation 19 | Episode 120 | Avg Reward -778.9\n",
      "Generation 19 | Episode 140 | Avg Reward -917.8\n",
      "Generation 19 | Episode 160 | Avg Reward -725.7\n",
      "Generation 19 | Episode 180 | Avg Reward -911.1\n",
      "Generation 19 | Episode 200 | Avg Reward -706.6\n",
      "Generation 19 Average Reward: -616.2\n",
      "Generation:  20\n",
      "Generation 20 | Episode 20 | Avg Reward -287.8\n",
      "Generation 20 | Episode 40 | Avg Reward -558.3\n",
      "Generation 20 | Episode 60 | Avg Reward -632.1\n",
      "Generation 20 | Episode 80 | Avg Reward -590.7\n",
      "Generation 20 | Episode 100 | Avg Reward -602.5\n",
      "Generation 20 | Episode 120 | Avg Reward -900.8\n",
      "Generation 20 | Episode 140 | Avg Reward -1031.5\n",
      "Generation 20 | Episode 160 | Avg Reward -915.5\n",
      "Generation 20 | Episode 180 | Avg Reward -881.0\n",
      "Generation 20 | Episode 200 | Avg Reward -817.6\n",
      "Generation 20 Average Reward: -721.8\n",
      "Generation:  21\n",
      "Generation 21 | Episode 20 | Avg Reward -754.2\n",
      "Generation 21 | Episode 40 | Avg Reward -759.4\n",
      "Generation 21 | Episode 60 | Avg Reward -959.8\n",
      "Generation 21 | Episode 80 | Avg Reward -963.4\n",
      "Generation 21 | Episode 100 | Avg Reward -687.0\n",
      "Generation 21 | Episode 120 | Avg Reward -203.4\n",
      "Generation 21 | Episode 140 | Avg Reward -724.6\n",
      "Generation 21 | Episode 160 | Avg Reward -314.1\n",
      "Generation 21 | Episode 180 | Avg Reward -149.1\n",
      "Generation 21 | Episode 200 | Avg Reward -412.1\n",
      "Generation 21 Average Reward: -592.7\n",
      "Generation:  22\n",
      "Generation 22 | Episode 20 | Avg Reward -491.1\n",
      "Generation 22 | Episode 40 | Avg Reward -131.4\n",
      "Generation 22 | Episode 60 | Avg Reward -131.7\n",
      "Generation 22 | Episode 80 | Avg Reward -756.5\n",
      "Generation 22 | Episode 100 | Avg Reward -897.4\n",
      "Generation 22 | Episode 120 | Avg Reward -784.2\n",
      "Generation 22 | Episode 140 | Avg Reward -760.9\n",
      "Generation 22 | Episode 160 | Avg Reward -785.8\n",
      "Generation 22 | Episode 180 | Avg Reward -757.1\n",
      "Generation 22 | Episode 200 | Avg Reward -789.1\n",
      "Generation 22 Average Reward: -628.5\n",
      "Generation:  23\n",
      "Generation 23 | Episode 20 | Avg Reward -295.1\n",
      "Generation 23 | Episode 40 | Avg Reward -506.9\n",
      "Generation 23 | Episode 60 | Avg Reward -574.1\n",
      "Generation 23 | Episode 80 | Avg Reward -461.0\n",
      "Generation 23 | Episode 100 | Avg Reward -644.2\n",
      "Generation 23 | Episode 120 | Avg Reward -400.8\n",
      "Generation 23 | Episode 140 | Avg Reward -187.5\n",
      "Generation 23 | Episode 160 | Avg Reward -262.3\n",
      "Generation 23 | Episode 180 | Avg Reward -833.8\n",
      "Generation 23 | Episode 200 | Avg Reward -1135.9\n",
      "Generation 23 Average Reward: -530.2\n",
      "Generation:  24\n",
      "Generation 24 | Episode 20 | Avg Reward -294.0\n",
      "Generation 24 | Episode 40 | Avg Reward -523.1\n",
      "Generation 24 | Episode 60 | Avg Reward -655.2\n",
      "Generation 24 | Episode 80 | Avg Reward -844.6\n",
      "Generation 24 | Episode 100 | Avg Reward -603.7\n",
      "Generation 24 | Episode 120 | Avg Reward -762.7\n",
      "Generation 24 | Episode 140 | Avg Reward -777.5\n",
      "Generation 24 | Episode 160 | Avg Reward -719.3\n",
      "Generation 24 | Episode 180 | Avg Reward -683.9\n",
      "Generation 24 | Episode 200 | Avg Reward -746.5\n",
      "Generation 24 Average Reward: -661.0\n",
      "Generation:  25\n",
      "Generation 25 | Episode 20 | Avg Reward -346.7\n",
      "Generation 25 | Episode 40 | Avg Reward -325.6\n",
      "Generation 25 | Episode 60 | Avg Reward -219.5\n",
      "Generation 25 | Episode 80 | Avg Reward -325.6\n",
      "Generation 25 | Episode 100 | Avg Reward -570.6\n",
      "Generation 25 | Episode 120 | Avg Reward -664.3\n",
      "Generation 25 | Episode 140 | Avg Reward -577.4\n",
      "Generation 25 | Episode 160 | Avg Reward -579.1\n",
      "Generation 25 | Episode 180 | Avg Reward -571.2\n",
      "Generation 25 | Episode 200 | Avg Reward -549.4\n",
      "Generation 25 Average Reward: -472.9\n",
      "Generation:  26\n",
      "Generation 26 | Episode 20 | Avg Reward -192.7\n",
      "Generation 26 | Episode 40 | Avg Reward -234.2\n",
      "Generation 26 | Episode 60 | Avg Reward -217.1\n",
      "Generation 26 | Episode 80 | Avg Reward -229.7\n",
      "Generation 26 | Episode 100 | Avg Reward -169.9\n",
      "Generation 26 | Episode 120 | Avg Reward -268.3\n",
      "Generation 26 | Episode 140 | Avg Reward -604.7\n",
      "Generation 26 | Episode 160 | Avg Reward -536.9\n",
      "Generation 26 | Episode 180 | Avg Reward -636.0\n",
      "Generation 26 | Episode 200 | Avg Reward -594.9\n",
      "Generation 26 Average Reward: -368.4\n",
      "Generation:  27\n",
      "Generation 27 | Episode 20 | Avg Reward -575.3\n",
      "Generation 27 | Episode 40 | Avg Reward -577.6\n",
      "Generation 27 | Episode 60 | Avg Reward -589.9\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import random\n",
    "\n",
    "sns.set()\n",
    "\n",
    "class ActorCriticNN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,action_dim))\n",
    "\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1))\n",
    "\n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "\n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value\n",
    "\n",
    "#trainer\n",
    "class PPOTrainer():\n",
    "    def __init__(self, actor_critic, ppo_clip_val, target_kl_div, policy_lr, value_lr, max_policy_train_iters = 80, value_train_iters = 80):\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_val = ppo_clip_val\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.value_train_iters = value_train_iters\n",
    "    \n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr = policy_lr)\n",
    "    \n",
    "        value_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr = value_lr)\n",
    "\n",
    "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "\n",
    "        for _ in range(self.max_policy_train_iters):\n",
    "                \n",
    "            self.policy_optim.zero_grad()\n",
    "    \n",
    "            new_logits = self.ac.policy(obs)\n",
    "            new_logits = Categorical(logits = new_logits)\n",
    "            new_log_probs = new_logits.log_prob(acts)\n",
    "    \n",
    "            policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            clipped_ratio = policy_ratio.clamp(1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "            clipped_loss = clipped_ratio * gaes\n",
    "            full_loss = policy_ratio * gaes\n",
    "            policy_loss = -torch.min(full_loss, clipped_loss).mean()\n",
    "    \n",
    "            policy_loss.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "            kl_div = (old_log_probs - new_log_probs).mean()\n",
    "            if kl_div >= self.target_kl_div:\n",
    "                break\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "\n",
    "        for _ in range (self.value_train_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "    \n",
    "            values = self.ac.value(obs)\n",
    "            value_loss = (returns-values) ** 2\n",
    "            value_loss = value_loss.mean()\n",
    "    \n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()\n",
    "\n",
    "class ActorCriticChromosome:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        # Actor-Critic neural network\n",
    "        self.model = ActorCriticNN(state_dim, action_dim)\n",
    "\n",
    "        #Hyperparameters\n",
    "        self.policy_lr = random.uniform(1e-5, 1e-2)  # Expanded range\n",
    "        self.value_lr = random.uniform(1e-5, 1e-2)  # Expanded range\n",
    "        self.ppo_clip_val = random.uniform(0.1, 0.4)  # Expanded range\n",
    "        self.target_kl_div = random.uniform(0.001, 0.05)\n",
    "\n",
    "def create_population(pop_size, state_dim, action_dim):\n",
    "    return [ActorCriticChromosome(state_dim, action_dim) for _ in range(pop_size)]\n",
    "\n",
    "\"\"\"\n",
    "When initializing PPOTrainer, you're passing the individual's hyperparameters (policy_lr, value_lr, etc.) \n",
    "to ensure the evaluation process reflects the effect of those hyperparameters.\n",
    "You're sampling actions from the model and accumulating rewards to calculate the fitness score.\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_population(population, env, max_steps=100):\n",
    "    fitness_scores = []\n",
    "    for individual in population:\n",
    "        total_reward = 0\n",
    "\n",
    "        # Initialize PPOTrainer with individual's hyperparameters\n",
    "        ppo = PPOTrainer(individual.model,\n",
    "                         ppo_clip_val=individual.ppo_clip_val,\n",
    "                         target_kl_div=individual.target_kl_div,\n",
    "                         policy_lr=individual.policy_lr,\n",
    "                         value_lr=individual.value_lr)\n",
    "\n",
    "        # Reset the environment and evaluate the model\n",
    "        obs, _ = env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "            logits, _ = individual.model(obs_tensor)  # Use the model from the individual\n",
    "            act_dist = Categorical(logits=logits)\n",
    "            action = act_dist.sample()\n",
    "            next_obs, reward, done, _, __ = env.step(action.item())\n",
    "            total_reward += reward\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n",
    "        fitness_scores.append(total_reward)\n",
    "    \n",
    "    return fitness_scores\n",
    "\n",
    "def select_parents(population, fitness_scores, num_parents):\n",
    "   # Get indices that would sort the fitness scores in descending order\n",
    "    sorted_indices = np.argsort(fitness_scores)[::-1]  # Reverse for descending order\n",
    "    \n",
    "    # Select the best parents based on the sorted indices\n",
    "    parents = np.array(population)[sorted_indices][:num_parents]\n",
    "    return parents.tolist()\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    child = ActorCriticChromosome(state_dim, action_dim)\n",
    "    for param1, param2, param_child in zip(parent1.model.parameters(), parent2.model.parameters(), child.model.parameters()):\n",
    "        param_child.data = (param1.data + param2.data) / 2  # Average weights\n",
    "\n",
    "    # Crossover for hyperparameters\n",
    "    child.policy_lr = (parent1.policy_lr + parent2.policy_lr) / 2\n",
    "    child.value_lr = (parent1.value_lr + parent2.value_lr) / 2\n",
    "    child.ppo_clip_val = (parent1.ppo_clip_val + parent2.ppo_clip_val) / 2\n",
    "    child.target_kl_div = (parent1.target_kl_div + parent2.target_kl_div) / 2\n",
    "    \n",
    "    return child\n",
    "\n",
    "def mutate(individual, mutation_rate=0.01):\n",
    "    for param in individual.model.parameters():\n",
    "        if random.random() < mutation_rate:\n",
    "            param.data += torch.randn(param.size()) * 0.1  # Add small random noise\n",
    "    \n",
    "    # Mutate hyperparameters\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.policy_lr += random.uniform(-1e-5, 1e-5)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.value_lr += random.uniform(-1e-4, 1e-4)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.ppo_clip_val += random.uniform(-0.01, 0.01)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.target_kl_div += random.uniform(-0.001, 0.001)\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Return discounted rewards based on the given rewards and the gamma param.\n",
    "    \"\"\"\n",
    "\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma = 0.99, decay = 0.97):\n",
    "    \"\"\"\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/abs/1506.02438\n",
    "    \"\"\"\n",
    "\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
    "\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "    return np.array(gaes[::-1])\n",
    "\n",
    "def rollout(model, env, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Performs a single rollout, sampling an action and collecting data.\n",
    "    Returns training data in the shape (n_steps, observation_shape) and the cumulative reward.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the data storage (obs, act, reward, values, act_log_probs)\n",
    "    train_data = [[], [], [], [], []]  \n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    #print(\"Environment reset, initial observation:\", obs)  # Check if env.reset() works\n",
    "\n",
    "    ep_reward = 0\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        #Convert observation to a PyTorch tensor and ensure correct shape\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "        logits, val = model(obs)\n",
    "\n",
    "        # Create action distribution and sample an action\n",
    "        act_dist = Categorical(logits=logits)\n",
    "        act = act_dist.sample()\n",
    "        act_log_prob = act_dist.log_prob(act).item()\n",
    "\n",
    "        act, val = act.item(), val.item()\n",
    "\n",
    "        # Take action in the environment\n",
    "        next_obs, reward, done, _, __ = env.step(act)\n",
    "\n",
    "        # Store the data for training\n",
    "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
    "            train_data[i].append(item)\n",
    "\n",
    "        #print(f\"Action taken: {act.item()}, Reward received: {reward}, Done: {done}\")\n",
    "\n",
    "        # Update observation and cumulative reward\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "\n",
    "        # Break if the episode ends\n",
    "        if done:\n",
    "            #print(f\"Episode ended after {step+1} steps with cumulative reward: {ep_reward}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "    \n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3])\n",
    "    \n",
    "    return train_data, ep_reward\n",
    "\n",
    "# Change environment to LunarLander-v2\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Define the environment dimensions for LunarLander\n",
    "state_dim = env.observation_space.shape[0]  # LunarLander has 8 state dimensions\n",
    "action_dim = env.action_space.n  # LunarLander has 4 discrete actions\n",
    "\n",
    "# Now you can proceed with the rest of your code\n",
    "# The rest of the code remains the same, starting with creating the population\n",
    "n_episodes = 200\n",
    "print_freq = 20\n",
    "pop_size = 30\n",
    "num_parents = 10\n",
    "num_generations = 50\n",
    "\n",
    "# Initialize the population\n",
    "population = create_population(pop_size, state_dim, action_dim)\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    # Evaluate the current population\n",
    "    fitness_scores = evaluate_population(population, env)\n",
    "    \n",
    "    # Select the best parents\n",
    "    parents = select_parents(population, fitness_scores, num_parents)\n",
    "\n",
    "    next_generation = []\n",
    "    \n",
    "    # Print the generation number\n",
    "    print(\"Generation: \", generation + 1)\n",
    "\n",
    "    # Create a selective parents list for crossover\n",
    "    selective_parents = [parents[0], parents[1], parents[2], parents[3], parents[4]]\n",
    "\n",
    "    best_select = []\n",
    "    \n",
    "    for i in range(num_parents):\n",
    "        parent1, parent2 = np.random.choice(selective_parents, 2, replace=False)\n",
    "        child = crossover(parent1, parent2)\n",
    "        mutate(child)\n",
    "        best_select.append(child)\n",
    "\n",
    "    for i in range(pop_size):\n",
    "        next_generation.append(population[i])\n",
    "\n",
    "    # Evaluate fitness of the new generation\n",
    "    next_generation_fitness = evaluate_population(next_generation, env)\n",
    "    \n",
    "    # Sort the next generation based on fitness scores (best to worst)\n",
    "    next_generation = [x for _, x in sorted(zip(next_generation_fitness, next_generation), key=lambda pair: pair[0], reverse=True)]\n",
    "\n",
    "    index = 0\n",
    "    for i in range ((pop_size-10),pop_size):\n",
    "        next_generation[i] = best_select[index]\n",
    "        index = index + 1\n",
    "    \n",
    "    population = next_generation\n",
    "\n",
    "    # Optionally fine-tune the best policy with PPO after EA\n",
    "    best_policy = parents[0]  # Select the best policy for further training with PPO\n",
    "    ppo = PPOTrainer(best_policy.model, ppo_clip_val = best_policy.ppo_clip_val, target_kl_div=best_policy.target_kl_div, \n",
    "                     policy_lr=best_policy.policy_lr, value_lr=best_policy.value_lr)\n",
    "\n",
    "    ep_rewards = []\n",
    "    for episode_idx in range(n_episodes):\n",
    "        # Perform rollout\n",
    "        train_data, reward = rollout(best_policy.model, env)\n",
    "        ep_rewards.append(reward)\n",
    "\n",
    "        permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "        obs = torch.tensor(train_data[0][permute_idxs], dtype=torch.float32)\n",
    "        act = torch.tensor(train_data[1][permute_idxs], dtype=torch.int32)\n",
    "        gaes = torch.tensor(train_data[3][permute_idxs], dtype=torch.float32)\n",
    "        act_log_probs = torch.tensor(train_data[4][permute_idxs], dtype=torch.float32)\n",
    "\n",
    "        # Value Data\n",
    "        returns = discount_rewards(train_data[2])[permute_idxs]\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        # Train Policy\n",
    "        ppo.train_policy(obs, act, act_log_probs, gaes)\n",
    "        ppo.train_value(obs, returns)\n",
    "\n",
    "        # Print average reward every 'print_freq' episodes\n",
    "        if (episode_idx + 1) % print_freq == 0:\n",
    "            avg_reward = np.mean(ep_rewards[-print_freq:])  # Calculate the average of the last 'print_freq' rewards\n",
    "            print('Generation {} | Episode {} | Avg Reward {:.1f}'.format(\n",
    "                generation + 1, episode_idx + 1, avg_reward))\n",
    "\n",
    "    # Calculate and print the overall average reward for this generation\n",
    "    generation_avg_reward = np.mean(ep_rewards)  # Calculate average for all episodes in this generation\n",
    "    print(\"Generation {} Average Reward: {:.1f}\".format(generation + 1, generation_avg_reward))\n",
    "\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa0d4581-aff3-4723-a9f0-d65230daf9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tr/5nyxxp8s7wlck89wj3ljstd00000gn/T/ipykernel_71052/1471796663.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  returns = torch.tensor(returns, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 | Episode 20 | Avg Reward -349.5\n",
      "Generation 1 | Episode 40 | Avg Reward -407.1\n",
      "Generation 1 | Episode 60 | Avg Reward -179.7\n",
      "Generation 1 | Episode 80 | Avg Reward -175.3\n",
      "Generation 1 | Episode 100 | Avg Reward -155.0\n",
      "Generation 1 | Episode 120 | Avg Reward -180.1\n",
      "Generation 1 | Episode 140 | Avg Reward -159.5\n",
      "Generation 1 | Episode 160 | Avg Reward -155.8\n",
      "Generation 1 | Episode 180 | Avg Reward -165.0\n",
      "Generation 1 | Episode 200 | Avg Reward -162.6\n",
      "Generation 1 Average Reward: -209.0\n",
      "Generation 2 | Episode 20 | Avg Reward -455.5\n",
      "Generation 2 | Episode 40 | Avg Reward -639.2\n",
      "Generation 2 | Episode 60 | Avg Reward -509.9\n",
      "Generation 2 | Episode 80 | Avg Reward -538.5\n",
      "Generation 2 | Episode 100 | Avg Reward -737.2\n",
      "Generation 2 | Episode 120 | Avg Reward -635.9\n",
      "Generation 2 | Episode 140 | Avg Reward -606.8\n",
      "Generation 2 | Episode 160 | Avg Reward -489.2\n",
      "Generation 2 | Episode 180 | Avg Reward -532.3\n",
      "Generation 2 | Episode 200 | Avg Reward -455.7\n",
      "Generation 2 Average Reward: -560.0\n",
      "Generation 3 | Episode 20 | Avg Reward -396.5\n",
      "Generation 3 | Episode 40 | Avg Reward -392.1\n",
      "Generation 3 | Episode 60 | Avg Reward -354.7\n",
      "Generation 3 | Episode 80 | Avg Reward -335.4\n",
      "Generation 3 | Episode 100 | Avg Reward -218.5\n",
      "Generation 3 | Episode 120 | Avg Reward -151.2\n",
      "Generation 3 | Episode 140 | Avg Reward -110.8\n",
      "Generation 3 | Episode 160 | Avg Reward -135.3\n",
      "Generation 3 | Episode 180 | Avg Reward -112.6\n",
      "Generation 3 | Episode 200 | Avg Reward -219.7\n",
      "Generation 3 Average Reward: -242.7\n",
      "Generation 4 | Episode 20 | Avg Reward -234.9\n",
      "Generation 4 | Episode 40 | Avg Reward -228.5\n",
      "Generation 4 | Episode 60 | Avg Reward -205.9\n",
      "Generation 4 | Episode 80 | Avg Reward -128.9\n",
      "Generation 4 | Episode 100 | Avg Reward -151.5\n",
      "Generation 4 | Episode 120 | Avg Reward -172.7\n",
      "Generation 4 | Episode 140 | Avg Reward -178.3\n",
      "Generation 4 | Episode 160 | Avg Reward -175.4\n",
      "Generation 4 | Episode 180 | Avg Reward -186.0\n",
      "Generation 4 | Episode 200 | Avg Reward -173.9\n",
      "Generation 4 Average Reward: -183.6\n",
      "Generation 5 | Episode 20 | Avg Reward -242.5\n",
      "Generation 5 | Episode 40 | Avg Reward -227.9\n",
      "Generation 5 | Episode 60 | Avg Reward -158.0\n",
      "Generation 5 | Episode 80 | Avg Reward -153.3\n",
      "Generation 5 | Episode 100 | Avg Reward -162.8\n",
      "Generation 5 | Episode 120 | Avg Reward -146.3\n",
      "Generation 5 | Episode 140 | Avg Reward -150.9\n",
      "Generation 5 | Episode 160 | Avg Reward -162.1\n",
      "Generation 5 | Episode 180 | Avg Reward -157.2\n",
      "Generation 5 | Episode 200 | Avg Reward -157.7\n",
      "Generation 5 Average Reward: -171.9\n",
      "Generation 6 | Episode 20 | Avg Reward -500.0\n",
      "Generation 6 | Episode 40 | Avg Reward -543.3\n",
      "Generation 6 | Episode 60 | Avg Reward -539.3\n",
      "Generation 6 | Episode 80 | Avg Reward -496.6\n",
      "Generation 6 | Episode 100 | Avg Reward -514.3\n",
      "Generation 6 | Episode 120 | Avg Reward -483.6\n",
      "Generation 6 | Episode 140 | Avg Reward -284.9\n",
      "Generation 6 | Episode 160 | Avg Reward -187.1\n",
      "Generation 6 | Episode 180 | Avg Reward -171.0\n",
      "Generation 6 | Episode 200 | Avg Reward -136.3\n",
      "Generation 6 Average Reward: -385.6\n",
      "Generation 7 | Episode 20 | Avg Reward -317.5\n",
      "Generation 7 | Episode 40 | Avg Reward -415.8\n",
      "Generation 7 | Episode 60 | Avg Reward -393.0\n",
      "Generation 7 | Episode 80 | Avg Reward -169.8\n",
      "Generation 7 | Episode 100 | Avg Reward -155.9\n",
      "Generation 7 | Episode 120 | Avg Reward -192.8\n",
      "Generation 7 | Episode 140 | Avg Reward -295.1\n",
      "Generation 7 | Episode 160 | Avg Reward -249.7\n",
      "Generation 7 | Episode 180 | Avg Reward -191.4\n",
      "Generation 7 | Episode 200 | Avg Reward -175.3\n",
      "Generation 7 Average Reward: -255.6\n",
      "Generation 8 | Episode 20 | Avg Reward -279.9\n",
      "Generation 8 | Episode 40 | Avg Reward -210.7\n",
      "Generation 8 | Episode 60 | Avg Reward -219.3\n",
      "Generation 8 | Episode 80 | Avg Reward -226.4\n",
      "Generation 8 | Episode 100 | Avg Reward -222.6\n",
      "Generation 8 | Episode 120 | Avg Reward -188.6\n",
      "Generation 8 | Episode 140 | Avg Reward -202.4\n",
      "Generation 8 | Episode 160 | Avg Reward -215.7\n",
      "Generation 8 | Episode 180 | Avg Reward -311.4\n",
      "Generation 8 | Episode 200 | Avg Reward -286.1\n",
      "Generation 8 Average Reward: -236.3\n",
      "Generation 9 | Episode 20 | Avg Reward -186.9\n",
      "Generation 9 | Episode 40 | Avg Reward -200.2\n",
      "Generation 9 | Episode 60 | Avg Reward -187.8\n",
      "Generation 9 | Episode 80 | Avg Reward -170.0\n",
      "Generation 9 | Episode 100 | Avg Reward -208.9\n",
      "Generation 9 | Episode 120 | Avg Reward -166.5\n",
      "Generation 9 | Episode 140 | Avg Reward -162.4\n",
      "Generation 9 | Episode 160 | Avg Reward -132.5\n",
      "Generation 9 | Episode 180 | Avg Reward -165.8\n",
      "Generation 9 | Episode 200 | Avg Reward -155.8\n",
      "Generation 9 Average Reward: -173.7\n",
      "Generation 10 | Episode 20 | Avg Reward -166.5\n",
      "Generation 10 | Episode 40 | Avg Reward -166.5\n",
      "Generation 10 | Episode 60 | Avg Reward -202.3\n",
      "Generation 10 | Episode 80 | Avg Reward -268.2\n",
      "Generation 10 | Episode 100 | Avg Reward -242.3\n",
      "Generation 10 | Episode 120 | Avg Reward -186.3\n",
      "Generation 10 | Episode 140 | Avg Reward -130.4\n",
      "Generation 10 | Episode 160 | Avg Reward -158.3\n",
      "Generation 10 | Episode 180 | Avg Reward -235.9\n",
      "Generation 10 | Episode 200 | Avg Reward -228.5\n",
      "Generation 10 Average Reward: -198.5\n",
      "Generation 11 | Episode 20 | Avg Reward -651.4\n",
      "Generation 11 | Episode 40 | Avg Reward -539.0\n",
      "Generation 11 | Episode 60 | Avg Reward -900.3\n",
      "Generation 11 | Episode 80 | Avg Reward -724.9\n",
      "Generation 11 | Episode 100 | Avg Reward -647.0\n",
      "Generation 11 | Episode 120 | Avg Reward -773.8\n",
      "Generation 11 | Episode 140 | Avg Reward -512.8\n",
      "Generation 11 | Episode 160 | Avg Reward -591.8\n",
      "Generation 11 | Episode 180 | Avg Reward -619.0\n",
      "Generation 11 | Episode 200 | Avg Reward -558.1\n",
      "Generation 11 Average Reward: -651.8\n",
      "Generation 12 | Episode 20 | Avg Reward -189.6\n",
      "Generation 12 | Episode 40 | Avg Reward -203.7\n",
      "Generation 12 | Episode 60 | Avg Reward -135.8\n",
      "Generation 12 | Episode 80 | Avg Reward -135.5\n",
      "Generation 12 | Episode 100 | Avg Reward -192.8\n",
      "Generation 12 | Episode 120 | Avg Reward -161.5\n",
      "Generation 12 | Episode 140 | Avg Reward -188.3\n",
      "Generation 12 | Episode 160 | Avg Reward -147.8\n",
      "Generation 12 | Episode 180 | Avg Reward -170.0\n",
      "Generation 12 | Episode 200 | Avg Reward -208.5\n",
      "Generation 12 Average Reward: -173.3\n",
      "Generation 13 | Episode 20 | Avg Reward -129.8\n",
      "Generation 13 | Episode 40 | Avg Reward -159.4\n",
      "Generation 13 | Episode 60 | Avg Reward -182.3\n",
      "Generation 13 | Episode 80 | Avg Reward -160.1\n",
      "Generation 13 | Episode 100 | Avg Reward -165.5\n",
      "Generation 13 | Episode 120 | Avg Reward -144.2\n",
      "Generation 13 | Episode 140 | Avg Reward -238.8\n",
      "Generation 13 | Episode 160 | Avg Reward -226.0\n",
      "Generation 13 | Episode 180 | Avg Reward -356.9\n",
      "Generation 13 | Episode 200 | Avg Reward -168.3\n",
      "Generation 13 Average Reward: -193.1\n",
      "Generation 14 | Episode 20 | Avg Reward -210.1\n",
      "Generation 14 | Episode 40 | Avg Reward -208.7\n",
      "Generation 14 | Episode 60 | Avg Reward -290.1\n",
      "Generation 14 | Episode 80 | Avg Reward -234.4\n",
      "Generation 14 | Episode 100 | Avg Reward -214.2\n",
      "Generation 14 | Episode 120 | Avg Reward -214.4\n",
      "Generation 14 | Episode 140 | Avg Reward -292.6\n",
      "Generation 14 | Episode 160 | Avg Reward -218.1\n",
      "Generation 14 | Episode 180 | Avg Reward -227.5\n",
      "Generation 14 | Episode 200 | Avg Reward -240.5\n",
      "Generation 14 Average Reward: -235.0\n",
      "Generation 15 | Episode 20 | Avg Reward -167.5\n",
      "Generation 15 | Episode 40 | Avg Reward -162.4\n",
      "Generation 15 | Episode 60 | Avg Reward -169.2\n",
      "Generation 15 | Episode 80 | Avg Reward -165.2\n",
      "Generation 15 | Episode 100 | Avg Reward -163.4\n",
      "Generation 15 | Episode 120 | Avg Reward -162.2\n",
      "Generation 15 | Episode 140 | Avg Reward -164.9\n",
      "Generation 15 | Episode 160 | Avg Reward -147.6\n",
      "Generation 15 | Episode 180 | Avg Reward -138.9\n",
      "Generation 15 | Episode 200 | Avg Reward -161.0\n",
      "Generation 15 Average Reward: -160.2\n",
      "Generation 16 | Episode 20 | Avg Reward -149.1\n",
      "Generation 16 | Episode 40 | Avg Reward -148.3\n",
      "Generation 16 | Episode 60 | Avg Reward -145.6\n",
      "Generation 16 | Episode 80 | Avg Reward -110.0\n",
      "Generation 16 | Episode 100 | Avg Reward -188.4\n",
      "Generation 16 | Episode 120 | Avg Reward -169.3\n",
      "Generation 16 | Episode 140 | Avg Reward -158.4\n",
      "Generation 16 | Episode 160 | Avg Reward -143.3\n",
      "Generation 16 | Episode 180 | Avg Reward -132.8\n",
      "Generation 16 | Episode 200 | Avg Reward -148.4\n",
      "Generation 16 Average Reward: -149.3\n",
      "Generation 17 | Episode 20 | Avg Reward -165.7\n",
      "Generation 17 | Episode 40 | Avg Reward -182.6\n",
      "Generation 17 | Episode 60 | Avg Reward -204.7\n",
      "Generation 17 | Episode 80 | Avg Reward -234.7\n",
      "Generation 17 | Episode 100 | Avg Reward -158.9\n",
      "Generation 17 | Episode 120 | Avg Reward -162.2\n",
      "Generation 17 | Episode 140 | Avg Reward -255.6\n",
      "Generation 17 | Episode 160 | Avg Reward -204.9\n",
      "Generation 17 | Episode 180 | Avg Reward -213.8\n",
      "Generation 17 | Episode 200 | Avg Reward -214.6\n",
      "Generation 17 Average Reward: -199.8\n",
      "Generation 18 | Episode 20 | Avg Reward -219.7\n",
      "Generation 18 | Episode 40 | Avg Reward -213.1\n",
      "Generation 18 | Episode 60 | Avg Reward -184.9\n",
      "Generation 18 | Episode 80 | Avg Reward -215.5\n",
      "Generation 18 | Episode 100 | Avg Reward -167.5\n",
      "Generation 18 | Episode 120 | Avg Reward -187.1\n",
      "Generation 18 | Episode 140 | Avg Reward -158.5\n",
      "Generation 18 | Episode 160 | Avg Reward -145.3\n",
      "Generation 18 | Episode 180 | Avg Reward -230.7\n",
      "Generation 18 | Episode 200 | Avg Reward -152.9\n",
      "Generation 18 Average Reward: -187.5\n",
      "Generation 19 | Episode 20 | Avg Reward -161.4\n",
      "Generation 19 | Episode 40 | Avg Reward -135.2\n",
      "Generation 19 | Episode 60 | Avg Reward -188.4\n",
      "Generation 19 | Episode 80 | Avg Reward -134.7\n",
      "Generation 19 | Episode 100 | Avg Reward -109.1\n",
      "Generation 19 | Episode 120 | Avg Reward -120.5\n",
      "Generation 19 | Episode 140 | Avg Reward -148.8\n",
      "Generation 19 | Episode 160 | Avg Reward -241.5\n",
      "Generation 19 | Episode 180 | Avg Reward -173.7\n",
      "Generation 19 | Episode 200 | Avg Reward -236.7\n",
      "Generation 19 Average Reward: -165.0\n",
      "Generation 20 | Episode 20 | Avg Reward -244.3\n",
      "Generation 20 | Episode 40 | Avg Reward -228.0\n",
      "Generation 20 | Episode 60 | Avg Reward -179.2\n",
      "Generation 20 | Episode 80 | Avg Reward -148.7\n",
      "Generation 20 | Episode 100 | Avg Reward -203.6\n",
      "Generation 20 | Episode 120 | Avg Reward -148.9\n",
      "Generation 20 | Episode 140 | Avg Reward -128.7\n",
      "Generation 20 | Episode 160 | Avg Reward -120.5\n",
      "Generation 20 | Episode 180 | Avg Reward -135.1\n",
      "Generation 20 | Episode 200 | Avg Reward -121.5\n",
      "Generation 20 Average Reward: -165.8\n",
      "Generation 21 | Episode 20 | Avg Reward -142.2\n",
      "Generation 21 | Episode 40 | Avg Reward -147.0\n",
      "Generation 21 | Episode 60 | Avg Reward -144.7\n",
      "Generation 21 | Episode 80 | Avg Reward -136.0\n",
      "Generation 21 | Episode 100 | Avg Reward -155.3\n",
      "Generation 21 | Episode 120 | Avg Reward -150.3\n",
      "Generation 21 | Episode 140 | Avg Reward -199.2\n",
      "Generation 21 | Episode 160 | Avg Reward -171.2\n",
      "Generation 21 | Episode 180 | Avg Reward -135.3\n",
      "Generation 21 | Episode 200 | Avg Reward -149.4\n",
      "Generation 21 Average Reward: -153.0\n",
      "Generation 22 | Episode 20 | Avg Reward -527.5\n",
      "Generation 22 | Episode 40 | Avg Reward -587.1\n",
      "Generation 22 | Episode 60 | Avg Reward -562.1\n",
      "Generation 22 | Episode 80 | Avg Reward -489.2\n",
      "Generation 22 | Episode 100 | Avg Reward -505.7\n",
      "Generation 22 | Episode 120 | Avg Reward -461.7\n",
      "Generation 22 | Episode 140 | Avg Reward -374.9\n",
      "Generation 22 | Episode 160 | Avg Reward -231.1\n",
      "Generation 22 | Episode 180 | Avg Reward -136.5\n",
      "Generation 22 | Episode 200 | Avg Reward -182.1\n",
      "Generation 22 Average Reward: -405.8\n",
      "Generation 23 | Episode 20 | Avg Reward -146.0\n",
      "Generation 23 | Episode 40 | Avg Reward -128.7\n",
      "Generation 23 | Episode 60 | Avg Reward -165.1\n",
      "Generation 23 | Episode 80 | Avg Reward -152.9\n",
      "Generation 23 | Episode 100 | Avg Reward -152.0\n",
      "Generation 23 | Episode 120 | Avg Reward -149.1\n",
      "Generation 23 | Episode 140 | Avg Reward -224.2\n",
      "Generation 23 | Episode 160 | Avg Reward -250.0\n",
      "Generation 23 | Episode 180 | Avg Reward -158.2\n",
      "Generation 23 | Episode 200 | Avg Reward -157.8\n",
      "Generation 23 Average Reward: -168.4\n",
      "Generation 24 | Episode 20 | Avg Reward -133.7\n",
      "Generation 24 | Episode 40 | Avg Reward -233.1\n",
      "Generation 24 | Episode 60 | Avg Reward -137.7\n",
      "Generation 24 | Episode 80 | Avg Reward -164.3\n",
      "Generation 24 | Episode 100 | Avg Reward -130.3\n",
      "Generation 24 | Episode 120 | Avg Reward -119.2\n",
      "Generation 24 | Episode 140 | Avg Reward -105.8\n",
      "Generation 24 | Episode 160 | Avg Reward -113.3\n",
      "Generation 24 | Episode 180 | Avg Reward -98.3\n",
      "Generation 24 | Episode 200 | Avg Reward -90.5\n",
      "Generation 24 Average Reward: -132.6\n",
      "Generation 25 | Episode 20 | Avg Reward -73.9\n",
      "Generation 25 | Episode 40 | Avg Reward -68.8\n",
      "Generation 25 | Episode 60 | Avg Reward -62.2\n",
      "Generation 25 | Episode 80 | Avg Reward -188.8\n",
      "Generation 25 | Episode 100 | Avg Reward -477.4\n",
      "Generation 25 | Episode 120 | Avg Reward -424.8\n",
      "Generation 25 | Episode 140 | Avg Reward -401.3\n",
      "Generation 25 | Episode 160 | Avg Reward -337.4\n",
      "Generation 25 | Episode 180 | Avg Reward -233.0\n",
      "Generation 25 | Episode 200 | Avg Reward -174.5\n",
      "Generation 25 Average Reward: -244.2\n",
      "Generation 26 | Episode 20 | Avg Reward -521.4\n",
      "Generation 26 | Episode 40 | Avg Reward -496.2\n",
      "Generation 26 | Episode 60 | Avg Reward -413.6\n",
      "Generation 26 | Episode 80 | Avg Reward -198.5\n",
      "Generation 26 | Episode 100 | Avg Reward -176.7\n",
      "Generation 26 | Episode 120 | Avg Reward -166.9\n",
      "Generation 26 | Episode 140 | Avg Reward -187.1\n",
      "Generation 26 | Episode 160 | Avg Reward -217.3\n",
      "Generation 26 | Episode 180 | Avg Reward -236.7\n",
      "Generation 26 | Episode 200 | Avg Reward -130.8\n",
      "Generation 26 Average Reward: -274.5\n",
      "Generation 27 | Episode 20 | Avg Reward -138.5\n",
      "Generation 27 | Episode 40 | Avg Reward -156.2\n",
      "Generation 27 | Episode 60 | Avg Reward -190.0\n",
      "Generation 27 | Episode 80 | Avg Reward -177.1\n",
      "Generation 27 | Episode 100 | Avg Reward -152.7\n",
      "Generation 27 | Episode 120 | Avg Reward -135.6\n",
      "Generation 27 | Episode 140 | Avg Reward -159.5\n",
      "Generation 27 | Episode 160 | Avg Reward -160.1\n",
      "Generation 27 | Episode 180 | Avg Reward -139.8\n",
      "Generation 27 | Episode 200 | Avg Reward -121.6\n",
      "Generation 27 Average Reward: -153.1\n",
      "Generation 28 | Episode 20 | Avg Reward -190.7\n",
      "Generation 28 | Episode 40 | Avg Reward -146.9\n",
      "Generation 28 | Episode 60 | Avg Reward -175.5\n",
      "Generation 28 | Episode 80 | Avg Reward -203.0\n",
      "Generation 28 | Episode 100 | Avg Reward -171.7\n",
      "Generation 28 | Episode 120 | Avg Reward -159.6\n",
      "Generation 28 | Episode 140 | Avg Reward -166.3\n",
      "Generation 28 | Episode 160 | Avg Reward -177.3\n",
      "Generation 28 | Episode 180 | Avg Reward -172.2\n",
      "Generation 28 | Episode 200 | Avg Reward -147.0\n",
      "Generation 28 Average Reward: -171.0\n",
      "Generation 29 | Episode 20 | Avg Reward -118.8\n",
      "Generation 29 | Episode 40 | Avg Reward -225.0\n",
      "Generation 29 | Episode 60 | Avg Reward -158.3\n",
      "Generation 29 | Episode 80 | Avg Reward -204.2\n",
      "Generation 29 | Episode 100 | Avg Reward -203.4\n",
      "Generation 29 | Episode 120 | Avg Reward -218.7\n",
      "Generation 29 | Episode 140 | Avg Reward -164.4\n",
      "Generation 29 | Episode 160 | Avg Reward -173.4\n",
      "Generation 29 | Episode 180 | Avg Reward -150.1\n",
      "Generation 29 | Episode 200 | Avg Reward -203.9\n",
      "Generation 29 Average Reward: -182.0\n",
      "Generation 30 | Episode 20 | Avg Reward -141.1\n",
      "Generation 30 | Episode 40 | Avg Reward -121.2\n",
      "Generation 30 | Episode 60 | Avg Reward -166.2\n",
      "Generation 30 | Episode 80 | Avg Reward -159.5\n",
      "Generation 30 | Episode 100 | Avg Reward -163.6\n",
      "Generation 30 | Episode 120 | Avg Reward -187.4\n",
      "Generation 30 | Episode 140 | Avg Reward -205.2\n",
      "Generation 30 | Episode 160 | Avg Reward -183.0\n",
      "Generation 30 | Episode 180 | Avg Reward -219.5\n",
      "Generation 30 | Episode 200 | Avg Reward -300.8\n",
      "Generation 30 Average Reward: -184.7\n",
      "Generation 31 | Episode 20 | Avg Reward -142.5\n",
      "Generation 31 | Episode 40 | Avg Reward -157.7\n",
      "Generation 31 | Episode 60 | Avg Reward -180.6\n",
      "Generation 31 | Episode 80 | Avg Reward -209.5\n",
      "Generation 31 | Episode 100 | Avg Reward -224.9\n",
      "Generation 31 | Episode 120 | Avg Reward -174.4\n",
      "Generation 31 | Episode 140 | Avg Reward -137.3\n",
      "Generation 31 | Episode 160 | Avg Reward -134.9\n",
      "Generation 31 | Episode 180 | Avg Reward -154.4\n",
      "Generation 31 | Episode 200 | Avg Reward -161.8\n",
      "Generation 31 Average Reward: -167.8\n",
      "Generation 32 | Episode 20 | Avg Reward -174.8\n",
      "Generation 32 | Episode 40 | Avg Reward -179.7\n",
      "Generation 32 | Episode 60 | Avg Reward -203.1\n",
      "Generation 32 | Episode 80 | Avg Reward -193.6\n",
      "Generation 32 | Episode 100 | Avg Reward -183.9\n",
      "Generation 32 | Episode 120 | Avg Reward -277.0\n",
      "Generation 32 | Episode 140 | Avg Reward -203.8\n",
      "Generation 32 | Episode 160 | Avg Reward -171.2\n",
      "Generation 32 | Episode 180 | Avg Reward -181.5\n",
      "Generation 32 | Episode 200 | Avg Reward -181.5\n",
      "Generation 32 Average Reward: -195.0\n",
      "Generation 33 | Episode 20 | Avg Reward -217.0\n",
      "Generation 33 | Episode 40 | Avg Reward -227.9\n",
      "Generation 33 | Episode 60 | Avg Reward -278.9\n",
      "Generation 33 | Episode 80 | Avg Reward -135.9\n",
      "Generation 33 | Episode 100 | Avg Reward -170.1\n",
      "Generation 33 | Episode 120 | Avg Reward -219.0\n",
      "Generation 33 | Episode 140 | Avg Reward -228.3\n",
      "Generation 33 | Episode 160 | Avg Reward -318.8\n",
      "Generation 33 | Episode 180 | Avg Reward -256.6\n",
      "Generation 33 | Episode 200 | Avg Reward -257.1\n",
      "Generation 33 Average Reward: -231.0\n",
      "Generation 34 | Episode 20 | Avg Reward -91.5\n",
      "Generation 34 | Episode 40 | Avg Reward -101.9\n",
      "Generation 34 | Episode 60 | Avg Reward -142.6\n",
      "Generation 34 | Episode 80 | Avg Reward -194.9\n",
      "Generation 34 | Episode 100 | Avg Reward -161.0\n",
      "Generation 34 | Episode 120 | Avg Reward -190.7\n",
      "Generation 34 | Episode 140 | Avg Reward -98.3\n",
      "Generation 34 | Episode 160 | Avg Reward -114.5\n",
      "Generation 34 | Episode 180 | Avg Reward -112.2\n",
      "Generation 34 | Episode 200 | Avg Reward -114.5\n",
      "Generation 34 Average Reward: -132.2\n",
      "Generation 35 | Episode 20 | Avg Reward -153.2\n",
      "Generation 35 | Episode 40 | Avg Reward -146.8\n",
      "Generation 35 | Episode 60 | Avg Reward -216.9\n",
      "Generation 35 | Episode 80 | Avg Reward -144.0\n",
      "Generation 35 | Episode 100 | Avg Reward -180.6\n",
      "Generation 35 | Episode 120 | Avg Reward -187.6\n",
      "Generation 35 | Episode 140 | Avg Reward -217.9\n",
      "Generation 35 | Episode 160 | Avg Reward -229.0\n",
      "Generation 35 | Episode 180 | Avg Reward -203.9\n",
      "Generation 35 | Episode 200 | Avg Reward -196.6\n",
      "Generation 35 Average Reward: -187.6\n",
      "Generation 36 | Episode 20 | Avg Reward -133.2\n",
      "Generation 36 | Episode 40 | Avg Reward -140.3\n",
      "Generation 36 | Episode 60 | Avg Reward -189.7\n",
      "Generation 36 | Episode 80 | Avg Reward -138.8\n",
      "Generation 36 | Episode 100 | Avg Reward -130.6\n",
      "Generation 36 | Episode 120 | Avg Reward -221.1\n",
      "Generation 36 | Episode 140 | Avg Reward -254.9\n",
      "Generation 36 | Episode 160 | Avg Reward -227.0\n",
      "Generation 36 | Episode 180 | Avg Reward -330.7\n",
      "Generation 36 | Episode 200 | Avg Reward -278.4\n",
      "Generation 36 Average Reward: -204.5\n",
      "Generation 37 | Episode 20 | Avg Reward -158.9\n",
      "Generation 37 | Episode 40 | Avg Reward -148.2\n",
      "Generation 37 | Episode 60 | Avg Reward -127.6\n",
      "Generation 37 | Episode 80 | Avg Reward -123.3\n",
      "Generation 37 | Episode 100 | Avg Reward -124.4\n",
      "Generation 37 | Episode 120 | Avg Reward -135.4\n",
      "Generation 37 | Episode 140 | Avg Reward -130.9\n",
      "Generation 37 | Episode 160 | Avg Reward -164.8\n",
      "Generation 37 | Episode 180 | Avg Reward -140.4\n",
      "Generation 37 | Episode 200 | Avg Reward -141.6\n",
      "Generation 37 Average Reward: -139.6\n",
      "Generation 38 | Episode 20 | Avg Reward -186.9\n",
      "Generation 38 | Episode 40 | Avg Reward -170.4\n",
      "Generation 38 | Episode 60 | Avg Reward -183.5\n",
      "Generation 38 | Episode 80 | Avg Reward -164.1\n",
      "Generation 38 | Episode 100 | Avg Reward -128.1\n",
      "Generation 38 | Episode 120 | Avg Reward -143.3\n",
      "Generation 38 | Episode 140 | Avg Reward -153.1\n",
      "Generation 38 | Episode 160 | Avg Reward -299.2\n",
      "Generation 38 | Episode 180 | Avg Reward -238.6\n",
      "Generation 38 | Episode 200 | Avg Reward -253.7\n",
      "Generation 38 Average Reward: -192.1\n",
      "Generation 39 | Episode 20 | Avg Reward -202.8\n",
      "Generation 39 | Episode 40 | Avg Reward -145.3\n",
      "Generation 39 | Episode 60 | Avg Reward -169.8\n",
      "Generation 39 | Episode 80 | Avg Reward -265.4\n",
      "Generation 39 | Episode 100 | Avg Reward -217.2\n",
      "Generation 39 | Episode 120 | Avg Reward -154.0\n",
      "Generation 39 | Episode 140 | Avg Reward -166.2\n",
      "Generation 39 | Episode 160 | Avg Reward -178.3\n",
      "Generation 39 | Episode 180 | Avg Reward -138.6\n",
      "Generation 39 | Episode 200 | Avg Reward -296.1\n",
      "Generation 39 Average Reward: -193.4\n",
      "Generation 40 | Episode 20 | Avg Reward -214.7\n",
      "Generation 40 | Episode 40 | Avg Reward -171.7\n",
      "Generation 40 | Episode 60 | Avg Reward -185.0\n",
      "Generation 40 | Episode 80 | Avg Reward -186.5\n",
      "Generation 40 | Episode 100 | Avg Reward -167.0\n",
      "Generation 40 | Episode 120 | Avg Reward -196.5\n",
      "Generation 40 | Episode 140 | Avg Reward -195.8\n",
      "Generation 40 | Episode 160 | Avg Reward -207.4\n",
      "Generation 40 | Episode 180 | Avg Reward -191.4\n",
      "Generation 40 | Episode 200 | Avg Reward -195.6\n",
      "Generation 40 Average Reward: -191.2\n",
      "Generation 41 | Episode 20 | Avg Reward -161.8\n",
      "Generation 41 | Episode 40 | Avg Reward -183.8\n",
      "Generation 41 | Episode 60 | Avg Reward -154.5\n",
      "Generation 41 | Episode 80 | Avg Reward -248.9\n",
      "Generation 41 | Episode 100 | Avg Reward -151.2\n",
      "Generation 41 | Episode 120 | Avg Reward -181.2\n",
      "Generation 41 | Episode 140 | Avg Reward -260.0\n",
      "Generation 41 | Episode 160 | Avg Reward -205.0\n",
      "Generation 41 | Episode 180 | Avg Reward -171.7\n",
      "Generation 41 | Episode 200 | Avg Reward -142.4\n",
      "Generation 41 Average Reward: -186.0\n",
      "Generation 42 | Episode 20 | Avg Reward -520.0\n",
      "Generation 42 | Episode 40 | Avg Reward -438.4\n",
      "Generation 42 | Episode 60 | Avg Reward -272.2\n",
      "Generation 42 | Episode 80 | Avg Reward -278.1\n",
      "Generation 42 | Episode 100 | Avg Reward -139.3\n",
      "Generation 42 | Episode 120 | Avg Reward -207.7\n",
      "Generation 42 | Episode 140 | Avg Reward -175.2\n",
      "Generation 42 | Episode 160 | Avg Reward -146.5\n",
      "Generation 42 | Episode 180 | Avg Reward -142.9\n",
      "Generation 42 | Episode 200 | Avg Reward -189.6\n",
      "Generation 42 Average Reward: -251.0\n",
      "Generation 43 | Episode 20 | Avg Reward -175.1\n",
      "Generation 43 | Episode 40 | Avg Reward -265.6\n",
      "Generation 43 | Episode 60 | Avg Reward -194.7\n",
      "Generation 43 | Episode 80 | Avg Reward -187.6\n",
      "Generation 43 | Episode 100 | Avg Reward -164.2\n",
      "Generation 43 | Episode 120 | Avg Reward -165.6\n",
      "Generation 43 | Episode 140 | Avg Reward -180.6\n",
      "Generation 43 | Episode 160 | Avg Reward -142.9\n",
      "Generation 43 | Episode 180 | Avg Reward -158.8\n",
      "Generation 43 | Episode 200 | Avg Reward -112.1\n",
      "Generation 43 Average Reward: -174.7\n",
      "Generation 44 | Episode 20 | Avg Reward -541.0\n",
      "Generation 44 | Episode 40 | Avg Reward -656.4\n",
      "Generation 44 | Episode 60 | Avg Reward -585.2\n",
      "Generation 44 | Episode 80 | Avg Reward -563.0\n",
      "Generation 44 | Episode 100 | Avg Reward -623.7\n",
      "Generation 44 | Episode 120 | Avg Reward -385.7\n",
      "Generation 44 | Episode 140 | Avg Reward -349.8\n",
      "Generation 44 | Episode 160 | Avg Reward -333.9\n",
      "Generation 44 | Episode 180 | Avg Reward -279.5\n",
      "Generation 44 | Episode 200 | Avg Reward -233.7\n",
      "Generation 44 Average Reward: -455.2\n",
      "Generation 45 | Episode 20 | Avg Reward -178.0\n",
      "Generation 45 | Episode 40 | Avg Reward -125.3\n",
      "Generation 45 | Episode 60 | Avg Reward -178.8\n",
      "Generation 45 | Episode 80 | Avg Reward -169.7\n",
      "Generation 45 | Episode 100 | Avg Reward -191.9\n",
      "Generation 45 | Episode 120 | Avg Reward -250.9\n",
      "Generation 45 | Episode 140 | Avg Reward -148.3\n",
      "Generation 45 | Episode 160 | Avg Reward -190.4\n",
      "Generation 45 | Episode 180 | Avg Reward -175.9\n",
      "Generation 45 | Episode 200 | Avg Reward -148.0\n",
      "Generation 45 Average Reward: -175.7\n",
      "Generation 46 | Episode 20 | Avg Reward -136.7\n",
      "Generation 46 | Episode 40 | Avg Reward -165.4\n",
      "Generation 46 | Episode 60 | Avg Reward -136.4\n",
      "Generation 46 | Episode 80 | Avg Reward -159.5\n",
      "Generation 46 | Episode 100 | Avg Reward -123.0\n",
      "Generation 46 | Episode 120 | Avg Reward -180.9\n",
      "Generation 46 | Episode 140 | Avg Reward -150.4\n",
      "Generation 46 | Episode 160 | Avg Reward -151.6\n",
      "Generation 46 | Episode 180 | Avg Reward -129.1\n",
      "Generation 46 | Episode 200 | Avg Reward -108.8\n",
      "Generation 46 Average Reward: -144.2\n",
      "Generation 47 | Episode 20 | Avg Reward -203.9\n",
      "Generation 47 | Episode 40 | Avg Reward -215.4\n",
      "Generation 47 | Episode 60 | Avg Reward -143.8\n",
      "Generation 47 | Episode 80 | Avg Reward -172.4\n",
      "Generation 47 | Episode 100 | Avg Reward -189.5\n",
      "Generation 47 | Episode 120 | Avg Reward -165.4\n",
      "Generation 47 | Episode 140 | Avg Reward -189.5\n",
      "Generation 47 | Episode 160 | Avg Reward -180.4\n",
      "Generation 47 | Episode 180 | Avg Reward -199.9\n",
      "Generation 47 | Episode 200 | Avg Reward -171.5\n",
      "Generation 47 Average Reward: -183.2\n",
      "Generation 48 | Episode 20 | Avg Reward -164.2\n",
      "Generation 48 | Episode 40 | Avg Reward -137.5\n",
      "Generation 48 | Episode 60 | Avg Reward -169.1\n",
      "Generation 48 | Episode 80 | Avg Reward -151.9\n",
      "Generation 48 | Episode 100 | Avg Reward -201.6\n",
      "Generation 48 | Episode 120 | Avg Reward -147.2\n",
      "Generation 48 | Episode 140 | Avg Reward -139.1\n",
      "Generation 48 | Episode 160 | Avg Reward -176.3\n",
      "Generation 48 | Episode 180 | Avg Reward -166.2\n",
      "Generation 48 | Episode 200 | Avg Reward -189.9\n",
      "Generation 48 Average Reward: -164.3\n",
      "Generation 49 | Episode 20 | Avg Reward -107.0\n",
      "Generation 49 | Episode 40 | Avg Reward -214.0\n",
      "Generation 49 | Episode 60 | Avg Reward -294.4\n",
      "Generation 49 | Episode 80 | Avg Reward -104.2\n",
      "Generation 49 | Episode 100 | Avg Reward -142.8\n",
      "Generation 49 | Episode 120 | Avg Reward -87.1\n",
      "Generation 49 | Episode 140 | Avg Reward -110.5\n",
      "Generation 49 | Episode 160 | Avg Reward -101.0\n",
      "Generation 49 | Episode 180 | Avg Reward -139.1\n",
      "Generation 49 | Episode 200 | Avg Reward -170.6\n",
      "Generation 49 Average Reward: -147.1\n",
      "Generation 50 | Episode 20 | Avg Reward -338.5\n",
      "Generation 50 | Episode 40 | Avg Reward -409.7\n",
      "Generation 50 | Episode 60 | Avg Reward -497.2\n",
      "Generation 50 | Episode 80 | Avg Reward -405.1\n",
      "Generation 50 | Episode 100 | Avg Reward -189.8\n",
      "Generation 50 | Episode 120 | Avg Reward -149.5\n",
      "Generation 50 | Episode 140 | Avg Reward -169.0\n",
      "Generation 50 | Episode 160 | Avg Reward -184.9\n",
      "Generation 50 | Episode 180 | Avg Reward -130.5\n",
      "Generation 50 | Episode 200 | Avg Reward -160.9\n",
      "Generation 50 Average Reward: -263.5\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import random\n",
    "\n",
    "sns.set()\n",
    "\n",
    "class ActorCriticNN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value\n",
    "\n",
    "\n",
    "class PPOTrainer():\n",
    "    def __init__(self, actor_critic, ppo_clip_val=0.2, target_kl_div=0.01,\n",
    "                 max_policy_train_iters=80, value_train_iters=80, \n",
    "                 policy_lr=3e-4, value_lr=1e-3):\n",
    "\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_val = ppo_clip_val\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.value_train_iters = value_train_iters\n",
    "\n",
    "        # Initialize optimizers\n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
    "    \n",
    "        value_params = list(self.ac.shared_layers.parameters()) + list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr=value_lr)\n",
    "\n",
    "        # Learning Rate Scheduler\n",
    "        self.policy_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.policy_optim, T_max=10000, eta_min=1e-5)\n",
    "        self.value_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.value_optim, T_max=10000, eta_min=1e-5)\n",
    "\n",
    "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "        for _ in range(self.max_policy_train_iters):\n",
    "            self.policy_optim.zero_grad()\n",
    "            new_logits = self.ac.policy_layers(self.ac.shared_layers(obs))\n",
    "            new_dist = Categorical(logits=new_logits)\n",
    "            new_log_probs = new_dist.log_prob(acts)\n",
    "\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            clipped_ratio = torch.clamp(ratio, 1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "            policy_loss = -torch.min(ratio * gaes, clipped_ratio * gaes).mean()\n",
    "\n",
    "            policy_loss.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "            if (old_log_probs - new_log_probs).mean() >= self.target_kl_div:\n",
    "                break\n",
    "\n",
    "        self.policy_scheduler.step()\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        for _ in range(self.value_train_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "            values = self.ac.value_layers(self.ac.shared_layers(obs))\n",
    "            value_loss = ((returns - values) ** 2).mean()\n",
    "\n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()\n",
    "\n",
    "        self.value_scheduler.step()\n",
    "\n",
    "\n",
    "def create_population(pop_size, state_dim, action_dim):\n",
    "    return [ActorCriticNN(state_dim, action_dim) for _ in range(pop_size)]\n",
    "\n",
    "\n",
    "def evaluate_population(population, env, max_steps=300):\n",
    "    fitness_scores = []\n",
    "    for policy in population:\n",
    "        total_reward = 0\n",
    "        obs, _ = env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "            logits, _ = policy(obs_tensor)\n",
    "            action = Categorical(logits=logits).sample()\n",
    "            next_obs, reward, done, _, _ = env.step(action.item())\n",
    "            total_reward += reward\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n",
    "        fitness_scores.append(total_reward)\n",
    "    return fitness_scores\n",
    "\n",
    "\n",
    "def select_parents(population, fitness_scores, num_parents):\n",
    "    sorted_indices = np.argsort(fitness_scores)[::-1]  # Sort indices for descending order\n",
    "    return [population[i] for i in sorted_indices[:num_parents]]\n",
    "\n",
    "\n",
    "def crossover(parent1, parent2, state_dim, action_dim):\n",
    "    child = ActorCriticNN(state_dim, action_dim)\n",
    "    for param1, param2, param_child in zip(parent1.parameters(), parent2.parameters(), child.parameters()):\n",
    "        param_child.data.copy_((param1.data + param2.data) / 2)\n",
    "    return child\n",
    "\n",
    "\n",
    "def mutate(policy, mutation_rate=0.01):\n",
    "    for param in policy.parameters():\n",
    "        if random.random() < mutation_rate:\n",
    "            param.data.add_(torch.randn_like(param) * 0.2)\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    discounted_rewards = np.zeros_like(rewards, dtype=np.float32)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    return discounted_rewards\n",
    "\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma=0.99, decay=0.97):\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = rewards + gamma * next_values - values\n",
    "    gaes = np.zeros_like(rewards, dtype=np.float32)\n",
    "    for t in reversed(range(len(deltas))):\n",
    "        gaes[t] = deltas[t] + (gamma * decay * gaes[t + 1] if t + 1 < len(deltas) else 0)\n",
    "    return gaes\n",
    "\n",
    "\n",
    "def rollout(model, env, max_steps=2000, initial_noise_std=0.1, noise_decay=0.99, generation=0):\n",
    "    train_data = [[], [], [], [], []]\n",
    "    obs, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    noise_std = initial_noise_std * (noise_decay ** generation)\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        logits, val = model(obs)\n",
    "        noisy_logits = logits + torch.randn_like(logits) * noise_std\n",
    "        action = Categorical(logits=noisy_logits).sample()\n",
    "        next_obs, reward, done, _, _ = env.step(action.item())\n",
    "        \n",
    "        train_data[0].append(obs)\n",
    "        train_data[1].append(action.item())\n",
    "        train_data[2].append(reward)\n",
    "        train_data[3].append(val.item())\n",
    "        train_data[4].append(noisy_logits[action].item())\n",
    "\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3])\n",
    "    return [np.array(data) for data in train_data], ep_reward\n",
    "\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make('LunarLander-v2')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Hyperparameters\n",
    "n_episodes = 200\n",
    "print_freq = 20\n",
    "pop_size = 30\n",
    "num_parents = 10\n",
    "num_generations = 50\n",
    "x = 15\n",
    "\n",
    "# Initialize population\n",
    "population = create_population(pop_size, state_dim, action_dim)\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    fitness_scores = evaluate_population(population, env)\n",
    "    parents = select_parents(population, fitness_scores, num_parents)\n",
    "    next_generation = population[:pop_size-x] + [crossover(*random.sample(parents, 2), state_dim, action_dim) for _ in range(x)]\n",
    "    for child in next_generation[-x:]:\n",
    "        mutate(child)\n",
    "\n",
    "    # Fine-tune best policy\n",
    "    best_policy = parents[0]\n",
    "    ppo = PPOTrainer(best_policy, policy_lr=3e-4, value_lr=1e-3)\n",
    "\n",
    "    ep_rewards = []\n",
    "    for episode_idx in range(n_episodes):\n",
    "        # Perform rollout\n",
    "        train_data, reward = rollout(best_policy, env)\n",
    "        ep_rewards.append(reward)\n",
    "\n",
    "        permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "        obs = torch.tensor(train_data[0][permute_idxs], dtype=torch.float32)\n",
    "        act = torch.tensor(train_data[1][permute_idxs], dtype=torch.int32)\n",
    "        gaes = torch.tensor(train_data[3][permute_idxs], dtype=torch.float32)\n",
    "        act_log_probs = torch.tensor(train_data[4][permute_idxs], dtype=torch.float32)\n",
    "\n",
    "        # Value Data\n",
    "        returns = discount_rewards(train_data[2])[permute_idxs]\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        # Train Policy\n",
    "        ppo.train_policy(obs, act, act_log_probs, gaes)\n",
    "        ppo.train_value(obs, returns)\n",
    "\n",
    "        # Print average reward every 'print_freq' episodes\n",
    "        if (episode_idx + 1) % print_freq == 0:\n",
    "            avg_reward = np.mean(ep_rewards[-print_freq:])  # Calculate the average of the last 'print_freq' rewards\n",
    "            print('Generation {} | Episode {} | Avg Reward {:.1f}'.format(\n",
    "                generation + 1, episode_idx + 1, avg_reward))\n",
    "\n",
    "    # Calculate and print the overall average reward for this generation\n",
    "    generation_avg_reward = np.mean(ep_rewards)  # Calculate average for all episodes in this generation\n",
    "    print(\"Generation {} Average Reward: {:.1f}\".format(generation + 1, generation_avg_reward))\n",
    "\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b393a289-8e2b-43e3-bd5c-aec2367ce6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05501c90-9f50-45a0-9109-687ae8afc4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 | Average Reward: -153.2\n",
      "Generation 2 | Average Reward: -494.6\n",
      "Generation 3 | Average Reward: -416.8\n",
      "Generation 4 | Average Reward: -178.6\n",
      "Generation 5 | Average Reward: -578.2\n",
      "Generation 6 | Average Reward: -573.6\n",
      "Generation 7 | Average Reward: -510.7\n",
      "Generation 8 | Average Reward: -155.8\n",
      "Generation 9 | Average Reward: -218.2\n",
      "Generation 10 | Average Reward: -206.9\n",
      "Generation 11 | Average Reward: -585.4\n",
      "Generation 12 | Average Reward: -204.1\n",
      "Generation 13 | Average Reward: -217.8\n",
      "Generation 14 | Average Reward: -309.4\n",
      "Generation 15 | Average Reward: -155.5\n",
      "Generation 16 | Average Reward: -273.1\n",
      "Generation 17 | Average Reward: -290.9\n",
      "Generation 18 | Average Reward: -274.8\n",
      "Generation 19 | Average Reward: -297.6\n",
      "Generation 20 | Average Reward: -355.1\n",
      "Generation 21 | Average Reward: -272.1\n",
      "Generation 22 | Average Reward: -403.6\n",
      "Generation 23 | Average Reward: -332.6\n",
      "Generation 24 | Average Reward: -247.9\n",
      "Generation 25 | Average Reward: -415.7\n",
      "Generation 26 | Average Reward: -551.2\n",
      "Generation 27 | Average Reward: -592.4\n",
      "Generation 28 | Average Reward: -134.0\n",
      "Generation 29 | Average Reward: -326.9\n",
      "Generation 30 | Average Reward: -306.7\n",
      "Generation 31 | Average Reward: -239.3\n",
      "Generation 32 | Average Reward: -312.3\n",
      "Generation 33 | Average Reward: -136.2\n",
      "Generation 34 | Average Reward: -183.6\n",
      "Generation 35 | Average Reward: -223.2\n",
      "Generation 36 | Average Reward: -176.5\n",
      "Generation 37 | Average Reward: -175.8\n",
      "Generation 38 | Average Reward: -107.4\n",
      "Generation 39 | Average Reward: -152.4\n",
      "Generation 40 | Average Reward: -138.3\n",
      "Generation 41 | Average Reward: -140.5\n",
      "Generation 42 | Average Reward: -192.7\n",
      "Generation 43 | Average Reward: -137.3\n",
      "Generation 44 | Average Reward: -263.1\n",
      "Generation 45 | Average Reward: -126.2\n",
      "Generation 46 | Average Reward: -196.0\n",
      "Generation 47 | Average Reward: -373.5\n",
      "Generation 48 | Average Reward: -345.4\n",
      "Generation 49 | Average Reward: -156.7\n",
      "Generation 50 | Average Reward: -125.9\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import random\n",
    "\n",
    "class ActorCriticNN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,action_dim))\n",
    "\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1))\n",
    "\n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "\n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value\n",
    "\n",
    "class PPOTrainer:\n",
    "    def __init__(self, actor_critic, ppo_clip_val=0.2, target_kl_div=0.01,\n",
    "                 max_policy_train_iters=80, value_train_iters=80, \n",
    "                 policy_lr=3e-4, value_lr=1e-3):\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_val = ppo_clip_val\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.value_train_iters = value_train_iters\n",
    "\n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
    "        value_params = list(self.ac.shared_layers.parameters()) + list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr=value_lr)\n",
    "\n",
    "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "        for _ in range(self.max_policy_train_iters):\n",
    "            self.policy_optim.zero_grad()\n",
    "            new_logits = self.ac.policy(obs)\n",
    "            new_logits = Categorical(logits=new_logits)\n",
    "            new_log_probs = new_logits.log_prob(acts)\n",
    "\n",
    "            policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            clipped_ratio = policy_ratio.clamp(1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "            policy_loss = -torch.min(policy_ratio * gaes, clipped_ratio * gaes).mean()\n",
    "\n",
    "            policy_loss.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "            kl_div = (old_log_probs - new_log_probs).mean()\n",
    "            if kl_div >= self.target_kl_div:\n",
    "                break\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "        for _ in range(self.value_train_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "            values = self.ac.value(obs)\n",
    "            value_loss = (returns - values) ** 2\n",
    "            value_loss = value_loss.mean()\n",
    "\n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()\n",
    "\n",
    "def evaluate_population(population, env, max_steps=200):\n",
    "    fitness_scores = []\n",
    "    for policy in population:\n",
    "        total_reward = 0\n",
    "        obs, _ = env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "            logits = policy.policy(obs_tensor)\n",
    "            action = Categorical(logits=logits).sample()\n",
    "            next_obs, reward, done, _, __ = env.step(action.item())\n",
    "            total_reward += reward\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n",
    "        fitness_scores.append(total_reward)\n",
    "    return fitness_scores\n",
    "\n",
    "def rollout(model, env, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Performs a single rollout, sampling an action and collecting data.\n",
    "    Returns training data in the shape (n_steps, observation_shape) and the cumulative reward.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the data storage (obs, act, reward, values, act_log_probs)\n",
    "    train_data = [[], [], [], [], []]  \n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    #print(\"Environment reset, initial observation:\", obs)  # Check if env.reset() works\n",
    "\n",
    "    ep_reward = 0\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        #Convert observation to a PyTorch tensor and ensure correct shape\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "        logits, val = model(obs)\n",
    "\n",
    "        # Create action distribution and sample an action\n",
    "        act_dist = Categorical(logits=logits)\n",
    "        act = act_dist.sample()\n",
    "        act_log_prob = act_dist.log_prob(act).item()\n",
    "\n",
    "        act, val = act.item(), val.item()\n",
    "\n",
    "        # Take action in the environment\n",
    "        next_obs, reward, done, _, __ = env.step(act)\n",
    "\n",
    "        # Store the data for training\n",
    "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
    "            train_data[i].append(item)\n",
    "\n",
    "        #print(f\"Action taken: {act.item()}, Reward received: {reward}, Done: {done}\")\n",
    "\n",
    "        # Update observation and cumulative reward\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "\n",
    "        # Break if the episode ends\n",
    "        if done:\n",
    "            #print(f\"Episode ended after {step+1} steps with cumulative reward: {ep_reward}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "    \n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3])\n",
    "    \n",
    "    return train_data, ep_reward\n",
    "\n",
    "def select_parents(population, fitness_scores, num_parents):\n",
    "    sorted_indices = np.argsort(fitness_scores)[::-1]\n",
    "    return [population[i] for i in sorted_indices[:num_parents]]\n",
    "\n",
    "def crossover(parent1, parent2, state_dim, action_dim):\n",
    "    child = ActorCriticNN(state_dim, action_dim)\n",
    "    for param1, param2, param_child in zip(parent1.parameters(), parent2.parameters(), child.parameters()):\n",
    "        param_child.data.copy_((param1.data + param2.data) / 2)\n",
    "    return child\n",
    "\n",
    "def mutate(policy, mutation_rate=0.02):\n",
    "    for param in policy.parameters():\n",
    "        if random.random() < mutation_rate:\n",
    "            param.data += torch.randn_like(param) * 0.05\n",
    "\n",
    "def discount_rewards(rewards, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Return discounted rewards based on the given rewards and the gamma param.\n",
    "    \"\"\"\n",
    "\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma = 0.99, decay = 0.97):\n",
    "    \"\"\"\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/abs/1506.02438\n",
    "    \"\"\"\n",
    "\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
    "\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "    return np.array(gaes[::-1])\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "state_dim, action_dim = env.observation_space.shape[0], env.action_space.n\n",
    "\n",
    "pop_size, num_parents, num_generations = 30, 10, 50\n",
    "population = [ActorCriticNN(state_dim, action_dim) for _ in range(pop_size)]\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    fitness_scores = evaluate_population(population, env)\n",
    "    parents = select_parents(population, fitness_scores, num_parents)\n",
    "\n",
    "    next_generation = []\n",
    "    for _ in range(pop_size - num_parents):\n",
    "        parent1, parent2 = random.sample(parents, 2)\n",
    "        child = crossover(parent1, parent2, state_dim, action_dim)\n",
    "        mutate(child)\n",
    "        next_generation.append(child)\n",
    "    \n",
    "    next_generation.extend(parents)  # Include top-performing parents directly\n",
    "    \n",
    "    population = next_generation\n",
    "\n",
    "    best_policy = parents[0]\n",
    "    ppo = PPOTrainer(best_policy)\n",
    "\n",
    "    ep_rewards = []\n",
    "    for episode_idx in range(200):\n",
    "        train_data, reward = rollout(best_policy, env)\n",
    "        ep_rewards.append(reward)\n",
    "\n",
    "        obs = torch.tensor(train_data[0], dtype=torch.float32)\n",
    "        acts = torch.tensor(train_data[1], dtype=torch.int64)\n",
    "        gaes = torch.tensor(train_data[3], dtype=torch.float32)\n",
    "        act_log_probs = torch.tensor(train_data[4], dtype=torch.float32)\n",
    "        returns = torch.tensor(discount_rewards(train_data[2]), dtype=torch.float32)\n",
    "\n",
    "        ppo.train_policy(obs, acts, act_log_probs, gaes)\n",
    "        ppo.train_value(obs, returns)\n",
    "\n",
    "    print(f'Generation {generation+1} | Average Reward: {np.mean(ep_rewards):.1f}')\n",
    "    env.reset()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
