{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f839e94-d0b5-4ddc-aed3-181f94f6e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6aa78070-ef16-48cc-93d0-63bff82ce543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a850506-326f-4768-b630-4e29de637ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: (array([ 0.02242215, -0.01580391,  0.04297751,  0.00947539], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(\"Initial state:\", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a88a5de0-89b5-4651-a878-08b793e3559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(nn.Module):\n",
    "    def __init__(self,state_dim,action_dim):\n",
    "        super(Action, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, 128)\n",
    "        self.l2 = nn.Linear(128, 128)\n",
    "        self.l3 = nn.Linear(128, 128)\n",
    "        self.l4 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x_self):\n",
    "        x_self = F.relu(self.l1(x_self))\n",
    "        x_self = F.relu(self.l2(x_self))\n",
    "        x_self = F.relu(self.l3(x_self))\n",
    "        x_self = self.l4(x_self)\n",
    "        \n",
    "        return F.softmax(x_self, dim=-1)\n",
    "        #This applies the Softmax function to the output of the last layer, transforming the raw scores into probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8efa98ae-e0f5-48fd-914b-07a097ba3b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    #The critic evaluates the action taken by the actor (another component of the actor-critic framework) by estimating the value of the current state.\n",
    "    #It helps provide the actor with feedback about how good or bad the actions taken are, facilitating the learning of a more effective policy.\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, 128)\n",
    "        self.l2 = nn.Linear(128,128)\n",
    "        self.l3 = nn.Linear(128,128)\n",
    "        self.l4 = nn.Linear(128,1)\n",
    "        # means that this layer takes an input of size 128 and outputs a size of 1\n",
    "\n",
    "    def forward(self, x_self):\n",
    "        x_self = F.relu(self.l1(x_self))\n",
    "        x_self = F.relu(self.l2(x_self))\n",
    "        x_self = F.relu(self.l3(x_self))\n",
    "        x_self = self.l4(x_self)\n",
    "        return x_self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "3fa04ffe-aafc-4c78-b6b4-6c5174ab4a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy_gradient(policy_net, states, actions, advantages):\n",
    "    #Calculates the loss to update the policy network based on the actions taken and their advantages.\n",
    "\n",
    "    states_tensor = torch.FloatTensor(np.array(states)) # Make sure states is a tensor\n",
    "    action = torch.FloatTensor(np.array(actions))\n",
    "    advanatages_tens = torch.FloatTensor(np.array(advantages.detach().numpy()))\n",
    "    \n",
    "    dist = torch.distributions.Categorical(policy_net(states_tensor)) #This distribution is used to model the probabilities of selecting each action\n",
    "    log_probs = dist.log_prob(action) #calculates the log probability of the actions that were taken, given the policy defined by the network\n",
    "    policy_loss = -(log_probs * advantages).mean()\n",
    "    # This computes the policy loss. The objective is to maximize the expected return, which is done by minimizing the negative log probability weighted by the advantages. Higher advantages will lead to a stronger push to increase the probability of those actions\n",
    "    \n",
    "    return policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "fad3c8d7-95d5-4ebc-bf10-69b8a5bb09d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_divergence(new_policy_net, old_policy_net, states):\n",
    "    #This function calculates the KL divergence between the new policy and the old policy. KL divergence measures how one probability distribution diverges from a second, expected probability distribution.\n",
    "    # Measures how much the new policy diverges from the old policy, which can be useful for ensuring that policy updates do not change the policy too drastically (often employed in trust region methods).\n",
    "\n",
    "    states_tensor = torch.FloatTensor(np.array(states))\n",
    "    \n",
    "    old_dist = torch.distributions.Categorical(old_policy_net(states_tensor))\n",
    "    new_dist = torch.distributions.Categorical(new_policy_net(states_tensor))\n",
    "    kl_div = torch.distributions.kl.kl_divergence(new_dist, old_dist).mean()\n",
    "    # This computes the KL divergence between the new and old policies. The mean is taken to summarize the divergence over the batch of states\n",
    "\n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "845e5684-6a0f-4cf1-ad08-fbdf438921bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(Avp, b, nsteps, epsilon=1e-10):\n",
    "    \n",
    "    #Avp: A function that computes the product of a matrix ùê¥ with a vector ùëù. This is useful for scenarios where you cannot directly compute ùê¥ or it is too large to store explicitly\n",
    "    #b: The right-hand side vector in the equation Ax = b\n",
    "    #nsteps : the number of iterations\n",
    "    #epsilon: A small number to avoid division by zero. This is used to ensure numerical stability, although it's not actively utilized in the provided code.\n",
    "\n",
    "    # Ensure b is 1D\n",
    "    if b.dim() == 0:\n",
    "        b = b.view(1)  # Convert scalar to 1D tensor\n",
    "    \n",
    "    solution_vec = torch.zeros_like(b) #This vector will store the solution, initialized to zeros.\n",
    "    residual_vec = b.clone() #The residual vector, initialized to ùëè. It represents the difference between the left-hand side and the right-hand side of the equation and indicates how far the current estimate is from the true solution.\n",
    "    search_direction = b.clone() #The search direction, initialized to ùëè. This direction is updated during each iteration of the algorithm.\n",
    "    rdotr = torch.dot(residual_vec, residual_vec) #The dot product of the residual with itself, which is a measure of how far the current solution is from satisfying Ax = b\n",
    "    for _ in range(nsteps):\n",
    "        Avp_ = Avp(search_direction) #Computes A*P using provided function\n",
    "        alpha = rdotr / torch.dot(search_direction, Avp_) #Determines how far to move along the direction ùëù to reduce the residual. This is based on the steepest descent method\n",
    "        solution_vec += alpha * search_direction #Updates the solution vector by moving in the direction of p\n",
    "        solution_vec -= alpha * Avp_ #Updates the residual to reflect how much closer the current estimate is to the solution\n",
    "        new_rdotr = torch.dot(residual_vec, residual_vec) #Calculates new dot product of the residual to see how much it has changed\n",
    "        beta = new_rdotr / rdotr #computes parameter for updating the search direction\n",
    "        search_direction = residual_vec + beta * seacrh_direction #Adjusts the search direction for the next iteration\n",
    "        rdotr = new_rdotr\n",
    "    return solution_vec\n",
    "\n",
    "#The conjugate_gradient function efficiently solves the linear system Ax = b by iteratively refining an initial guess for ùë•. It uses the properties of the Conjugate Gradient method to ensure that each search direction is optimal with respect to the residual, making it particularly useful in large-scale optimization problems found in machine learning and numerical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "43342dc0-8570-42e7-8068-f9b49ceaa3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRO(policy_net, old_policy_net, states, actions, advantages, step_direction, kl_div, max_iterations=10):\n",
    "    for _ in range(max_iterations):\n",
    "        proposed_update = step_direction\n",
    "\n",
    "        if kl_div <= delta:\n",
    "            return proposed_update\n",
    "        else:\n",
    "            proposed_update *= 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "8fbe250f-b3bc-43e7-b407-2ac9a3f97940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectory(env, policy_net, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Collects a trajectory of states, actions, rewards, next states, and done flags \n",
    "    by interacting with the specified environment using the provided policy network.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The environment to interact with (e.g., a Gym environment).\n",
    "    - policy_net: The neural network model used to select actions based on state input.\n",
    "    - max_steps: The maximum number of steps to take in the environment during this trajectory.\n",
    "\n",
    "    Returns:\n",
    "    - states: A list of states observed during the trajectory.\n",
    "    - actions: A list of actions taken by the agent.\n",
    "    - rewards: A list of rewards received for each action taken.\n",
    "    - next_states: A list of next states observed after taking actions.\n",
    "    - dones: A list indicating whether each state was terminal (done).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to store trajectory data\n",
    "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "    \n",
    "    # Reset the environment to get the initial state\n",
    "    state = env.reset()  # Obtain the initial observation from the environment\n",
    "    print(\"Initial state:\", state)  # Print the initial state for debugging\n",
    "\n",
    "    state_array = state[0]  # Extract the first element of the state (the observation)\n",
    "    state_tens = torch.FloatTensor(state_array).unsqueeze(0)  # Convert to tensor and reshape to [1, state_dim]\n",
    "\n",
    "    # Loop for a maximum number of steps\n",
    "    for _ in range(max_steps):\n",
    "        # Get action probabilities from the policy network based on the current state\n",
    "        action_probs = policy_net(state_tens)\n",
    "        \n",
    "        # Sample an action based on the action probabilities\n",
    "        action = torch.multinomial(action_probs, 1).item()  # Select an action stochastically\n",
    "\n",
    "        # Take a step in the environment using the selected action\n",
    "        step_result = env.step(action)  # Interact with the environment\n",
    "        print(\"Step result:\", step_result)  # Print the result of the step for debugging\n",
    "        \n",
    "        # Check the structure of the result from the environment\n",
    "        if isinstance(step_result, tuple):\n",
    "            print(\"Step result is a tuple with length:\", len(step_result))\n",
    "\n",
    "        # Unpack the result based on its length (the expected output of env.step)\n",
    "        if len(step_result) == 4:\n",
    "            next_state, reward, done, info = step_result  # Unpack the 4 expected values\n",
    "        elif len(step_result) == 5:  # Handle cases with additional return values\n",
    "            next_state, reward, done, info, additional_info = step_result\n",
    "        else:\n",
    "            print(\"Unexpected step result format:\", step_result)  # Print an error message\n",
    "            break  # Exit the loop if the result format is not as expected\n",
    "        \n",
    "        # Print the next state for debugging\n",
    "        print(\"Next state:\", next_state)  # Output the next state for inspection\n",
    "\n",
    "        # Extract the next state\n",
    "        next_state_array = next_state[0] if isinstance(next_state, tuple) else next_state  # Handle tuple formats\n",
    "        if isinstance(next_state_array, np.ndarray):\n",
    "            # Convert next state to tensor and reshape it for consistency\n",
    "            next_state_tens = torch.FloatTensor(next_state_array).unsqueeze(0)  # Reshape for next state\n",
    "        else:\n",
    "            print(\"Unexpected next_state format:\", next_state_array)  # Print an error message\n",
    "            continue  # Skip further processing if the format is unexpected\n",
    "\n",
    "        # Store the current state, action, reward, next state, and done flag for later analysis\n",
    "        states.append(state_tens.squeeze(0).numpy())  # Convert state tensor to NumPy array\n",
    "        actions.append(action)  # Store the action taken\n",
    "        rewards.append(reward)  # Store the received reward\n",
    "        next_states.append(next_state_tens.squeeze(0).numpy())  # Convert next state tensor to NumPy\n",
    "        dones.append(done)  # Store whether the episode is done\n",
    "\n",
    "        # Update the current state tensor to the next state tensor for the next iteration\n",
    "        state_tens = next_state_tens  # Move to the next state\n",
    "\n",
    "        # Check if the episode is done; if so, exit the loop\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Return the collected trajectory data\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "\n",
    "# Had to make sure that the environment was returning the needed states, then I realized it was returning it in a tuple format so we had to unpack it and make it a tensor.\n",
    "# Had to resize the data so it was in a format that our models could input\n",
    "# Our models intake tensors so always check the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "0e7b2e20-daa2-4f90-8141-b25399b76a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages_and_returns(rewards, states, value_net, gamma=0.95, lam=0.95):\n",
    "    \"\"\"\n",
    "    Computes advantages and returns using Generalized Advantage Estimation (GAE).\n",
    "    \n",
    "    Parameters:\n",
    "    - rewards: List of rewards from the trajectory.\n",
    "    - states: List of states from the trajectory.\n",
    "    - value_net: A neural network model that estimates the value of each state.\n",
    "    - gamma: Discount factor for future rewards.\n",
    "    - lam: Lambda for GAE, controlling the bias-variance tradeoff.\n",
    "\n",
    "    Returns:\n",
    "    - advantages: Computed advantages for each state.\n",
    "    - returns: Computed returns for each state.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "\n",
    "    # Get state values from the value network\n",
    "    state_values = value_net(states).squeeze()  # Assuming value_net returns a tensor of shape [N, 1]\n",
    "\n",
    "    # Calculate the returns and advantages\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "\n",
    "    # Compute returns and advantages in reverse order\n",
    "    # By iterating backward, the method can use the value of future states to compute the current state‚Äôs advantage effectively\n",
    "    # The delta represents the temporal difference error, which is the difference between the actual reward plus the discounted value of the next state and the estimated value of the current state.\n",
    "    \n",
    "    for t in reversed(range(len(rewards))):\n",
    "        if t == len(rewards) - 1:\n",
    "            delta = rewards[t] + gamma * state_values[t] - state_values[t]  # Last state value (next state)\n",
    "        else:\n",
    "            delta = rewards[t] + gamma * state_values[t + 1] - state_values[t]\n",
    "\n",
    "        advantages[t] = delta + gamma * lam * advantages[t + 1] if t + 1 < len(rewards) else delta\n",
    "        returns[t] = advantages[t] + state_values[t]  # R_t = A_t + V(s_t)\n",
    "\n",
    "    return torch.FloatTensor(advantages).unsqueeze(0), torch.FloatTensor(returns).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "98fcee5f-cea5-4e57-b1c3-d6e1ec8b7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Avp(vector):\n",
    "    # Compute the product of the Fisher information matrix and the vector\n",
    "    # This often involves using the policy network to compute gradients\n",
    "    # and then take the dot product with the vector.\n",
    "    policy_net.zero_grad()  # Clear previous gradients\n",
    "    \n",
    "    # Calculate the gradient of the policy loss\n",
    "    loss = compute_policy_gradient(policy_net, states, actions, advantages)\n",
    "    loss.backward(retain_graph=True)  # Backpropagate to compute gradients\n",
    "\n",
    "    # Get the gradient and flatten it\n",
    "    grad = torch.cat([param.grad.view(-1) for param in policy_net.parameters()])\n",
    "\n",
    "    # Initialize the Fisher information product\n",
    "    fisher_info_product = torch.zeros_like(grad)\n",
    "\n",
    "    # Loop through parameters to compute Fisher information product\n",
    "    offset = 0\n",
    "    for param in policy_net.parameters():\n",
    "        param_grad = param.grad.view(-1)  # Flatten the gradient\n",
    "        param_size = param_grad.size(0)  # Total number of elements in the flattened gradient\n",
    "\n",
    "        if param_grad is not None:\n",
    "            # Check the size of the vector for compatibility\n",
    "            vector_slice = vector[offset:offset + param_size]\n",
    "\n",
    "            # Ensure the sizes are compatible\n",
    "            if vector_slice.size(0) != param_size:\n",
    "                raise ValueError(f\"Size mismatch: vector_slice size {vector_slice.size(0)} does not match param_grad size {param_size}\")\n",
    "\n",
    "            # Compute the Fisher information product\n",
    "            fisher_info_product[offset:offset + param_size] += (param_grad * vector_slice).sum() * param_grad\n",
    "        \n",
    "        offset += param_size  # Update the offset\n",
    "\n",
    "    return fisher_info_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "6f6c1e69-127d-4cc0-b5ed-4886c758b64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: (array([-0.03367027,  0.03490659,  0.02899447,  0.03661744], dtype=float32), {})\n",
      "Step result: (array([-0.03297214, -0.16061889,  0.02972682,  0.33830556], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [-0.03297214 -0.16061889  0.02972682  0.33830556]\n",
      "Step result: (array([-0.03618452,  0.03406772,  0.03649293,  0.05514307], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [-0.03618452  0.03406772  0.03649293  0.05514307]\n",
      "Step result: (array([-0.03550316, -0.16155796,  0.03759579,  0.35911277], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [-0.03550316 -0.16155796  0.03759579  0.35911277]\n",
      "Step result: (array([-0.03873432,  0.03300994,  0.04477805,  0.07851771], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [-0.03873432  0.03300994  0.04477805  0.07851771]\n",
      "Step result: (array([-0.03807412,  0.22746232,  0.0463484 , -0.19970815], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [-0.03807412  0.22746232  0.0463484  -0.19970815]\n",
      "Step result: (array([-0.03352488,  0.03170917,  0.04235423,  0.1072278 ], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [-0.03352488  0.03170917  0.04235423  0.1072278 ]\n",
      "Step result: (array([-0.0328907 ,  0.22619939,  0.04449879, -0.17179747], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [-0.0328907   0.22619939  0.04449879 -0.17179747]\n",
      "Step result: (array([-0.02836671,  0.42065713,  0.04106284, -0.45011702], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [-0.02836671  0.42065713  0.04106284 -0.45011702]\n",
      "Step result: (array([-0.01995357,  0.61517495,  0.0320605 , -0.72957873], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [-0.01995357  0.61517495  0.0320605  -0.72957873]\n",
      "Step result: (array([-0.00765007,  0.8098394 ,  0.01746893, -1.0120013 ], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [-0.00765007  0.8098394   0.01746893 -1.0120013 ]\n",
      "Step result: (array([ 0.00854672,  0.61448884, -0.0027711 , -0.7138844 ], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [ 0.00854672  0.61448884 -0.0027711  -0.7138844 ]\n",
      "Step result: (array([ 0.0208365 ,  0.80964905, -0.01704879, -1.0074383 ], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [ 0.0208365   0.80964905 -0.01704879 -1.0074383 ]\n",
      "Step result: (array([ 0.03702948,  1.0049944 , -0.03719755, -1.3054259 ], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [ 0.03702948  1.0049944  -0.03719755 -1.3054259 ]\n",
      "Step result: (array([ 0.05712937,  0.8103633 , -0.06330607, -1.0246147 ], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [ 0.05712937  0.8103633  -0.06330607 -1.0246147 ]\n",
      "Step result: (array([ 0.07333663,  1.0062686 , -0.08379836, -1.3364835 ], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [ 0.07333663  1.0062686  -0.08379836 -1.3364835 ]\n",
      "Step result: (array([ 0.09346201,  1.2023404 , -0.11052804, -1.654167  ], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [ 0.09346201  1.2023404  -0.11052804 -1.654167  ]\n",
      "Step result: (array([ 0.11750881,  1.3985654 , -0.14361137, -1.9791377 ], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [ 0.11750881  1.3985654  -0.14361137 -1.9791377 ]\n",
      "Step result: (array([ 0.14548013,  1.5948776 , -0.18319413, -2.3126514 ], dtype=float32), 1.0, False, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [ 0.14548013  1.5948776  -0.18319413 -2.3126514 ]\n",
      "Step result: (array([ 0.17737767,  1.7911413 , -0.22944716, -2.655679  ], dtype=float32), 1.0, True, False, {})\n",
      "Step result is a tuple with length: 5\n",
      "Next state: [ 0.17737767  1.7911413  -0.22944716 -2.655679  ]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Size mismatch: vector_slice size 1 does not match param_grad size 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[407], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m policy_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# print(policy_loss.grad)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m step_direction \u001b[38;5;241m=\u001b[39m conjugate_gradient(Avp, policy_loss\u001b[38;5;241m.\u001b[39mgrad, nsteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Perform line search to find the optimal step size\u001b[39;00m\n\u001b[1;32m     33\u001b[0m step_size \u001b[38;5;241m=\u001b[39m TRO(policy_net, old_policy_net, states, actions, advantages, step_direction, kl_div)\n",
      "Cell \u001b[0;32mIn[343], line 17\u001b[0m, in \u001b[0;36mconjugate_gradient\u001b[0;34m(Avp, b, nsteps, epsilon)\u001b[0m\n\u001b[1;32m     15\u001b[0m rdotr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdot(residual_vec, residual_vec) \u001b[38;5;66;03m#The dot product of the residual with itself, which is a measure of how far the current solution is from satisfying Ax = b\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nsteps):\n\u001b[0;32m---> 17\u001b[0m     Avp_ \u001b[38;5;241m=\u001b[39m Avp(search_direction) \u001b[38;5;66;03m#Computes A*P using provided function\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m rdotr \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mdot(search_direction, Avp_) \u001b[38;5;66;03m#Determines how far to move along the direction ùëù to reduce the residual. This is based on the steepest descent method\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     solution_vec \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m search_direction \u001b[38;5;66;03m#Updates the solution vector by moving in the direction of p\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[400], line 29\u001b[0m, in \u001b[0;36mAvp\u001b[0;34m(vector)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Ensure the sizes are compatible\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vector_slice\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m param_size:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch: vector_slice size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvector_slice\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match param_grad size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Compute the Fisher information product\u001b[39;00m\n\u001b[1;32m     32\u001b[0m fisher_info_product[offset:offset \u001b[38;5;241m+\u001b[39m param_size] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (param_grad \u001b[38;5;241m*\u001b[39m vector_slice)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m*\u001b[39m param_grad\n",
      "\u001b[0;31mValueError\u001b[0m: Size mismatch: vector_slice size 1 does not match param_grad size 512"
     ]
    }
   ],
   "source": [
    "policy_net = Action(state_dim=env.observation_space.shape[0], action_dim=env.action_space.n)\n",
    "old_policy_net = Action(state_dim=env.observation_space.shape[0], action_dim=env.action_space.n)\n",
    "value_net = Critic(state_dim=env.observation_space.shape[0])\n",
    "\n",
    "# Optimizers\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "value_optimizer = optim.Adam(value_net.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #print(collect_trajectory(env, policy_net))\n",
    "    states, actions, rewards, next_states, dones = collect_trajectory(env, policy_net)\n",
    "    \n",
    "    # Compute advantages and returns\n",
    "    advantages, returns = compute_advantages_and_returns(rewards, states, value_net)\n",
    "    \n",
    "    # Compute policy gradient and KL divergence\n",
    "    policy_loss = compute_policy_gradient(policy_net, states, actions, advantages)\n",
    "    kl_div = compute_kl_divergence(policy_net, old_policy_net, states)\n",
    "    \n",
    "    # Compute the step direction using conjugate gradient\n",
    "    # print(policy_loss)\n",
    "    # print(policy_loss.shape)  # Should be a scalar (0-dimensional)\n",
    "    policy_loss.retain_grad()\n",
    "    policy_loss.backward(retain_graph=True)\n",
    "    policy_optimizer.zero_grad()\n",
    "    \n",
    "    # print(policy_loss.grad)\n",
    "    step_direction = conjugate_gradient(Avp, policy_loss.grad, nsteps=10)\n",
    "    \n",
    "    # Perform line search to find the optimal step size\n",
    "    step_size = TRO(policy_net, old_policy_net, states, actions, advantages, step_direction, kl_div)\n",
    "    \n",
    "    # Update policy network\n",
    "    for param, step in zip(policy_net.parameters(), step_direction):\n",
    "        param.data += step_size * step\n",
    "    \n",
    "    # Update value network\n",
    "    value_loss = compute_value_loss(value_net, states, returns)\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "    value_optimizer.zero_grad()\n",
    "\n",
    "    # Update old policy network\n",
    "    old_policy_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    #render the environment\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71e648-41c2-48d3-be30-a3f89da26003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
