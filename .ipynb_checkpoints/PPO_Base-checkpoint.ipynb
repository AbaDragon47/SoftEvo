{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf59ce8-b6cd-497c-af38-38c7f7ab7788",
   "metadata": {},
   "source": [
    "PPO MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "671b88a0-4948-4740-8a47-9ac539bc0884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/anaconda3/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /opt/anaconda3/lib/python3.12/site-packages (from gym) (0.0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73136781-c30c-4333-966e-1cacd00a954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchrl\n",
      "  Downloading torchrl-0.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: torch>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchrl) (2.4.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchrl) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from torchrl) (23.2)\n",
      "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.12/site-packages (from torchrl) (2.2.1)\n",
      "Collecting tensordict>=0.5.0 (from torchrl)\n",
      "  Downloading tensordict-0.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (22 kB)\n",
      "Collecting orjson (from tensordict>=0.5.0->torchrl)\n",
      "  Downloading orjson-3.10.7-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.3.0->torchrl) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.3.0->torchrl) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.3.0->torchrl) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.3.0->torchrl) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.3.0->torchrl) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.3.0->torchrl) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.3.0->torchrl) (69.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.3.0->torchrl) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=2.3.0->torchrl) (1.3.0)\n",
      "Downloading torchrl-0.5.0-cp312-cp312-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensordict-0.5.0-cp312-cp312-macosx_11_0_arm64.whl (648 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.4/648.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.7-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (251 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.4/251.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: orjson, tensordict, torchrl\n",
      "Successfully installed orjson-3.10.7 tensordict-0.5.0 torchrl-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "435d30d1-043d-497e-9a17-7f3109b94001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mujoco in /opt/anaconda3/lib/python3.12/site-packages (3.2.3)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/lib/python3.12/site-packages (from mujoco) (2.1.0)\n",
      "Requirement already satisfied: etils[epath] in /opt/anaconda3/lib/python3.12/site-packages (from mujoco) (1.9.4)\n",
      "Requirement already satisfied: glfw in /opt/anaconda3/lib/python3.12/site-packages (from mujoco) (2.7.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from mujoco) (1.26.4)\n",
      "Requirement already satisfied: pyopengl in /opt/anaconda3/lib/python3.12/site-packages (from mujoco) (3.1.7)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from etils[epath]->mujoco) (2024.3.1)\n",
      "Requirement already satisfied: importlib_resources in /opt/anaconda3/lib/python3.12/site-packages (from etils[epath]->mujoco) (6.4.5)\n",
      "Requirement already satisfied: typing_extensions in /opt/anaconda3/lib/python3.12/site-packages (from etils[epath]->mujoco) (4.11.0)\n",
      "Requirement already satisfied: zipp in /opt/anaconda3/lib/python3.12/site-packages (from etils[epath]->mujoco) (3.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48cc651-fbba-47e3-ba83-b8f5cee99298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import random\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "728c3c4f-f101-484e-a082-ebcc3a4be37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,action_dim))\n",
    "\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1))\n",
    "\n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "\n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3fcdae-a4aa-442a-98ad-48c38da4b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer\n",
    "class PPOTrainer():\n",
    "    def __init__(self, actor_critic, ppo_clip_val = 0.2, target_kl_div = 0.01, max_policy_train_iters = 80, value_train_iters=80, \n",
    "                policy_lr = 3e-4, value_lr = 1e-2):\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_val = ppo_clip_val\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.value_train_iters = value_train_iters\n",
    "    \n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr = policy_lr)\n",
    "    \n",
    "        value_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr = value_lr)\n",
    "\n",
    "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "\n",
    "        for _ in range(self.max_policy_train_iters):\n",
    "                \n",
    "            self.policy_optim.zero_grad()\n",
    "    \n",
    "            new_logits = self.ac.policy(obs)\n",
    "            new_logits = Categorical(logits = new_logits)\n",
    "            new_log_probs = new_logits.log_prob(acts)\n",
    "    \n",
    "            policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            clipped_ratio = policy_ratio.clamp(1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "            clipped_loss = clipped_ratio * gaes\n",
    "            full_loss = policy_ratio * gaes\n",
    "            policy_loss = -torch.min(full_loss, clipped_loss).mean()\n",
    "    \n",
    "            policy_loss.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "            kl_div = (old_log_probs - new_log_probs).mean()\n",
    "            if kl_div >= self.target_kl_div:\n",
    "                break\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "\n",
    "        for _ in range (self.value_train_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "    \n",
    "            values = self.ac.value(obs)\n",
    "            value_loss = (returns-values) ** 2\n",
    "            value_loss = value_loss.mean()\n",
    "    \n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ea4ba4e-5e76-46b5-bef9-9efc44097c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Return discounted rewards based on the given rewards and the gamma param.\n",
    "    \"\"\"\n",
    "\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma = 0.99, decay = 0.97):\n",
    "    \"\"\"\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/abs/1506.02438\n",
    "    \"\"\"\n",
    "\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
    "\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "    return np.array(gaes[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dbb31d0-666d-46d3-ae22-17dc583d9ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, env, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Performs a single rollout, sampling an action and collecting data.\n",
    "    Returns training data in the shape (n_steps, observation_shape) and the cumulative reward.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the data storage (obs, act, reward, values, act_log_probs)\n",
    "    train_data = [[], [], [], [], []]  \n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    #print(\"Environment reset, initial observation:\", obs)  # Check if env.reset() works\n",
    "\n",
    "    ep_reward = 0\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        #Convert observation to a PyTorch tensor and ensure correct shape\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "        logits, val = model(obs)\n",
    "\n",
    "        # Create action distribution and sample an action\n",
    "        act_dist = Categorical(logits=logits)\n",
    "        act = act_dist.sample()\n",
    "        act_log_prob = act_dist.log_prob(act).item()\n",
    "\n",
    "        act, val = act.item(), val.item()\n",
    "\n",
    "        # Take action in the environment\n",
    "        next_obs, reward, done, _, __ = env.step(act)\n",
    "\n",
    "        # Store the data for training\n",
    "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
    "            train_data[i].append(item)\n",
    "\n",
    "        #print(f\"Action taken: {act.item()}, Reward received: {reward}, Done: {done}\")\n",
    "\n",
    "        # Update observation and cumulative reward\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "\n",
    "        # Break if the episode ends\n",
    "        if done:\n",
    "            #print(f\"Episode ended after {step+1} steps with cumulative reward: {ep_reward}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "    \n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3])\n",
    "    \n",
    "    return train_data, ep_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e257cc-9327-4075-b431-74634ee6c291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "model = ActorCriticNN(env.observation_space.shape[0], env.action_space.n)\n",
    "model = model.to('cpu')\n",
    "train_data, reward = rollout(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13bc4bd8-468e-43ef-93c9-e5913e025dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Training Params\n",
    "n_episodes = 200\n",
    "print_freq = 10\n",
    "\n",
    "ppo = PPOTrainer(model, policy_lr = 3e-4, value_lr = 1e-3, target_kl_div = 0.02, max_policy_train_iters = 40, value_train_iters = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b60941bb-86ce-4752-b24d-ae5ab8e3dce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 | Avg Reward 16.0\n",
      "Episode 20 | Avg Reward 14.0\n",
      "Episode 30 | Avg Reward 29.0\n",
      "Episode 40 | Avg Reward 25.0\n",
      "Episode 50 | Avg Reward 58.0\n",
      "Episode 60 | Avg Reward 27.0\n",
      "Episode 70 | Avg Reward 183.0\n",
      "Episode 80 | Avg Reward 37.0\n",
      "Episode 90 | Avg Reward 110.0\n",
      "Episode 100 | Avg Reward 113.0\n",
      "Episode 110 | Avg Reward 101.0\n",
      "Episode 120 | Avg Reward 111.0\n",
      "Episode 130 | Avg Reward 233.0\n",
      "Episode 140 | Avg Reward 175.0\n",
      "Episode 150 | Avg Reward 203.0\n",
      "Episode 160 | Avg Reward 190.0\n",
      "Episode 170 | Avg Reward 243.0\n",
      "Episode 180 | Avg Reward 409.0\n",
      "Episode 190 | Avg Reward 524.0\n",
      "Episode 200 | Avg Reward 1000.0\n"
     ]
    }
   ],
   "source": [
    "#Training Loops\n",
    "ep_rewards = []\n",
    "for episode_idx in range (n_episodes):\n",
    "    #Perform rollout\n",
    "    train_data, reward = rollout(model, env)\n",
    "    ep_rewards.append(reward)\n",
    "\n",
    "    permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "    obs = torch.tensor(train_data[0][permute_idxs], dtype = torch.float32)\n",
    "    act = torch.tensor(train_data[1][permute_idxs], dtype = torch.int32)\n",
    "    gaes = torch.tensor(train_data[3][permute_idxs], dtype = torch.float32)\n",
    "    act_log_probs = torch.tensor(train_data[4][permute_idxs], dtype = torch.float32)\n",
    "\n",
    "    #Value Data\n",
    "    returns = discount_rewards(train_data[2])[permute_idxs]\n",
    "    returns = torch.tensor(returns, dtype = torch.float32)\n",
    "\n",
    "    #Train Policy\n",
    "    ppo.train_policy(obs, act, act_log_probs, gaes)\n",
    "    ppo.train_value(obs, returns)\n",
    "\n",
    "    if(episode_idx + 1) % print_freq == 0:\n",
    "        print('Episode {} | Avg Reward {:.1f}'.format(\n",
    "            episode_idx + 1, np.mean(ep_rewards[-print_freq])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2324a9-c059-4f51-af3a-12a09363e4da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
