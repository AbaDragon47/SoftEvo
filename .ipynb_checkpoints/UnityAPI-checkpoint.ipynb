{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c1eafb-e865-452e-b713-abe9bec3884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EA_PPO.ipynb           PPO_EA_Pseudo.ipynb    UnityAPI.ipynb\n",
      "Model.py               README.md              \u001b[34m__pycache__\u001b[m\u001b[m/\n",
      "PPO_Base.ipynb         TRPO_Base_Model.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c387fa56-fc4e-482a-ade2-56477850a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Define the path to ml-agents-envs\n",
    "folder_path = \"../ml-agents/ml-agents-envs\"\n",
    "sys.path.append(os.path.abspath(folder_path))\n",
    "\n",
    "# Import UnityEnvironment\n",
    "from mlagents_envs.environment import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18030553-3309-4974-90ad-082f1cffada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.exception import UnityWorkerInUseException\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import random\n",
    "from torch.distributions import Normal\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea40554f-6875-4447-97ca-08d93b45c7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731499133.445141 8721786 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "Mono path[0] = '/Users/rohitpenna/Documents/GitHub/Evo_Project.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/rohitpenna/Documents/GitHub/Evo_Project.app/Contents/MonoBleedingEdge/etc'\n",
      "[Physics::Module] Initialized fallback backend.\n",
      "[Physics::Module] Id: 0xdecafbad\n",
      "New input system (experimental) initialized\n",
      "Initialize engine version: 6000.0.25f1 (4859ab7b5a49)\n",
      "[Subsystems] Discovering subsystems at path /Users/rohitpenna/Documents/GitHub/Evo_Project.app/Contents/Resources/Data/UnitySubsystems\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "Begin MonoManager ReloadAssembly\n",
      "- Loaded All Assemblies, in  0.064 seconds\n",
      "- Finished resetting the current domain, in  0.001 seconds\n",
      "[Physics::Module] Selected backend.\n",
      "[Physics::Module] Name: PhysX\n",
      "[Physics::Module] Id: 0xf2b8ea05\n",
      "[Physics::Module] SDK Version: 4.1.2\n",
      "[Physics::Module] Integration Version: 1.0.0\n",
      "[Physics::Module] Threading Mode: Multi-Threaded\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/LutBuilderLdr shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/CameraMotionBlur shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/TemporalAA shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/Scaling Setup shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/CoreSRP/CoreCopy shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Hidden/Universal Render Pipeline/FallbackError' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/FallbackError shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Hidden/Universal Render Pipeline/FallbackError' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Hidden/Universal Render Pipeline/StencilDeferred' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Hidden/Universal Render Pipeline/StencilDeferred' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/GaussianDepthOfField shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/Debug/DebugReplacement shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/LutBuilderHdr shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/Sampling shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/LensFlareDataDriven shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/CameraMotionVectors shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/UberPost shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/Bloom shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/BokehDepthOfField shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/FinalPost shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/CopyDepth shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/SubpixelMorphologicalAntialiasing shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/ScreenSpaceAmbientOcclusion shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/Stop NaN shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal/HDRDebugView shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/LensFlareScreenSpace shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/PaniniProjection shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Universal Render Pipeline/Lit' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Universal Render Pipeline/Lit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Universal Render Pipeline/Lit' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.189584 ms\n",
      "Registered Communicator in Agent.\n",
      "Generation: 1\n",
      "Generation 1 | Episode 20 | Avg Reward 0.1\n",
      "Generation 1 | Episode 40 | Avg Reward 0.2\n",
      "Generation 1 | Episode 60 | Avg Reward 0.1\n",
      "Generation 1 | Episode 80 | Avg Reward 0.0\n",
      "Generation 1 | Episode 100 | Avg Reward 0.1\n",
      "Generation 1 | Episode 120 | Avg Reward 0.1\n",
      "Generation 1 | Episode 140 | Avg Reward 0.2\n",
      "Generation 1 | Episode 160 | Avg Reward 0.1\n",
      "Generation 1 | Episode 180 | Avg Reward 0.1\n",
      "Generation 1 | Episode 200 | Avg Reward 0.1\n",
      "Generation 1 Average Reward: 0.1\n",
      "Generation: 2\n",
      "Generation 2 | Episode 20 | Avg Reward 0.1\n",
      "Generation 2 | Episode 40 | Avg Reward 0.1\n",
      "Generation 2 | Episode 60 | Avg Reward 0.3\n",
      "Generation 2 | Episode 80 | Avg Reward 0.2\n",
      "Generation 2 | Episode 100 | Avg Reward 0.1\n",
      "Generation 2 | Episode 120 | Avg Reward 0.1\n",
      "Generation 2 | Episode 140 | Avg Reward 0.2\n",
      "Generation 2 | Episode 160 | Avg Reward 0.1\n",
      "Generation 2 | Episode 180 | Avg Reward 0.1\n",
      "Generation 2 | Episode 200 | Avg Reward 0.1\n",
      "Generation 2 Average Reward: 0.1\n",
      "Generation: 3\n",
      "Generation 3 | Episode 20 | Avg Reward 0.1\n",
      "Generation 3 | Episode 40 | Avg Reward 0.1\n",
      "Generation 3 | Episode 60 | Avg Reward 0.0\n",
      "Generation 3 | Episode 80 | Avg Reward 0.1\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Generation 3 | Episode 100 | Avg Reward 0.2\n",
      "Generation 3 | Episode 120 | Avg Reward 0.3\n",
      "Generation 3 | Episode 140 | Avg Reward 0.1\n",
      "Generation 3 | Episode 160 | Avg Reward 0.1\n",
      "Generation 3 | Episode 180 | Avg Reward 0.1\n",
      "Generation 3 | Episode 200 | Avg Reward 0.2\n",
      "Generation 3 Average Reward: 0.1\n",
      "Generation: 4\n",
      "Generation 4 | Episode 20 | Avg Reward 0.2\n",
      "Generation 4 | Episode 40 | Avg Reward 0.1\n",
      "Generation 4 | Episode 60 | Avg Reward 0.0\n",
      "Generation 4 | Episode 80 | Avg Reward 0.1\n",
      "Generation 4 | Episode 100 | Avg Reward 0.1\n",
      "Generation 4 | Episode 120 | Avg Reward 0.2\n",
      "Generation 4 | Episode 140 | Avg Reward 0.0\n",
      "Generation 4 | Episode 160 | Avg Reward 0.1\n",
      "Generation 4 | Episode 180 | Avg Reward 0.1\n",
      "Generation 4 | Episode 200 | Avg Reward 0.1\n",
      "Generation 4 Average Reward: 0.1\n",
      "Generation: 5\n",
      "Generation 5 | Episode 20 | Avg Reward 0.0\n",
      "Generation 5 | Episode 40 | Avg Reward 0.1\n",
      "Generation 5 | Episode 60 | Avg Reward 0.1\n",
      "Generation 5 | Episode 80 | Avg Reward 0.1\n",
      "Generation 5 | Episode 100 | Avg Reward 0.1\n",
      "Generation 5 | Episode 120 | Avg Reward 0.1\n",
      "Generation 5 | Episode 140 | Avg Reward 0.2\n",
      "Generation 5 | Episode 160 | Avg Reward 0.2\n",
      "Generation 5 | Episode 180 | Avg Reward 0.1\n",
      "Generation 5 | Episode 200 | Avg Reward 0.1\n",
      "Generation 5 Average Reward: 0.1\n",
      "Generation: 6\n",
      "Generation 6 | Episode 20 | Avg Reward 0.1\n",
      "Generation 6 | Episode 40 | Avg Reward 0.1\n",
      "Generation 6 | Episode 60 | Avg Reward 0.1\n",
      "Generation 6 | Episode 80 | Avg Reward 0.1\n",
      "Generation 6 | Episode 100 | Avg Reward 0.1\n",
      "Generation 6 | Episode 120 | Avg Reward 0.1\n",
      "Generation 6 | Episode 140 | Avg Reward 0.1\n",
      "Generation 6 | Episode 160 | Avg Reward 0.1\n",
      "Generation 6 | Episode 180 | Avg Reward 0.1\n",
      "Generation 6 | Episode 200 | Avg Reward 0.1\n",
      "Generation 6 Average Reward: 0.1\n",
      "Generation: 7\n",
      "Generation 7 | Episode 20 | Avg Reward 0.1\n",
      "Generation 7 | Episode 40 | Avg Reward 0.1\n",
      "Generation 7 | Episode 60 | Avg Reward 0.2\n",
      "Generation 7 | Episode 80 | Avg Reward 0.2\n",
      "Generation 7 | Episode 100 | Avg Reward 0.1\n",
      "Generation 7 | Episode 120 | Avg Reward 0.1\n",
      "Generation 7 | Episode 140 | Avg Reward 0.0\n",
      "Generation 7 | Episode 160 | Avg Reward 0.2\n",
      "Generation 7 | Episode 180 | Avg Reward 0.2\n",
      "Generation 7 | Episode 200 | Avg Reward 0.1\n",
      "Generation 7 Average Reward: 0.1\n",
      "Generation: 8\n",
      "Generation 8 | Episode 20 | Avg Reward 0.1\n",
      "Generation 8 | Episode 40 | Avg Reward 0.1\n",
      "Generation 8 | Episode 60 | Avg Reward 0.0\n",
      "Generation 8 | Episode 80 | Avg Reward 0.1\n",
      "Generation 8 | Episode 100 | Avg Reward 0.0\n",
      "Generation 8 | Episode 120 | Avg Reward 0.1\n",
      "Generation 8 | Episode 140 | Avg Reward 0.2\n",
      "Generation 8 | Episode 160 | Avg Reward 0.3\n",
      "Generation 8 | Episode 180 | Avg Reward 0.1\n",
      "Generation 8 | Episode 200 | Avg Reward 0.1\n",
      "Generation 8 Average Reward: 0.1\n",
      "Generation: 9\n",
      "Generation 9 | Episode 20 | Avg Reward 0.1\n",
      "Generation 9 | Episode 40 | Avg Reward 0.2\n",
      "Generation 9 | Episode 60 | Avg Reward 0.1\n",
      "Generation 9 | Episode 80 | Avg Reward 0.0\n",
      "Generation 9 | Episode 100 | Avg Reward 0.1\n",
      "Generation 9 | Episode 120 | Avg Reward 0.1\n",
      "Generation 9 | Episode 140 | Avg Reward 0.1\n",
      "Generation 9 | Episode 160 | Avg Reward 0.1\n",
      "Generation 9 | Episode 180 | Avg Reward 0.1\n",
      "Generation 9 | Episode 200 | Avg Reward 0.1\n",
      "Generation 9 Average Reward: 0.1\n",
      "Generation: 10\n",
      "Generation 10 | Episode 20 | Avg Reward 0.1\n",
      "Generation 10 | Episode 40 | Avg Reward 0.1\n",
      "Generation 10 | Episode 60 | Avg Reward 0.1\n",
      "Generation 10 | Episode 80 | Avg Reward 0.1\n",
      "Generation 10 | Episode 100 | Avg Reward 0.1\n",
      "Generation 10 | Episode 120 | Avg Reward 0.1\n",
      "Generation 10 | Episode 140 | Avg Reward 0.1\n",
      "Generation 10 | Episode 160 | Avg Reward 0.1\n",
      "Generation 10 | Episode 180 | Avg Reward 0.2\n",
      "Generation 10 | Episode 200 | Avg Reward 0.1\n",
      "Generation 10 Average Reward: 0.1\n",
      "Generation: 11\n",
      "Generation 11 | Episode 20 | Avg Reward 0.0\n",
      "Generation 11 | Episode 40 | Avg Reward 0.1\n",
      "Generation 11 | Episode 60 | Avg Reward 0.0\n",
      "Generation 11 | Episode 80 | Avg Reward 0.1\n",
      "Generation 11 | Episode 100 | Avg Reward 0.1\n",
      "Generation 11 | Episode 120 | Avg Reward 0.2\n",
      "Generation 11 | Episode 140 | Avg Reward 0.1\n",
      "Generation 11 | Episode 160 | Avg Reward 0.1\n",
      "Generation 11 | Episode 180 | Avg Reward 0.1\n",
      "Generation 11 | Episode 200 | Avg Reward 0.2\n",
      "Generation 11 Average Reward: 0.1\n",
      "Generation: 12\n",
      "Generation 12 | Episode 20 | Avg Reward 0.1\n",
      "Generation 12 | Episode 40 | Avg Reward 0.1\n",
      "Generation 12 | Episode 60 | Avg Reward 0.1\n",
      "Generation 12 | Episode 80 | Avg Reward 0.1\n",
      "Generation 12 | Episode 100 | Avg Reward 0.1\n",
      "Generation 12 | Episode 120 | Avg Reward 0.0\n",
      "Generation 12 | Episode 140 | Avg Reward 0.2\n",
      "Generation 12 | Episode 160 | Avg Reward 0.1\n",
      "Generation 12 | Episode 180 | Avg Reward 0.1\n",
      "Generation 12 | Episode 200 | Avg Reward 0.2\n",
      "Generation 12 Average Reward: 0.1\n",
      "Generation: 13\n",
      "Generation 13 | Episode 20 | Avg Reward 0.2\n",
      "Generation 13 | Episode 40 | Avg Reward 0.2\n",
      "Generation 13 | Episode 60 | Avg Reward 0.3\n",
      "Generation 13 | Episode 80 | Avg Reward 0.1\n",
      "Generation 13 | Episode 100 | Avg Reward 0.1\n",
      "Generation 13 | Episode 120 | Avg Reward 0.1\n",
      "Generation 13 | Episode 140 | Avg Reward 0.1\n",
      "Generation 13 | Episode 160 | Avg Reward 0.1\n",
      "Generation 13 | Episode 180 | Avg Reward 0.1\n",
      "Generation 13 | Episode 200 | Avg Reward 0.2\n",
      "Generation 13 Average Reward: 0.2\n",
      "Generation: 14\n",
      "Generation 14 | Episode 20 | Avg Reward 0.2\n",
      "Generation 14 | Episode 40 | Avg Reward 0.1\n",
      "Generation 14 | Episode 60 | Avg Reward 0.1\n",
      "Generation 14 | Episode 80 | Avg Reward 0.1\n",
      "Generation 14 | Episode 100 | Avg Reward 0.1\n",
      "Generation 14 | Episode 120 | Avg Reward 0.1\n",
      "Generation 14 | Episode 140 | Avg Reward 0.1\n",
      "Generation 14 | Episode 160 | Avg Reward 0.2\n",
      "Generation 14 | Episode 180 | Avg Reward 0.1\n",
      "Generation 14 | Episode 200 | Avg Reward 0.2\n",
      "Generation 14 Average Reward: 0.1\n",
      "Generation: 15\n",
      "Generation 15 | Episode 20 | Avg Reward 0.1\n",
      "Generation 15 | Episode 40 | Avg Reward 0.1\n",
      "Generation 15 | Episode 60 | Avg Reward 0.1\n",
      "Generation 15 | Episode 80 | Avg Reward 0.1\n",
      "Generation 15 | Episode 100 | Avg Reward 0.1\n",
      "Generation 15 | Episode 120 | Avg Reward 0.3\n",
      "Generation 15 | Episode 140 | Avg Reward 0.1\n",
      "Generation 15 | Episode 160 | Avg Reward 0.2\n",
      "Generation 15 | Episode 180 | Avg Reward 0.2\n",
      "Generation 15 | Episode 200 | Avg Reward 0.2\n",
      "Generation 15 Average Reward: 0.1\n",
      "Generation: 16\n",
      "Generation 16 | Episode 20 | Avg Reward 0.1\n",
      "Generation 16 | Episode 40 | Avg Reward 0.1\n",
      "Generation 16 | Episode 60 | Avg Reward 0.1\n",
      "Generation 16 | Episode 80 | Avg Reward 0.2\n",
      "Generation 16 | Episode 100 | Avg Reward 0.0\n",
      "Generation 16 | Episode 120 | Avg Reward 0.1\n",
      "Generation 16 | Episode 140 | Avg Reward 0.1\n",
      "Generation 16 | Episode 160 | Avg Reward 0.1\n",
      "Generation 16 | Episode 180 | Avg Reward 0.2\n",
      "Generation 16 | Episode 200 | Avg Reward 0.1\n",
      "Generation 16 Average Reward: 0.1\n",
      "Generation: 17\n",
      "Generation 17 | Episode 20 | Avg Reward 0.1\n",
      "Generation 17 | Episode 40 | Avg Reward 0.1\n",
      "Generation 17 | Episode 60 | Avg Reward 0.1\n",
      "Generation 17 | Episode 80 | Avg Reward 0.1\n",
      "Generation 17 | Episode 100 | Avg Reward 0.2\n",
      "Generation 17 | Episode 120 | Avg Reward 0.1\n",
      "Generation 17 | Episode 140 | Avg Reward 0.2\n",
      "Generation 17 | Episode 160 | Avg Reward 0.0\n",
      "Generation 17 | Episode 180 | Avg Reward 0.1\n",
      "Generation 17 | Episode 200 | Avg Reward 0.1\n",
      "Generation 17 Average Reward: 0.1\n",
      "Generation: 18\n",
      "Generation 18 | Episode 20 | Avg Reward 0.1\n",
      "Generation 18 | Episode 40 | Avg Reward 0.1\n",
      "Generation 18 | Episode 60 | Avg Reward 0.1\n",
      "Generation 18 | Episode 80 | Avg Reward 0.1\n",
      "Generation 18 | Episode 100 | Avg Reward 0.1\n",
      "Generation 18 | Episode 120 | Avg Reward 0.1\n",
      "Generation 18 | Episode 140 | Avg Reward 0.1\n",
      "Generation 18 | Episode 160 | Avg Reward 0.0\n",
      "Generation 18 | Episode 180 | Avg Reward 0.0\n",
      "Generation 18 | Episode 200 | Avg Reward 0.1\n",
      "Generation 18 Average Reward: 0.1\n",
      "Generation: 19\n",
      "Generation 19 | Episode 20 | Avg Reward 0.1\n",
      "Generation 19 | Episode 40 | Avg Reward 0.1\n",
      "Generation 19 | Episode 60 | Avg Reward 0.1\n",
      "Generation 19 | Episode 80 | Avg Reward 0.2\n",
      "Generation 19 | Episode 100 | Avg Reward 0.2\n",
      "Generation 19 | Episode 120 | Avg Reward 0.2\n",
      "Generation 19 | Episode 140 | Avg Reward 0.1\n",
      "Generation 19 | Episode 160 | Avg Reward 0.1\n",
      "Generation 19 | Episode 180 | Avg Reward 0.2\n",
      "Generation 19 | Episode 200 | Avg Reward 0.1\n",
      "Generation 19 Average Reward: 0.1\n",
      "Generation: 20\n",
      "Generation 20 | Episode 20 | Avg Reward 0.1\n",
      "Generation 20 | Episode 40 | Avg Reward 0.1\n",
      "Generation 20 | Episode 60 | Avg Reward 0.1\n",
      "Generation 20 | Episode 80 | Avg Reward 0.0\n",
      "Generation 20 | Episode 100 | Avg Reward 0.3\n",
      "Generation 20 | Episode 120 | Avg Reward 0.1\n",
      "Generation 20 | Episode 140 | Avg Reward 0.2\n",
      "Generation 20 | Episode 160 | Avg Reward 0.1\n",
      "Generation 20 | Episode 180 | Avg Reward 0.1\n",
      "Generation 20 | Episode 200 | Avg Reward 0.0\n",
      "Generation 20 Average Reward: 0.1\n",
      "Generation: 21\n",
      "Generation 21 | Episode 20 | Avg Reward 0.1\n",
      "Generation 21 | Episode 40 | Avg Reward 0.1\n",
      "Generation 21 | Episode 60 | Avg Reward 0.1\n",
      "Generation 21 | Episode 80 | Avg Reward 0.1\n",
      "Generation 21 | Episode 100 | Avg Reward 0.1\n",
      "Generation 21 | Episode 120 | Avg Reward 0.1\n",
      "Generation 21 | Episode 140 | Avg Reward 0.1\n",
      "Generation 21 | Episode 160 | Avg Reward 0.1\n",
      "Generation 21 | Episode 180 | Avg Reward 0.0\n",
      "Generation 21 | Episode 200 | Avg Reward 0.1\n",
      "Generation 21 Average Reward: 0.1\n",
      "Generation: 22\n",
      "Generation 22 | Episode 20 | Avg Reward 0.1\n",
      "Generation 22 | Episode 40 | Avg Reward 0.1\n",
      "Generation 22 | Episode 60 | Avg Reward 0.1\n",
      "Generation 22 | Episode 80 | Avg Reward 0.0\n",
      "Generation 22 | Episode 100 | Avg Reward 0.1\n",
      "Generation 22 | Episode 120 | Avg Reward 0.1\n",
      "Generation 22 | Episode 140 | Avg Reward 0.1\n",
      "Generation 22 | Episode 160 | Avg Reward 0.1\n",
      "Generation 22 | Episode 180 | Avg Reward 0.2\n",
      "Generation 22 | Episode 200 | Avg Reward 0.1\n",
      "Generation 22 Average Reward: 0.1\n",
      "Generation: 23\n",
      "Generation 23 | Episode 20 | Avg Reward 0.0\n",
      "Generation 23 | Episode 40 | Avg Reward 0.2\n",
      "Generation 23 | Episode 60 | Avg Reward 0.0\n",
      "Generation 23 | Episode 80 | Avg Reward 0.1\n",
      "Generation 23 | Episode 100 | Avg Reward 0.1\n",
      "Generation 23 | Episode 120 | Avg Reward 0.1\n",
      "Generation 23 | Episode 140 | Avg Reward 0.2\n",
      "Generation 23 | Episode 160 | Avg Reward 0.2\n",
      "Generation 23 | Episode 180 | Avg Reward 0.1\n",
      "Generation 23 | Episode 200 | Avg Reward 0.2\n",
      "Generation 23 Average Reward: 0.1\n",
      "Generation: 24\n",
      "Generation 24 | Episode 20 | Avg Reward 0.1\n",
      "Generation 24 | Episode 40 | Avg Reward 0.1\n",
      "Generation 24 | Episode 60 | Avg Reward 0.1\n",
      "Generation 24 | Episode 80 | Avg Reward 0.0\n",
      "Generation 24 | Episode 100 | Avg Reward 0.1\n",
      "Generation 24 | Episode 120 | Avg Reward 0.1\n",
      "Generation 24 | Episode 140 | Avg Reward 0.1\n",
      "Generation 24 | Episode 160 | Avg Reward 0.1\n",
      "Generation 24 | Episode 180 | Avg Reward 0.1\n",
      "Generation 24 | Episode 200 | Avg Reward 0.2\n",
      "Generation 24 Average Reward: 0.1\n",
      "Generation: 25\n",
      "Generation 25 | Episode 20 | Avg Reward 0.2\n",
      "Generation 25 | Episode 40 | Avg Reward 0.2\n",
      "Generation 25 | Episode 60 | Avg Reward 0.1\n",
      "Generation 25 | Episode 80 | Avg Reward 0.0\n",
      "Generation 25 | Episode 100 | Avg Reward 0.1\n",
      "Generation 25 | Episode 120 | Avg Reward 0.1\n",
      "Generation 25 | Episode 140 | Avg Reward 0.1\n",
      "Generation 25 | Episode 160 | Avg Reward 0.1\n",
      "Generation 25 | Episode 180 | Avg Reward 0.2\n",
      "Generation 25 | Episode 200 | Avg Reward 0.1\n",
      "Generation 25 Average Reward: 0.1\n",
      "Generation: 26\n",
      "Generation 26 | Episode 20 | Avg Reward 0.2\n",
      "Generation 26 | Episode 40 | Avg Reward 0.1\n",
      "Generation 26 | Episode 60 | Avg Reward 0.2\n",
      "Generation 26 | Episode 80 | Avg Reward 0.0\n",
      "Generation 26 | Episode 100 | Avg Reward 0.1\n",
      "Generation 26 | Episode 120 | Avg Reward 0.1\n",
      "Generation 26 | Episode 140 | Avg Reward 0.0\n",
      "Generation 26 | Episode 160 | Avg Reward 0.1\n",
      "Generation 26 | Episode 180 | Avg Reward 0.1\n",
      "Generation 26 | Episode 200 | Avg Reward 0.1\n",
      "Generation 26 Average Reward: 0.1\n",
      "Generation: 27\n",
      "Generation 27 | Episode 20 | Avg Reward 0.1\n",
      "Generation 27 | Episode 40 | Avg Reward 0.1\n",
      "Generation 27 | Episode 60 | Avg Reward 0.1\n",
      "Generation 27 | Episode 80 | Avg Reward 0.1\n",
      "Generation 27 | Episode 100 | Avg Reward 0.1\n",
      "Generation 27 | Episode 120 | Avg Reward 0.1\n",
      "Generation 27 | Episode 140 | Avg Reward 0.1\n",
      "Generation 27 | Episode 160 | Avg Reward 0.1\n",
      "Generation 27 | Episode 180 | Avg Reward 0.1\n",
      "Generation 27 | Episode 200 | Avg Reward 0.1\n",
      "Generation 27 Average Reward: 0.1\n",
      "Generation: 28\n",
      "Generation 28 | Episode 20 | Avg Reward 0.2\n",
      "Generation 28 | Episode 40 | Avg Reward 0.1\n",
      "Generation 28 | Episode 60 | Avg Reward 0.1\n",
      "Generation 28 | Episode 80 | Avg Reward 0.1\n",
      "Generation 28 | Episode 100 | Avg Reward 0.1\n",
      "Generation 28 | Episode 120 | Avg Reward 0.1\n",
      "Generation 28 | Episode 140 | Avg Reward 0.1\n",
      "Generation 28 | Episode 160 | Avg Reward 0.0\n",
      "Generation 28 | Episode 180 | Avg Reward 0.1\n",
      "Generation 28 | Episode 200 | Avg Reward 0.1\n",
      "Generation 28 Average Reward: 0.1\n",
      "Generation: 29\n",
      "Generation 29 | Episode 20 | Avg Reward 0.1\n",
      "Generation 29 | Episode 40 | Avg Reward 0.1\n",
      "Generation 29 | Episode 60 | Avg Reward 0.1\n",
      "Generation 29 | Episode 80 | Avg Reward 0.2\n",
      "Generation 29 | Episode 100 | Avg Reward 0.2\n",
      "Generation 29 | Episode 120 | Avg Reward 0.1\n",
      "Generation 29 | Episode 140 | Avg Reward 0.1\n",
      "Generation 29 | Episode 160 | Avg Reward 0.1\n",
      "Generation 29 | Episode 180 | Avg Reward 0.1\n",
      "Generation 29 | Episode 200 | Avg Reward 0.1\n",
      "Generation 29 Average Reward: 0.1\n",
      "Generation: 30\n",
      "Generation 30 | Episode 20 | Avg Reward 0.0\n",
      "Generation 30 | Episode 40 | Avg Reward 0.1\n",
      "Generation 30 | Episode 60 | Avg Reward 0.1\n",
      "Generation 30 | Episode 80 | Avg Reward 0.1\n",
      "Generation 30 | Episode 100 | Avg Reward 0.1\n",
      "Generation 30 | Episode 120 | Avg Reward 0.1\n",
      "Generation 30 | Episode 140 | Avg Reward 0.1\n",
      "Generation 30 | Episode 160 | Avg Reward 0.1\n",
      "Generation 30 | Episode 180 | Avg Reward 0.1\n",
      "Generation 30 | Episode 200 | Avg Reward 0.1\n",
      "Generation 30 Average Reward: 0.1\n",
      "Generation: 31\n",
      "Generation 31 | Episode 20 | Avg Reward 0.2\n",
      "Generation 31 | Episode 40 | Avg Reward 0.1\n",
      "Generation 31 | Episode 60 | Avg Reward 0.1\n",
      "Generation 31 | Episode 80 | Avg Reward 0.1\n",
      "Generation 31 | Episode 100 | Avg Reward 0.1\n",
      "Generation 31 | Episode 120 | Avg Reward 0.0\n",
      "Generation 31 | Episode 140 | Avg Reward 0.1\n",
      "Generation 31 | Episode 160 | Avg Reward 0.2\n",
      "Generation 31 | Episode 180 | Avg Reward 0.1\n",
      "Generation 31 | Episode 200 | Avg Reward 0.1\n",
      "Generation 31 Average Reward: 0.1\n",
      "Generation: 32\n",
      "Generation 32 | Episode 20 | Avg Reward 0.1\n",
      "Generation 32 | Episode 40 | Avg Reward 0.3\n",
      "Generation 32 | Episode 60 | Avg Reward 0.1\n",
      "Generation 32 | Episode 80 | Avg Reward 0.2\n",
      "Generation 32 | Episode 100 | Avg Reward 0.1\n",
      "Generation 32 | Episode 120 | Avg Reward 0.1\n",
      "Generation 32 | Episode 140 | Avg Reward 0.2\n",
      "Generation 32 | Episode 160 | Avg Reward 0.1\n",
      "Generation 32 | Episode 180 | Avg Reward 0.0\n",
      "Generation 32 | Episode 200 | Avg Reward 0.1\n",
      "Generation 32 Average Reward: 0.1\n",
      "Generation: 33\n",
      "Generation 33 | Episode 20 | Avg Reward 0.2\n",
      "Generation 33 | Episode 40 | Avg Reward 0.2\n",
      "Generation 33 | Episode 60 | Avg Reward 0.1\n",
      "Generation 33 | Episode 80 | Avg Reward 0.1\n",
      "Generation 33 | Episode 100 | Avg Reward 0.1\n",
      "Generation 33 | Episode 120 | Avg Reward 0.1\n",
      "Generation 33 | Episode 140 | Avg Reward 0.1\n",
      "Generation 33 | Episode 160 | Avg Reward 0.0\n",
      "Generation 33 | Episode 180 | Avg Reward 0.1\n",
      "Generation 33 | Episode 200 | Avg Reward 0.1\n",
      "Generation 33 Average Reward: 0.1\n",
      "Generation: 34\n",
      "Generation 34 | Episode 20 | Avg Reward 0.1\n",
      "Generation 34 | Episode 40 | Avg Reward 0.1\n",
      "Generation 34 | Episode 60 | Avg Reward 0.2\n",
      "Generation 34 | Episode 80 | Avg Reward 0.1\n",
      "Generation 34 | Episode 100 | Avg Reward 0.1\n",
      "Generation 34 | Episode 120 | Avg Reward 0.3\n",
      "Generation 34 | Episode 140 | Avg Reward 0.1\n",
      "Generation 34 | Episode 160 | Avg Reward 0.2\n",
      "Generation 34 | Episode 180 | Avg Reward 0.1\n",
      "Generation 34 | Episode 200 | Avg Reward 0.1\n",
      "Generation 34 Average Reward: 0.2\n",
      "Generation: 35\n",
      "Generation 35 | Episode 20 | Avg Reward 0.2\n",
      "Generation 35 | Episode 40 | Avg Reward 0.1\n",
      "Generation 35 | Episode 60 | Avg Reward 0.2\n",
      "Generation 35 | Episode 80 | Avg Reward 0.1\n",
      "Generation 35 | Episode 100 | Avg Reward 0.1\n",
      "Generation 35 | Episode 120 | Avg Reward 0.0\n",
      "Generation 35 | Episode 140 | Avg Reward 0.0\n",
      "Generation 35 | Episode 160 | Avg Reward 0.1\n",
      "Generation 35 | Episode 180 | Avg Reward 0.2\n",
      "Generation 35 | Episode 200 | Avg Reward 0.1\n",
      "Generation 35 Average Reward: 0.1\n",
      "Generation: 36\n",
      "Generation 36 | Episode 20 | Avg Reward 0.1\n",
      "Generation 36 | Episode 40 | Avg Reward 0.2\n",
      "Generation 36 | Episode 60 | Avg Reward 0.2\n",
      "Generation 36 | Episode 80 | Avg Reward 0.2\n",
      "Generation 36 | Episode 100 | Avg Reward 0.2\n",
      "Generation 36 | Episode 120 | Avg Reward 0.1\n",
      "Generation 36 | Episode 140 | Avg Reward 0.2\n",
      "Generation 36 | Episode 160 | Avg Reward 0.2\n",
      "Generation 36 | Episode 180 | Avg Reward 0.2\n",
      "Generation 36 | Episode 200 | Avg Reward 0.1\n",
      "Generation 36 Average Reward: 0.2\n",
      "Generation: 37\n",
      "Generation 37 | Episode 20 | Avg Reward 0.0\n",
      "Generation 37 | Episode 40 | Avg Reward 0.1\n",
      "Generation 37 | Episode 60 | Avg Reward 0.0\n",
      "Generation 37 | Episode 80 | Avg Reward 0.1\n",
      "Generation 37 | Episode 100 | Avg Reward 0.0\n",
      "Generation 37 | Episode 120 | Avg Reward 0.2\n",
      "Generation 37 | Episode 140 | Avg Reward 0.1\n",
      "Generation 37 | Episode 160 | Avg Reward 0.1\n",
      "Generation 37 | Episode 180 | Avg Reward 0.1\n",
      "Generation 37 | Episode 200 | Avg Reward 0.1\n",
      "Generation 37 Average Reward: 0.1\n",
      "Generation: 38\n",
      "Generation 38 | Episode 20 | Avg Reward 0.2\n",
      "Generation 38 | Episode 40 | Avg Reward 0.1\n",
      "Generation 38 | Episode 60 | Avg Reward 0.1\n",
      "Generation 38 | Episode 80 | Avg Reward 0.2\n",
      "Generation 38 | Episode 100 | Avg Reward 0.1\n",
      "Generation 38 | Episode 120 | Avg Reward 0.1\n",
      "Generation 38 | Episode 140 | Avg Reward 0.2\n",
      "Generation 38 | Episode 160 | Avg Reward 0.1\n",
      "Generation 38 | Episode 180 | Avg Reward 0.1\n",
      "Generation 38 | Episode 200 | Avg Reward 0.1\n",
      "Generation 38 Average Reward: 0.1\n",
      "Generation: 39\n",
      "Generation 39 | Episode 20 | Avg Reward 0.1\n",
      "Generation 39 | Episode 40 | Avg Reward 0.0\n",
      "Generation 39 | Episode 60 | Avg Reward 0.2\n",
      "Generation 39 | Episode 80 | Avg Reward 0.1\n",
      "Generation 39 | Episode 100 | Avg Reward 0.1\n",
      "Generation 39 | Episode 120 | Avg Reward 0.1\n",
      "Generation 39 | Episode 140 | Avg Reward 0.1\n",
      "Generation 39 | Episode 160 | Avg Reward 0.1\n"
     ]
    }
   ],
   "source": [
    "sns.set()\n",
    "\n",
    "class ActorCriticNN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,action_dim))\n",
    "\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1))\n",
    "\n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "\n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value\n",
    "\n",
    "#trainer\n",
    "class PPOTrainer():\n",
    "    def __init__(self, actor_critic, ppo_clip_val, target_kl_div, policy_lr, value_lr, max_policy_train_iters = 80, \n",
    "                 value_train_iters = 80, entropy_coeff = 0.01):\n",
    "        self.entropy_coeff = entropy_coeff\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_val = ppo_clip_val\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.value_train_iters = value_train_iters\n",
    "    \n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr = policy_lr)\n",
    "    \n",
    "        value_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr = value_lr)\n",
    "\n",
    "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "        \"\"\"\n",
    "        Train the policy using PPO with the given observations, actions, old log probabilities, and GAEs.\n",
    "        \"\"\"\n",
    "        # Forward pass through the policy network\n",
    "        new_logits = self.ac.policy(obs)  # Output from policy network\n",
    "        new_logits = Categorical(logits=new_logits)\n",
    "\n",
    "        acts = acts.squeeze(1)\n",
    "        \n",
    "        # Ensure actions are reshaped to 1D if necessary\n",
    "        acts = acts.reshape(-1)  # Flatten actions to a 1D tensor\n",
    "        \n",
    "        # Ensure batch sizes match\n",
    "        batch_size = new_logits.logits.size(0)\n",
    "        if acts.size(0) != batch_size:\n",
    "            #print(f\"Warning: Mismatch in batch size: logits size {batch_size}, actions size {acts.size(0)}\")\n",
    "            acts = acts[:batch_size]  # Slice actions to match logits batch size\n",
    "        \n",
    "        # Compute the log probabilities of the actions\n",
    "        new_log_probs = new_logits.log_prob(acts)  # This requires acts to be a 1D tensor, not 2D\n",
    "        \n",
    "        # Compute the policy ratio\n",
    "        policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        clipped_ratio = policy_ratio.clamp(1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = -torch.min(policy_ratio * gaes, clipped_ratio * gaes).mean()\n",
    "        \n",
    "        # Add entropy bonus to encourage exploration\n",
    "        #entropy_loss = -torch.mean(new_logits.entropy())\n",
    "        #loss = loss - self.entropy_coeff * entropy_loss\n",
    "\n",
    "        # Backpropagate and optimize (assuming optimizer is defined)\n",
    "        self.policy_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        kl_div = (old_log_probs - new_log_probs).mean()\n",
    "\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "\n",
    "        for _ in range (self.value_train_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "    \n",
    "            values = self.ac.value(obs)\n",
    "            value_loss = (returns-values) ** 2\n",
    "            value_loss = value_loss.mean()\n",
    "    \n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()\n",
    "\n",
    "class ActorCriticChromosome:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        # Actor-Critic neural network\n",
    "        self.model = ActorCriticNN(state_dim, action_dim)\n",
    "\n",
    "        #Hyperparameters\n",
    "        self.policy_lr = random.uniform(1e-5, 1e-2)  # Expanded range\n",
    "        self.value_lr = random.uniform(1e-5, 1e-2)  # Expanded range\n",
    "        self.ppo_clip_val = random.uniform(0.1, 0.4)  # Expanded range\n",
    "        self.target_kl_div = random.uniform(0.001, 0.05)\n",
    "\n",
    "def create_population(pop_size, state_dim, action_dim):\n",
    "    return [ActorCriticChromosome(state_dim, action_dim) for _ in range(pop_size)]\n",
    "\n",
    "\"\"\"\n",
    "When initializing PPOTrainer, you're passing the individual's hyperparameters (policy_lr, value_lr, etc.) \n",
    "to ensure the evaluation process reflects the effect of those hyperparameters.\n",
    "You're sampling actions from the model and accumulating rewards to calculate the fitness score.\n",
    "\"\"\"\n",
    "\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "\n",
    "def evaluate_population(population, env, behavior_name):\n",
    "    \"\"\"\n",
    "    Evaluate each individual in the population within the Unity environment.\n",
    "    \n",
    "    Parameters:\n",
    "        population (list): List of policies (or models) to evaluate.\n",
    "        env (UnityEnvironment): Unity environment object.\n",
    "        behavior_name (str): Name of the behavior being evaluated.\n",
    "        \n",
    "    Returns:\n",
    "        fitness_scores (list): List of fitness scores (rewards) for each individual.\n",
    "    \"\"\"\n",
    "    fitness_scores = []\n",
    "\n",
    "    for policy in population:\n",
    "        # Reset the environment and initialize reward for the episode\n",
    "        env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Get the current observation and decision steps\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "            if len(decision_steps) > 0:\n",
    "                # Extract observation data for the agent\n",
    "                obs = decision_steps.obs[0][0]  # Get the first observation from the agent (assuming single agent)\n",
    "\n",
    "                # Convert obs to a PyTorch tensor with dtype float32\n",
    "                obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "                # Ensure the tensor is correctly shaped for the model (batch dimension)\n",
    "                if len(obs_tensor.shape) == 1:  # if obs is 1D, add a batch dimension\n",
    "                    obs_tensor = obs_tensor.unsqueeze(0)\n",
    "\n",
    "                # Get the action from the policy model\n",
    "                logits, _ = policy.model(obs_tensor)  # Assuming 'policy' method returns both logits and value\n",
    "\n",
    "                # Convert logits into a categorical distribution and sample an action\n",
    "                action_dist = Categorical(logits=logits)\n",
    "                action = action_dist.sample().item()  # Get the action as a scalar\n",
    "\n",
    "                # Ensure action is a 2D array with shape (1, 2) for continuous action space\n",
    "                action = np.array([[action, action]])  # Example to make action a 2D array\n",
    "\n",
    "                # Convert the action to the correct Unity format\n",
    "                action_tuple = ActionTuple(continuous=action)\n",
    "\n",
    "                # Set the action and step the environment\n",
    "                env.set_actions(behavior_name, action_tuple)\n",
    "                env.step()\n",
    "\n",
    "            # Collect rewards from decision and terminal steps\n",
    "            for agent_id in decision_steps:\n",
    "                total_reward += decision_steps[agent_id].reward\n",
    "            \n",
    "            for agent_id in terminal_steps:\n",
    "                total_reward += terminal_steps[agent_id].reward\n",
    "                done = True  # End of episode if there are terminal steps\n",
    "\n",
    "        # Append the total reward as the fitness score for the policy\n",
    "        fitness_scores.append(total_reward)\n",
    "\n",
    "    return fitness_scores\n",
    "\n",
    "\n",
    "\n",
    "def select_parents(population, fitness_scores, num_parents):\n",
    "   # Get indices that would sort the fitness scores in descending order\n",
    "    sorted_indices = np.argsort(fitness_scores)[::-1]  # Reverse for descending order\n",
    "    \n",
    "    # Select the best parents based on the sorted indices\n",
    "    parents = np.array(population)[sorted_indices][:num_parents]\n",
    "    return parents.tolist()\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    child = ActorCriticChromosome(state_dim, action_dim)\n",
    "    \n",
    "    # Crossover the weights of the model in a way that keeps useful features\n",
    "    for param1, param2, param_child in zip(parent1.model.parameters(), parent2.model.parameters(), child.model.parameters()):\n",
    "        param_child.data = (param1.data + param2.data) / 2  # Averaging weights of the parents\n",
    "\n",
    "    # Crossover the hyperparameters (keeping them in a range that allows for continued learning)\n",
    "    child.policy_lr = (parent1.policy_lr + parent2.policy_lr) / 2\n",
    "    child.value_lr = (parent1.value_lr + parent2.value_lr) / 2\n",
    "    child.ppo_clip_val = (parent1.ppo_clip_val + parent2.ppo_clip_val) / 2\n",
    "    child.target_kl_div = (parent1.target_kl_div + parent2.target_kl_div) / 2\n",
    "    \n",
    "    return child\n",
    "\n",
    "\n",
    "def mutate(individual, mutation_rate=0.01):\n",
    "    for param in individual.model.parameters():\n",
    "        if random.random() < mutation_rate:\n",
    "            param.data += torch.randn(param.size()) * 0.1  # Add small random noise\n",
    "    \n",
    "    # Mutate hyperparameters with greater changes\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.policy_lr += random.uniform(-5e-4, 5e-4)  # Much larger mutation range\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.value_lr += random.uniform(-5e-4, 5e-4)  # Increased mutation range\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.ppo_clip_val += random.uniform(-0.1, 0.1)  # Increased mutation range\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.target_kl_div += random.uniform(-0.01, 0.01)  # Increased mutation range\n",
    "\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Return discounted rewards based on the given rewards and the gamma param.\n",
    "    \"\"\"\n",
    "\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma = 0.99, decay = 0.97):\n",
    "    \"\"\"\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/abs/1506.02438\n",
    "    \"\"\"\n",
    "\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
    "\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "    return np.array(gaes[::-1])\n",
    "\n",
    "def unity_rollout(model, env, name, max_steps=1000, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Perform a rollout of the model in the Unity environment and collect rewards, \n",
    "    actions, values, and log probabilities.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The trained policy model.\n",
    "        env (UnityEnvironment): The Unity environment object.\n",
    "        name (str): The behavior name for the agent.\n",
    "        max_steps (int): Maximum number of steps in an episode.\n",
    "\n",
    "    Returns:\n",
    "        train_data (tuple): Collected training data (observations, actions, rewards, values, log_probs).\n",
    "        reward (float): Total reward for the episode.\n",
    "    \"\"\"\n",
    "    train_data = [[], [], [], [], []]  # To store observations, actions, rewards, values, log_probs\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    # Reset environment at the start\n",
    "    env.reset()\n",
    "\n",
    "    while not done and step < max_steps:\n",
    "        decision_steps, terminal_steps = env.get_steps(name)\n",
    "\n",
    "        if len(decision_steps) > 0:\n",
    "            # Get observation from the environment\n",
    "            obs = decision_steps.obs[0][0]  # Assuming single agent\n",
    "            \n",
    "            # Convert the observation to a tensor\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "            # Get the action and value from the policy model\n",
    "            action_logits, value = model(obs_tensor)\n",
    "\n",
    "            # Ensure the action is a 2D tensor (1, 2) for continuous action space\n",
    "            if action_logits.shape != (1, 2):\n",
    "                action_logits = action_logits.reshape(1, 2)  # Reshape to (1, 2) if necessary\n",
    "\n",
    "            # Create action distribution and sample an action\n",
    "            act_dist = Categorical(logits=action_logits)\n",
    "            action = act_dist.sample()\n",
    "            act_log_prob = act_dist.log_prob(action).item()\n",
    "\n",
    "            # Ensure action has the correct shape (1, 2) for continuous action\n",
    "            if action.shape == (1,):  # Check if action is a single value\n",
    "                action = action.unsqueeze(0)  # Reshape to (1, 2) if it's a scalar\n",
    "                action = action.expand(1, 2)  # Duplicate the value to match the shape (1, 2)\n",
    "\n",
    "            # Store the observation, action, reward, value, and log probability\n",
    "            train_data[0].append(obs)  # Observation\n",
    "            train_data[1].append(action.detach().numpy())  # Action\n",
    "            train_data[2].append(decision_steps[0].reward)  # Reward (for decision steps)\n",
    "            train_data[3].append(value.item())  # Value from the model\n",
    "            train_data[4].append(act_log_prob)  # Log probability of the action\n",
    "\n",
    "            # Set the action and step the environment\n",
    "            action_tuple = ActionTuple()\n",
    "            action_tuple.add_continuous(action.detach().numpy())  # Detach the tensor before converting to numpy\n",
    "            env.set_actions(name, action_tuple)\n",
    "            env.step()\n",
    "\n",
    "            # Collect rewards\n",
    "            for agent_id in decision_steps:\n",
    "                total_reward += decision_steps[agent_id].reward\n",
    "\n",
    "            # Check if the episode has ended (by terminal steps)\n",
    "            if len(terminal_steps) > 0:\n",
    "                for agent_id in terminal_steps:\n",
    "                    total_reward += terminal_steps[agent_id].reward\n",
    "                    # Store at the end of the episode\n",
    "                    train_data[0].append(obs)\n",
    "                    train_data[1].append(action.detach().numpy())\n",
    "                    train_data[2].append(terminal_steps[agent_id].reward)\n",
    "                    train_data[3].append(value.item())  # Final value prediction\n",
    "                    train_data[4].append(act_log_prob)  # Final log probability\n",
    "                done = True\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "\n",
    "    # Calculate GAEs using the stored rewards and values\n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3], gamma, lam)\n",
    "\n",
    "    return train_data, total_reward\n",
    "\n",
    "worker_id = 0\n",
    "show_visuals = False;\n",
    "while worker_id <= 64:\n",
    "    try:\n",
    "        env = UnityEnvironment(file_name=\"../Evo_Project\", worker_id=worker_id, no_graphics=not show_visuals)\n",
    "        env.reset()\n",
    "        break\n",
    "    except UnityWorkerInUseException:\n",
    "        worker_id += 1\n",
    "\n",
    "# Keep top N best individuals\n",
    "def elitism(population, fitness_scores, num_best=5):\n",
    "    sorted_indices = np.argsort(fitness_scores)[::-1]  # Descending order of fitness scores\n",
    "    best_individuals = [population[i] for i in sorted_indices[:num_best]]\n",
    "    return best_individuals\n",
    "\n",
    "def adaptive_mutation_rate(generation, max_generations, initial_rate=0.1, min_rate=0.01):\n",
    "    # Linearly decay mutation rate from initial_rate to min_rate over generations\n",
    "    rate = initial_rate - (initial_rate - min_rate) * (generation / max_generations)\n",
    "    return rate\n",
    "\n",
    "def evolutionary_process(population, fitness_scores, num_parents, pop_size):\n",
    "    \n",
    "    # Create next generation using selected parents\n",
    "    next_generation = []\n",
    "    for i in range(pop_size):\n",
    "        parent1, parent2 = np.random.choice(parents, 2, replace=False)  # Select parents from the elite\n",
    "        child = crossover(parent1, parent2)\n",
    "        mutate(child)  # Apply mutation\n",
    "        next_generation.append(child)\n",
    "\n",
    "    return next_generation\n",
    "\n",
    "behavior_name = list(env.behavior_specs.keys())[0]\n",
    "spec = env.behavior_specs[behavior_name]\n",
    "state_dim = sum([obs.shape[0] for obs in spec.observation_specs])  # Total observation dimensions\n",
    "action_spec = spec.action_spec\n",
    "action_dim = action_spec.discrete_size if action_spec.is_discrete() else action_spec.continuous_size\n",
    "\n",
    "# Define the environment and parameters\n",
    "# state_dim = env.observation_space.shape[0]\n",
    "# action_dim = env.action_space.n\n",
    "n_episodes = 200\n",
    "print_freq = 20\n",
    "pop_size = 50\n",
    "num_parents = 10\n",
    "num_generations = 50\n",
    "\n",
    "\n",
    "# Initialize the population\n",
    "population = create_population(pop_size, state_dim, action_dim)\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    \n",
    "        # Evaluate the current population\n",
    "        fitness_scores = evaluate_population(population, env, behavior_name)\n",
    "\n",
    "        # Elitism: select the top 5 individuals based on fitness\n",
    "        parents = elitism(population, fitness_scores, num_best=5)\n",
    "\n",
    "        # Select the best parents based on fitness scores\n",
    "        parents = select_parents(population, fitness_scores, num_parents)\n",
    "\n",
    "        # Apply the evolutionary process to generate the next generation\n",
    "        next_generation = evolutionary_process(parents, fitness_scores, num_parents, pop_size)\n",
    "\n",
    "        print(f\"Generation: {generation + 1}\")\n",
    "\n",
    "        # Fine-tune the best policy using PPO\n",
    "        best_policy = parents[0]  # Select the best policy for further training with PPO\n",
    "        ppo = PPOTrainer(best_policy.model, ppo_clip_val=best_policy.ppo_clip_val, target_kl_div=best_policy.target_kl_div,\n",
    "                         policy_lr=best_policy.policy_lr, value_lr=best_policy.value_lr)\n",
    "\n",
    "        ep_rewards = []\n",
    "        for episode_idx in range(n_episodes):\n",
    "            # Perform rollout\n",
    "            train_data, reward = unity_rollout(best_policy.model, env, behavior_name)\n",
    "            ep_rewards.append(reward)\n",
    "\n",
    "            # Permute the indices\n",
    "            permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "\n",
    "            # Convert the train_data[0] (observations) and train_data[1] (actions) to torch tensors\n",
    "            obs = torch.tensor(np.array(train_data[0])[permute_idxs], dtype=torch.float32)\n",
    "            act = torch.tensor(np.array(train_data[1])[permute_idxs], dtype=torch.float32)\n",
    "            gaes = torch.tensor(np.array(train_data[3])[permute_idxs], dtype=torch.float32)  # If using GAEs, ensure it's in train_data[3]\n",
    "\n",
    "            act_log_probs = torch.tensor(train_data[4][permute_idxs], dtype=torch.float32)\n",
    "\n",
    "            # Value Data\n",
    "            returns = discount_rewards(train_data[2])[permute_idxs]\n",
    "            returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "            # Train Policy\n",
    "            ppo.train_policy(obs, act, act_log_probs, gaes)\n",
    "            ppo.train_value(obs, returns)\n",
    "\n",
    "            # Print average reward every 'print_freq' episodes\n",
    "            if (episode_idx + 1) % print_freq == 0:\n",
    "                avg_reward = np.mean(ep_rewards[-print_freq:])  # Calculate the average of the last 'print_freq' rewards\n",
    "                print(f'Generation {generation + 1} | Episode {episode_idx + 1} | Avg Reward {avg_reward:.1f}')\n",
    "\n",
    "        # Calculate and print the overall average reward for this generation\n",
    "        generation_avg_reward = np.mean(ep_rewards)  # Calculate average for all episodes in this generation\n",
    "        print(f\"Generation {generation + 1} Average Reward: {generation_avg_reward:.1f}\")\n",
    "\n",
    "        # Set the population for the next generation\n",
    "        population = next_generation\n",
    "\n",
    "        # Reset the environment for the next generation\n",
    "        env.reset()\n",
    "\n",
    "    # After training finishes\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a824d5a-3f33-44bc-9f59-1b5031657147",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()  # Reset environment before starting\n",
    "max_reward = -float('inf')  # Initialize max reward to very low\n",
    "\n",
    "for _ in range(100):  # Run a number of steps to gather rewards\n",
    "    action = env.action_space.sample()  # Take a random action\n",
    "    state, reward, done, truncated, _ = env.step(action)  # Perform the action\n",
    "    max_reward = max(max_reward, reward)  # Track maximum reward\n",
    "\n",
    "    if done or truncated:\n",
    "        env.reset()  # Reset environment if done or truncated\n",
    "\n",
    "print(f\"Maximum reward observed: {max_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10da9f0-9157-4554-9d96-da916c8a84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only with modified population not hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9613a333-815f-4a4b-8d25-12d4a7f84ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731500056.625042 8729940 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "Mono path[0] = '/Users/rohitpenna/Documents/GitHub/Evo_Project.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/rohitpenna/Documents/GitHub/Evo_Project.app/Contents/MonoBleedingEdge/etc'\n",
      "[Physics::Module] Initialized fallback backend.\n",
      "[Physics::Module] Id: 0xdecafbad\n",
      "New input system (experimental) initialized\n",
      "Initialize engine version: 6000.0.25f1 (4859ab7b5a49)\n",
      "[Subsystems] Discovering subsystems at path /Users/rohitpenna/Documents/GitHub/Evo_Project.app/Contents/Resources/Data/UnitySubsystems\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "Begin MonoManager ReloadAssembly\n",
      "- Loaded All Assemblies, in  0.067 seconds\n",
      "- Finished resetting the current domain, in  0.001 seconds\n",
      "[Physics::Module] Selected backend.\n",
      "[Physics::Module] Name: PhysX\n",
      "[Physics::Module] Id: 0xf2b8ea05\n",
      "[Physics::Module] SDK Version: 4.1.2\n",
      "[Physics::Module] Integration Version: 1.0.0\n",
      "[Physics::Module] Threading Mode: Multi-Threaded\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/LutBuilderLdr shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/CameraMotionBlur shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/TemporalAA shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/Scaling Setup shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/CoreSRP/CoreCopy shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Hidden/Universal Render Pipeline/FallbackError' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/FallbackError shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Hidden/Universal Render Pipeline/FallbackError' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Hidden/Universal Render Pipeline/StencilDeferred' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Hidden/Universal Render Pipeline/StencilDeferred' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/GaussianDepthOfField shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/Debug/DebugReplacement shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/LutBuilderHdr shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/Sampling shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/LensFlareDataDriven shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/CameraMotionVectors shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/UberPost shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/Bloom shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/BokehDepthOfField shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/FinalPost shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/CopyDepth shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/SubpixelMorphologicalAntialiasing shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/ScreenSpaceAmbientOcclusion shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/Stop NaN shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal/HDRDebugView shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/LensFlareScreenSpace shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Hidden/Universal Render Pipeline/PaniniProjection shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Universal Render Pipeline/Lit' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Universal Render Pipeline/Lit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Universal Render Pipeline/Lit' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.167166 ms\n",
      "Registered Communicator in Agent.\n",
      "Generation: 1\n",
      "Generation 1 | Episode 20 | Avg Reward 0.1\n",
      "Generation 1 | Episode 40 | Avg Reward 0.1\n",
      "Generation 1 | Episode 60 | Avg Reward 0.1\n",
      "Generation 1 | Episode 80 | Avg Reward 0.1\n",
      "Generation 1 | Episode 100 | Avg Reward 0.2\n",
      "Generation 1 | Episode 120 | Avg Reward 0.1\n",
      "Generation 1 | Episode 140 | Avg Reward 0.1\n",
      "Generation 1 | Episode 160 | Avg Reward 0.1\n",
      "Generation 1 | Episode 180 | Avg Reward 0.1\n",
      "Generation 1 | Episode 200 | Avg Reward 0.3\n",
      "Generation 1 Average Reward: 0.1\n",
      "Generation: 2\n",
      "Generation 2 | Episode 20 | Avg Reward 0.1\n",
      "Generation 2 | Episode 40 | Avg Reward 0.2\n",
      "Generation 2 | Episode 60 | Avg Reward 0.1\n",
      "Generation 2 | Episode 80 | Avg Reward 0.1\n",
      "Generation 2 | Episode 100 | Avg Reward 0.1\n",
      "Generation 2 | Episode 120 | Avg Reward 0.1\n",
      "Generation 2 | Episode 140 | Avg Reward 0.3\n",
      "Generation 2 | Episode 160 | Avg Reward 0.1\n",
      "Generation 2 | Episode 180 | Avg Reward 0.1\n",
      "Generation 2 | Episode 200 | Avg Reward 0.3\n",
      "Generation 2 Average Reward: 0.2\n",
      "Generation: 3\n",
      "Generation 3 | Episode 20 | Avg Reward 0.0\n",
      "Generation 3 | Episode 40 | Avg Reward 0.1\n",
      "Generation 3 | Episode 60 | Avg Reward 0.0\n",
      "Generation 3 | Episode 80 | Avg Reward 0.1\n",
      "Generation 3 | Episode 100 | Avg Reward 0.1\n",
      "Generation 3 | Episode 120 | Avg Reward 0.2\n",
      "Generation 3 | Episode 140 | Avg Reward 0.1\n",
      "Generation 3 | Episode 160 | Avg Reward 0.1\n",
      "Generation 3 | Episode 180 | Avg Reward 0.1\n",
      "Generation 3 | Episode 200 | Avg Reward 0.1\n",
      "Generation 3 Average Reward: 0.1\n",
      "Generation: 4\n",
      "Generation 4 | Episode 20 | Avg Reward 0.1\n",
      "Generation 4 | Episode 40 | Avg Reward 0.1\n",
      "Generation 4 | Episode 60 | Avg Reward 0.1\n",
      "Generation 4 | Episode 80 | Avg Reward 0.1\n",
      "Generation 4 | Episode 100 | Avg Reward 0.1\n",
      "Generation 4 | Episode 120 | Avg Reward 0.1\n",
      "Generation 4 | Episode 140 | Avg Reward 0.0\n",
      "Generation 4 | Episode 160 | Avg Reward 0.1\n",
      "Generation 4 | Episode 180 | Avg Reward 0.1\n",
      "Generation 4 | Episode 200 | Avg Reward 0.1\n",
      "Generation 4 Average Reward: 0.1\n",
      "Generation: 5\n",
      "Generation 5 | Episode 20 | Avg Reward 0.0\n",
      "Generation 5 | Episode 40 | Avg Reward 0.1\n",
      "Generation 5 | Episode 60 | Avg Reward 0.1\n",
      "Generation 5 | Episode 80 | Avg Reward 0.1\n",
      "Generation 5 | Episode 100 | Avg Reward 0.1\n",
      "Generation 5 | Episode 120 | Avg Reward 0.1\n",
      "Generation 5 | Episode 140 | Avg Reward 0.2\n",
      "Generation 5 | Episode 160 | Avg Reward 0.1\n",
      "Generation 5 | Episode 180 | Avg Reward 0.1\n",
      "Generation 5 | Episode 200 | Avg Reward 0.1\n",
      "Generation 5 Average Reward: 0.1\n",
      "Generation: 6\n",
      "Generation 6 | Episode 20 | Avg Reward 0.1\n",
      "Generation 6 | Episode 40 | Avg Reward 0.1\n",
      "Generation 6 | Episode 60 | Avg Reward 0.1\n",
      "Generation 6 | Episode 80 | Avg Reward 0.0\n",
      "Generation 6 | Episode 100 | Avg Reward 0.1\n",
      "Generation 6 | Episode 120 | Avg Reward 0.1\n",
      "Generation 6 | Episode 140 | Avg Reward 0.1\n",
      "Generation 6 | Episode 160 | Avg Reward 0.2\n",
      "Generation 6 | Episode 180 | Avg Reward 0.1\n",
      "Generation 6 | Episode 200 | Avg Reward 0.1\n",
      "Generation 6 Average Reward: 0.1\n",
      "Generation: 7\n",
      "Generation 7 | Episode 20 | Avg Reward 0.1\n",
      "Generation 7 | Episode 40 | Avg Reward 0.0\n",
      "Generation 7 | Episode 60 | Avg Reward 0.3\n",
      "Generation 7 | Episode 80 | Avg Reward 0.2\n",
      "Generation 7 | Episode 100 | Avg Reward 0.1\n",
      "Generation 7 | Episode 120 | Avg Reward 0.1\n",
      "Generation 7 | Episode 140 | Avg Reward 0.0\n",
      "Generation 7 | Episode 160 | Avg Reward 0.2\n",
      "Generation 7 | Episode 180 | Avg Reward 0.1\n",
      "Generation 7 | Episode 200 | Avg Reward 0.1\n",
      "Generation 7 Average Reward: 0.1\n",
      "Generation: 8\n",
      "Generation 8 | Episode 20 | Avg Reward 0.0\n",
      "Generation 8 | Episode 40 | Avg Reward 0.1\n",
      "Generation 8 | Episode 60 | Avg Reward 0.1\n",
      "Generation 8 | Episode 80 | Avg Reward 0.1\n",
      "Generation 8 | Episode 100 | Avg Reward 0.1\n",
      "Generation 8 | Episode 120 | Avg Reward 0.1\n",
      "Generation 8 | Episode 140 | Avg Reward 0.2\n",
      "Generation 8 | Episode 160 | Avg Reward 0.3\n",
      "Generation 8 | Episode 180 | Avg Reward 0.1\n",
      "Generation 8 | Episode 200 | Avg Reward 0.1\n",
      "Generation 8 Average Reward: 0.1\n",
      "Generation: 9\n",
      "Generation 9 | Episode 20 | Avg Reward 0.1\n",
      "Generation 9 | Episode 40 | Avg Reward 0.2\n",
      "Generation 9 | Episode 60 | Avg Reward 0.1\n",
      "Generation 9 | Episode 80 | Avg Reward 0.0\n",
      "Generation 9 | Episode 100 | Avg Reward 0.1\n",
      "Generation 9 | Episode 120 | Avg Reward 0.0\n",
      "Generation 9 | Episode 140 | Avg Reward 0.1\n",
      "Generation 9 | Episode 160 | Avg Reward 0.1\n",
      "Generation 9 | Episode 180 | Avg Reward 0.1\n",
      "Generation 9 | Episode 200 | Avg Reward 0.1\n",
      "Generation 9 Average Reward: 0.1\n",
      "Generation: 10\n",
      "Generation 10 | Episode 20 | Avg Reward 0.1\n",
      "Generation 10 | Episode 40 | Avg Reward 0.1\n",
      "Generation 10 | Episode 60 | Avg Reward 0.1\n",
      "Generation 10 | Episode 80 | Avg Reward 0.2\n",
      "Generation 10 | Episode 100 | Avg Reward 0.0\n",
      "Generation 10 | Episode 120 | Avg Reward 0.1\n",
      "Generation 10 | Episode 140 | Avg Reward 0.1\n",
      "Generation 10 | Episode 160 | Avg Reward 0.1\n",
      "Generation 10 | Episode 180 | Avg Reward 0.2\n",
      "Generation 10 | Episode 200 | Avg Reward 0.1\n",
      "Generation 10 Average Reward: 0.1\n",
      "Generation: 11\n",
      "Generation 11 | Episode 20 | Avg Reward 0.1\n",
      "Generation 11 | Episode 40 | Avg Reward 0.1\n",
      "Generation 11 | Episode 60 | Avg Reward 0.0\n",
      "Generation 11 | Episode 80 | Avg Reward 0.1\n",
      "Generation 11 | Episode 100 | Avg Reward 0.1\n",
      "Generation 11 | Episode 120 | Avg Reward 0.1\n",
      "Generation 11 | Episode 140 | Avg Reward 0.1\n",
      "Generation 11 | Episode 160 | Avg Reward 0.1\n",
      "Generation 11 | Episode 180 | Avg Reward 0.2\n",
      "Generation 11 | Episode 200 | Avg Reward 0.1\n",
      "Generation 11 Average Reward: 0.1\n",
      "Generation: 12\n",
      "Generation 12 | Episode 20 | Avg Reward 0.2\n",
      "Generation 12 | Episode 40 | Avg Reward 0.1\n",
      "Generation 12 | Episode 60 | Avg Reward 0.2\n",
      "Generation 12 | Episode 80 | Avg Reward 0.1\n",
      "Generation 12 | Episode 100 | Avg Reward 0.1\n",
      "Generation 12 | Episode 120 | Avg Reward 0.1\n",
      "Generation 12 | Episode 140 | Avg Reward 0.2\n",
      "Generation 12 | Episode 160 | Avg Reward 0.1\n",
      "Generation 12 | Episode 180 | Avg Reward 0.1\n",
      "Generation 12 | Episode 200 | Avg Reward 0.2\n",
      "Generation 12 Average Reward: 0.1\n",
      "Generation: 13\n",
      "Generation 13 | Episode 20 | Avg Reward 0.1\n",
      "Generation 13 | Episode 40 | Avg Reward 0.1\n",
      "Generation 13 | Episode 60 | Avg Reward 0.3\n",
      "Generation 13 | Episode 80 | Avg Reward 0.2\n",
      "Generation 13 | Episode 100 | Avg Reward 0.1\n",
      "Generation 13 | Episode 120 | Avg Reward 0.1\n",
      "Generation 13 | Episode 140 | Avg Reward 0.1\n",
      "Generation 13 | Episode 160 | Avg Reward 0.1\n",
      "Generation 13 | Episode 180 | Avg Reward 0.1\n",
      "Generation 13 | Episode 200 | Avg Reward 0.1\n",
      "Generation 13 Average Reward: 0.1\n",
      "Generation: 14\n",
      "Generation 14 | Episode 20 | Avg Reward 0.2\n",
      "Generation 14 | Episode 40 | Avg Reward 0.1\n",
      "Generation 14 | Episode 60 | Avg Reward 0.1\n",
      "Generation 14 | Episode 80 | Avg Reward 0.0\n",
      "Generation 14 | Episode 100 | Avg Reward 0.2\n",
      "Generation 14 | Episode 120 | Avg Reward 0.1\n",
      "Generation 14 | Episode 140 | Avg Reward 0.0\n",
      "Generation 14 | Episode 160 | Avg Reward 0.2\n",
      "Generation 14 | Episode 180 | Avg Reward 0.0\n",
      "Generation 14 | Episode 200 | Avg Reward 0.1\n",
      "Generation 14 Average Reward: 0.1\n",
      "Generation: 15\n",
      "Generation 15 | Episode 20 | Avg Reward 0.1\n",
      "Generation 15 | Episode 40 | Avg Reward 0.1\n",
      "Generation 15 | Episode 60 | Avg Reward 0.1\n",
      "Generation 15 | Episode 80 | Avg Reward 0.0\n",
      "Generation 15 | Episode 100 | Avg Reward 0.3\n",
      "Generation 15 | Episode 120 | Avg Reward 0.1\n",
      "Generation 15 | Episode 140 | Avg Reward 0.1\n",
      "Generation 15 | Episode 160 | Avg Reward 0.2\n",
      "Generation 15 | Episode 180 | Avg Reward 0.1\n",
      "Generation 15 | Episode 200 | Avg Reward 0.2\n",
      "Generation 15 Average Reward: 0.1\n",
      "Generation: 16\n",
      "Generation 16 | Episode 20 | Avg Reward 0.1\n",
      "Generation 16 | Episode 40 | Avg Reward 0.2\n",
      "Generation 16 | Episode 60 | Avg Reward 0.1\n",
      "Generation 16 | Episode 80 | Avg Reward 0.1\n",
      "Generation 16 | Episode 100 | Avg Reward 0.0\n",
      "Generation 16 | Episode 120 | Avg Reward 0.1\n",
      "Generation 16 | Episode 140 | Avg Reward 0.1\n",
      "Generation 16 | Episode 160 | Avg Reward 0.1\n",
      "Generation 16 | Episode 180 | Avg Reward 0.2\n",
      "Generation 16 | Episode 200 | Avg Reward 0.1\n",
      "Generation 16 Average Reward: 0.1\n",
      "Generation: 17\n",
      "Generation 17 | Episode 20 | Avg Reward 0.2\n",
      "Generation 17 | Episode 40 | Avg Reward 0.1\n",
      "Generation 17 | Episode 60 | Avg Reward 0.1\n",
      "Generation 17 | Episode 80 | Avg Reward 0.1\n",
      "Generation 17 | Episode 100 | Avg Reward 0.1\n",
      "Generation 17 | Episode 120 | Avg Reward 0.1\n",
      "Generation 17 | Episode 140 | Avg Reward 0.1\n",
      "Generation 17 | Episode 160 | Avg Reward 0.1\n",
      "Generation 17 | Episode 180 | Avg Reward 0.1\n",
      "Generation 17 | Episode 200 | Avg Reward 0.1\n",
      "Generation 17 Average Reward: 0.1\n",
      "Generation: 18\n",
      "Generation 18 | Episode 20 | Avg Reward 0.1\n",
      "Generation 18 | Episode 40 | Avg Reward 0.1\n",
      "Generation 18 | Episode 60 | Avg Reward 0.1\n",
      "Generation 18 | Episode 80 | Avg Reward 0.2\n",
      "Generation 18 | Episode 100 | Avg Reward 0.2\n",
      "Generation 18 | Episode 120 | Avg Reward 0.1\n",
      "Generation 18 | Episode 140 | Avg Reward 0.1\n",
      "Generation 18 | Episode 160 | Avg Reward 0.1\n",
      "Generation 18 | Episode 180 | Avg Reward 0.0\n",
      "Generation 18 | Episode 200 | Avg Reward 0.1\n",
      "Generation 18 Average Reward: 0.1\n",
      "Generation: 19\n",
      "Generation 19 | Episode 20 | Avg Reward 0.2\n",
      "Generation 19 | Episode 40 | Avg Reward 0.1\n",
      "Generation 19 | Episode 60 | Avg Reward 0.1\n",
      "Generation 19 | Episode 80 | Avg Reward 0.1\n",
      "Generation 19 | Episode 100 | Avg Reward 0.1\n",
      "Generation 19 | Episode 120 | Avg Reward 0.2\n",
      "Generation 19 | Episode 140 | Avg Reward 0.0\n",
      "Generation 19 | Episode 160 | Avg Reward 0.0\n",
      "Generation 19 | Episode 180 | Avg Reward 0.1\n",
      "Generation 19 | Episode 200 | Avg Reward 0.1\n",
      "Generation 19 Average Reward: 0.1\n",
      "Generation: 20\n",
      "Generation 20 | Episode 20 | Avg Reward 0.2\n",
      "Generation 20 | Episode 40 | Avg Reward 0.2\n",
      "Generation 20 | Episode 60 | Avg Reward 0.1\n",
      "Generation 20 | Episode 80 | Avg Reward 0.1\n",
      "Generation 20 | Episode 100 | Avg Reward 0.2\n",
      "Generation 20 | Episode 120 | Avg Reward 0.1\n",
      "Generation 20 | Episode 140 | Avg Reward 0.1\n",
      "Generation 20 | Episode 160 | Avg Reward 0.1\n",
      "Generation 20 | Episode 180 | Avg Reward 0.1\n",
      "Generation 20 | Episode 200 | Avg Reward 0.0\n",
      "Generation 20 Average Reward: 0.1\n",
      "Generation: 21\n",
      "Generation 21 | Episode 20 | Avg Reward 0.1\n",
      "Generation 21 | Episode 40 | Avg Reward 0.2\n",
      "Generation 21 | Episode 60 | Avg Reward 0.1\n",
      "Generation 21 | Episode 80 | Avg Reward 0.1\n",
      "Generation 21 | Episode 100 | Avg Reward 0.1\n",
      "Generation 21 | Episode 120 | Avg Reward 0.1\n",
      "Generation 21 | Episode 140 | Avg Reward 0.2\n",
      "Generation 21 | Episode 160 | Avg Reward 0.1\n",
      "Generation 21 | Episode 180 | Avg Reward 0.0\n",
      "Generation 21 | Episode 200 | Avg Reward 0.2\n",
      "Generation 21 Average Reward: 0.1\n",
      "Generation: 22\n",
      "Generation 22 | Episode 20 | Avg Reward 0.1\n",
      "Generation 22 | Episode 40 | Avg Reward 0.1\n",
      "Generation 22 | Episode 60 | Avg Reward 0.1\n",
      "Generation 22 | Episode 80 | Avg Reward 0.1\n",
      "Generation 22 | Episode 100 | Avg Reward 0.1\n",
      "Generation 22 | Episode 120 | Avg Reward 0.1\n",
      "Generation 22 | Episode 140 | Avg Reward 0.2\n",
      "Generation 22 | Episode 160 | Avg Reward 0.2\n",
      "Generation 22 | Episode 180 | Avg Reward 0.1\n",
      "Generation 22 | Episode 200 | Avg Reward 0.1\n",
      "Generation 22 Average Reward: 0.1\n",
      "Generation: 23\n",
      "Generation 23 | Episode 20 | Avg Reward 0.0\n",
      "Generation 23 | Episode 40 | Avg Reward 0.1\n",
      "Generation 23 | Episode 60 | Avg Reward 0.2\n",
      "Generation 23 | Episode 80 | Avg Reward 0.1\n",
      "Generation 23 | Episode 100 | Avg Reward 0.1\n",
      "Generation 23 | Episode 120 | Avg Reward 0.1\n",
      "Generation 23 | Episode 140 | Avg Reward 0.1\n",
      "Generation 23 | Episode 160 | Avg Reward 0.1\n",
      "Generation 23 | Episode 180 | Avg Reward 0.2\n",
      "Generation 23 | Episode 200 | Avg Reward 0.1\n",
      "Generation 23 Average Reward: 0.1\n",
      "Generation: 24\n",
      "Generation 24 | Episode 20 | Avg Reward 0.2\n",
      "Generation 24 | Episode 40 | Avg Reward 0.3\n",
      "Generation 24 | Episode 60 | Avg Reward 0.1\n",
      "Generation 24 | Episode 80 | Avg Reward 0.0\n",
      "Generation 24 | Episode 100 | Avg Reward 0.1\n",
      "Generation 24 | Episode 120 | Avg Reward 0.1\n",
      "Generation 24 | Episode 140 | Avg Reward 0.1\n",
      "Generation 24 | Episode 160 | Avg Reward 0.1\n",
      "Generation 24 | Episode 180 | Avg Reward 0.0\n",
      "Generation 24 | Episode 200 | Avg Reward 0.3\n",
      "Generation 24 Average Reward: 0.1\n",
      "Generation: 25\n",
      "Generation 25 | Episode 20 | Avg Reward 0.1\n",
      "Generation 25 | Episode 40 | Avg Reward 0.2\n",
      "Generation 25 | Episode 60 | Avg Reward 0.1\n",
      "Generation 25 | Episode 80 | Avg Reward 0.1\n",
      "Generation 25 | Episode 100 | Avg Reward 0.1\n",
      "Generation 25 | Episode 120 | Avg Reward 0.1\n",
      "Generation 25 | Episode 140 | Avg Reward 0.1\n",
      "Generation 25 | Episode 160 | Avg Reward 0.1\n",
      "Generation 25 | Episode 180 | Avg Reward 0.2\n",
      "Generation 25 | Episode 200 | Avg Reward 0.1\n",
      "Generation 25 Average Reward: 0.1\n",
      "Generation: 26\n",
      "Generation 26 | Episode 20 | Avg Reward 0.1\n",
      "Generation 26 | Episode 40 | Avg Reward 0.1\n",
      "Generation 26 | Episode 60 | Avg Reward 0.1\n",
      "Generation 26 | Episode 80 | Avg Reward 0.1\n",
      "Generation 26 | Episode 100 | Avg Reward 0.1\n",
      "Generation 26 | Episode 120 | Avg Reward 0.1\n",
      "Generation 26 | Episode 140 | Avg Reward 0.1\n",
      "Generation 26 | Episode 160 | Avg Reward 0.1\n",
      "Generation 26 | Episode 180 | Avg Reward 0.1\n",
      "Generation 26 | Episode 200 | Avg Reward 0.2\n",
      "Generation 26 Average Reward: 0.1\n",
      "Generation: 27\n",
      "Generation 27 | Episode 20 | Avg Reward 0.1\n",
      "Generation 27 | Episode 40 | Avg Reward 0.1\n",
      "Generation 27 | Episode 60 | Avg Reward 0.1\n",
      "Generation 27 | Episode 80 | Avg Reward 0.0\n",
      "Generation 27 | Episode 100 | Avg Reward 0.1\n",
      "Generation 27 | Episode 120 | Avg Reward 0.1\n",
      "Generation 27 | Episode 140 | Avg Reward 0.0\n",
      "Generation 27 | Episode 160 | Avg Reward 0.1\n",
      "Generation 27 | Episode 180 | Avg Reward 0.2\n",
      "Generation 27 | Episode 200 | Avg Reward 0.3\n",
      "Generation 27 Average Reward: 0.1\n",
      "Generation: 28\n",
      "Generation 28 | Episode 20 | Avg Reward 0.1\n",
      "Generation 28 | Episode 40 | Avg Reward 0.2\n",
      "Generation 28 | Episode 60 | Avg Reward 0.1\n",
      "Generation 28 | Episode 80 | Avg Reward 0.1\n",
      "Generation 28 | Episode 100 | Avg Reward 0.1\n",
      "Generation 28 | Episode 120 | Avg Reward 0.1\n",
      "Generation 28 | Episode 140 | Avg Reward 0.1\n",
      "Generation 28 | Episode 160 | Avg Reward 0.0\n",
      "Generation 28 | Episode 180 | Avg Reward 0.1\n",
      "Generation 28 | Episode 200 | Avg Reward 0.1\n",
      "Generation 28 Average Reward: 0.1\n",
      "Generation: 29\n",
      "Generation 29 | Episode 20 | Avg Reward 0.1\n",
      "Generation 29 | Episode 40 | Avg Reward 0.0\n",
      "Generation 29 | Episode 60 | Avg Reward 0.1\n",
      "Generation 29 | Episode 80 | Avg Reward 0.1\n",
      "Generation 29 | Episode 100 | Avg Reward 0.1\n",
      "Generation 29 | Episode 120 | Avg Reward 0.1\n",
      "Generation 29 | Episode 140 | Avg Reward 0.0\n",
      "Generation 29 | Episode 160 | Avg Reward 0.1\n",
      "Generation 29 | Episode 180 | Avg Reward 0.1\n",
      "Generation 29 | Episode 200 | Avg Reward 0.0\n",
      "Generation 29 Average Reward: 0.1\n",
      "Generation: 30\n",
      "Generation 30 | Episode 20 | Avg Reward 0.1\n",
      "Generation 30 | Episode 40 | Avg Reward 0.1\n",
      "Generation 30 | Episode 60 | Avg Reward 0.1\n",
      "Generation 30 | Episode 80 | Avg Reward 0.2\n",
      "Generation 30 | Episode 100 | Avg Reward 0.1\n",
      "Generation 30 | Episode 120 | Avg Reward 0.1\n",
      "Generation 30 | Episode 140 | Avg Reward 0.2\n",
      "Generation 30 | Episode 160 | Avg Reward 0.1\n",
      "Generation 30 | Episode 180 | Avg Reward 0.0\n",
      "Generation 30 | Episode 200 | Avg Reward 0.3\n",
      "Generation 30 Average Reward: 0.1\n",
      "Generation: 31\n",
      "Generation 31 | Episode 20 | Avg Reward 0.2\n",
      "Generation 31 | Episode 40 | Avg Reward 0.1\n",
      "Generation 31 | Episode 60 | Avg Reward 0.0\n",
      "Generation 31 | Episode 80 | Avg Reward 0.1\n",
      "Generation 31 | Episode 100 | Avg Reward 0.1\n",
      "Generation 31 | Episode 120 | Avg Reward 0.0\n",
      "Generation 31 | Episode 140 | Avg Reward 0.1\n",
      "Generation 31 | Episode 160 | Avg Reward 0.1\n",
      "Generation 31 | Episode 180 | Avg Reward 0.2\n",
      "Generation 31 | Episode 200 | Avg Reward 0.1\n",
      "Generation 31 Average Reward: 0.1\n",
      "Generation: 32\n",
      "Generation 32 | Episode 20 | Avg Reward 0.1\n",
      "Generation 32 | Episode 40 | Avg Reward 0.2\n",
      "Generation 32 | Episode 60 | Avg Reward 0.1\n",
      "Generation 32 | Episode 80 | Avg Reward 0.2\n",
      "Generation 32 | Episode 100 | Avg Reward 0.2\n",
      "Generation 32 | Episode 120 | Avg Reward 0.1\n",
      "Generation 32 | Episode 140 | Avg Reward 0.3\n",
      "Generation 32 | Episode 160 | Avg Reward 0.1\n",
      "Generation 32 | Episode 180 | Avg Reward 0.1\n",
      "Generation 32 | Episode 200 | Avg Reward 0.1\n",
      "Generation 32 Average Reward: 0.1\n",
      "Generation: 33\n",
      "Generation 33 | Episode 20 | Avg Reward 0.2\n",
      "Generation 33 | Episode 40 | Avg Reward 0.2\n",
      "Generation 33 | Episode 60 | Avg Reward 0.1\n",
      "Generation 33 | Episode 80 | Avg Reward 0.1\n",
      "Generation 33 | Episode 100 | Avg Reward 0.2\n",
      "Generation 33 | Episode 120 | Avg Reward 0.1\n",
      "Generation 33 | Episode 140 | Avg Reward 0.1\n",
      "Generation 33 | Episode 160 | Avg Reward 0.0\n",
      "Generation 33 | Episode 180 | Avg Reward 0.1\n",
      "Generation 33 | Episode 200 | Avg Reward 0.1\n",
      "Generation 33 Average Reward: 0.1\n",
      "Generation: 34\n",
      "Generation 34 | Episode 20 | Avg Reward 0.1\n",
      "Generation 34 | Episode 40 | Avg Reward 0.1\n",
      "Generation 34 | Episode 60 | Avg Reward 0.1\n",
      "Generation 34 | Episode 80 | Avg Reward 0.1\n",
      "Generation 34 | Episode 100 | Avg Reward 0.1\n",
      "Generation 34 | Episode 120 | Avg Reward 0.3\n",
      "Generation 34 | Episode 140 | Avg Reward 0.1\n",
      "Generation 34 | Episode 160 | Avg Reward 0.1\n",
      "Generation 34 | Episode 180 | Avg Reward 0.3\n",
      "Generation 34 | Episode 200 | Avg Reward 0.2\n",
      "Generation 34 Average Reward: 0.1\n",
      "Generation: 35\n",
      "Generation 35 | Episode 20 | Avg Reward 0.1\n",
      "Generation 35 | Episode 40 | Avg Reward 0.1\n",
      "Generation 35 | Episode 60 | Avg Reward 0.1\n",
      "Generation 35 | Episode 80 | Avg Reward 0.2\n",
      "Generation 35 | Episode 100 | Avg Reward 0.1\n",
      "Generation 35 | Episode 120 | Avg Reward 0.0\n",
      "Generation 35 | Episode 140 | Avg Reward 0.1\n",
      "Generation 35 | Episode 160 | Avg Reward 0.1\n",
      "Generation 35 | Episode 180 | Avg Reward 0.1\n",
      "Generation 35 | Episode 200 | Avg Reward 0.1\n",
      "Generation 35 Average Reward: 0.1\n",
      "Generation: 36\n",
      "Generation 36 | Episode 20 | Avg Reward 0.1\n",
      "Generation 36 | Episode 40 | Avg Reward 0.1\n",
      "Generation 36 | Episode 60 | Avg Reward 0.3\n",
      "Generation 36 | Episode 80 | Avg Reward 0.1\n",
      "Generation 36 | Episode 100 | Avg Reward 0.3\n",
      "Generation 36 | Episode 120 | Avg Reward 0.1\n",
      "Generation 36 | Episode 140 | Avg Reward 0.1\n",
      "Generation 36 | Episode 160 | Avg Reward 0.1\n",
      "Generation 36 | Episode 180 | Avg Reward 0.1\n",
      "Generation 36 | Episode 200 | Avg Reward 0.2\n",
      "Generation 36 Average Reward: 0.2\n",
      "Generation: 37\n",
      "Generation 37 | Episode 20 | Avg Reward 0.1\n",
      "Generation 37 | Episode 40 | Avg Reward 0.1\n",
      "Generation 37 | Episode 60 | Avg Reward 0.0\n",
      "Generation 37 | Episode 80 | Avg Reward 0.1\n",
      "Generation 37 | Episode 100 | Avg Reward 0.0\n",
      "Generation 37 | Episode 120 | Avg Reward 0.1\n",
      "Generation 37 | Episode 140 | Avg Reward 0.1\n",
      "Generation 37 | Episode 160 | Avg Reward 0.1\n",
      "Generation 37 | Episode 180 | Avg Reward 0.0\n",
      "Generation 37 | Episode 200 | Avg Reward 0.2\n",
      "Generation 37 Average Reward: 0.1\n",
      "Generation: 38\n",
      "Generation 38 | Episode 20 | Avg Reward 0.2\n",
      "Generation 38 | Episode 40 | Avg Reward 0.1\n",
      "Generation 38 | Episode 60 | Avg Reward 0.1\n",
      "Generation 38 | Episode 80 | Avg Reward 0.1\n",
      "Generation 38 | Episode 100 | Avg Reward 0.1\n",
      "Generation 38 | Episode 120 | Avg Reward 0.1\n",
      "Generation 38 | Episode 140 | Avg Reward 0.1\n",
      "Generation 38 | Episode 160 | Avg Reward 0.1\n",
      "Generation 38 | Episode 180 | Avg Reward 0.1\n",
      "Generation 38 | Episode 200 | Avg Reward 0.1\n",
      "Generation 38 Average Reward: 0.1\n",
      "Generation: 39\n",
      "Generation 39 | Episode 20 | Avg Reward 0.1\n",
      "Generation 39 | Episode 40 | Avg Reward 0.0\n",
      "Generation 39 | Episode 60 | Avg Reward 0.3\n",
      "Generation 39 | Episode 80 | Avg Reward 0.1\n",
      "Generation 39 | Episode 100 | Avg Reward 0.1\n",
      "Generation 39 | Episode 120 | Avg Reward 0.1\n",
      "Generation 39 | Episode 140 | Avg Reward 0.1\n",
      "Generation 39 | Episode 160 | Avg Reward 0.1\n",
      "Generation 39 | Episode 180 | Avg Reward 0.1\n",
      "Generation 39 | Episode 200 | Avg Reward 0.1\n",
      "Generation 39 Average Reward: 0.1\n",
      "Generation: 40\n",
      "Generation 40 | Episode 20 | Avg Reward 0.1\n",
      "Generation 40 | Episode 40 | Avg Reward 0.1\n",
      "Generation 40 | Episode 60 | Avg Reward 0.0\n",
      "Generation 40 | Episode 80 | Avg Reward 0.1\n",
      "Generation 40 | Episode 100 | Avg Reward 0.1\n",
      "Generation 40 | Episode 120 | Avg Reward 0.1\n",
      "Generation 40 | Episode 140 | Avg Reward 0.1\n",
      "Generation 40 | Episode 160 | Avg Reward 0.1\n",
      "Generation 40 | Episode 180 | Avg Reward 0.2\n",
      "Generation 40 | Episode 200 | Avg Reward 0.1\n",
      "Generation 40 Average Reward: 0.1\n",
      "Generation: 41\n",
      "Generation 41 | Episode 20 | Avg Reward 0.1\n",
      "Generation 41 | Episode 40 | Avg Reward 0.1\n",
      "Generation 41 | Episode 60 | Avg Reward 0.1\n",
      "Generation 41 | Episode 80 | Avg Reward 0.1\n",
      "Generation 41 | Episode 100 | Avg Reward 0.2\n",
      "Generation 41 | Episode 120 | Avg Reward 0.2\n",
      "Generation 41 | Episode 140 | Avg Reward 0.1\n",
      "Generation 41 | Episode 160 | Avg Reward 0.2\n",
      "Generation 41 | Episode 180 | Avg Reward 0.1\n",
      "Generation 41 | Episode 200 | Avg Reward 0.1\n",
      "Generation 41 Average Reward: 0.1\n",
      "Generation: 42\n",
      "Generation 42 | Episode 20 | Avg Reward 0.0\n",
      "Generation 42 | Episode 40 | Avg Reward 0.1\n",
      "Generation 42 | Episode 60 | Avg Reward 0.0\n",
      "Generation 42 | Episode 80 | Avg Reward 0.1\n",
      "Generation 42 | Episode 100 | Avg Reward 0.0\n",
      "Generation 42 | Episode 120 | Avg Reward 0.1\n",
      "Generation 42 | Episode 140 | Avg Reward 0.2\n",
      "Generation 42 | Episode 160 | Avg Reward 0.1\n",
      "Generation 42 | Episode 180 | Avg Reward 0.1\n",
      "Generation 42 | Episode 200 | Avg Reward 0.1\n",
      "Generation 42 Average Reward: 0.1\n",
      "Generation: 43\n",
      "Generation 43 | Episode 20 | Avg Reward 0.1\n",
      "Generation 43 | Episode 40 | Avg Reward 0.2\n",
      "Generation 43 | Episode 60 | Avg Reward 0.1\n",
      "Generation 43 | Episode 80 | Avg Reward 0.1\n",
      "Generation 43 | Episode 100 | Avg Reward 0.1\n",
      "Generation 43 | Episode 120 | Avg Reward 0.2\n",
      "Generation 43 | Episode 140 | Avg Reward 0.2\n",
      "Generation 43 | Episode 160 | Avg Reward 0.1\n",
      "Generation 43 | Episode 180 | Avg Reward 0.1\n",
      "Generation 43 | Episode 200 | Avg Reward 0.0\n",
      "Generation 43 Average Reward: 0.1\n",
      "Generation: 44\n",
      "Generation 44 | Episode 20 | Avg Reward 0.1\n",
      "Generation 44 | Episode 40 | Avg Reward 0.2\n",
      "Generation 44 | Episode 60 | Avg Reward 0.1\n",
      "Generation 44 | Episode 80 | Avg Reward 0.2\n",
      "Generation 44 | Episode 100 | Avg Reward 0.1\n",
      "Generation 44 | Episode 120 | Avg Reward 0.2\n",
      "Generation 44 | Episode 140 | Avg Reward 0.1\n",
      "Generation 44 | Episode 160 | Avg Reward 0.1\n",
      "Generation 44 | Episode 180 | Avg Reward 0.1\n",
      "Generation 44 | Episode 200 | Avg Reward 0.1\n",
      "Generation 44 Average Reward: 0.1\n",
      "Generation: 45\n",
      "Generation 45 | Episode 20 | Avg Reward 0.1\n",
      "Generation 45 | Episode 40 | Avg Reward 0.2\n",
      "Generation 45 | Episode 60 | Avg Reward 0.2\n",
      "Generation 45 | Episode 80 | Avg Reward 0.0\n",
      "Generation 45 | Episode 100 | Avg Reward 0.1\n",
      "Generation 45 | Episode 120 | Avg Reward 0.2\n",
      "Generation 45 | Episode 140 | Avg Reward 0.1\n",
      "Generation 45 | Episode 160 | Avg Reward 0.1\n",
      "Generation 45 | Episode 180 | Avg Reward 0.1\n",
      "Generation 45 | Episode 200 | Avg Reward 0.1\n",
      "Generation 45 Average Reward: 0.1\n",
      "Generation: 46\n",
      "Generation 46 | Episode 20 | Avg Reward 0.1\n",
      "Generation 46 | Episode 40 | Avg Reward 0.1\n",
      "Generation 46 | Episode 60 | Avg Reward 0.1\n",
      "Generation 46 | Episode 80 | Avg Reward 0.1\n",
      "Generation 46 | Episode 100 | Avg Reward 0.1\n",
      "Generation 46 | Episode 120 | Avg Reward 0.0\n",
      "Generation 46 | Episode 140 | Avg Reward 0.1\n",
      "Generation 46 | Episode 160 | Avg Reward 0.1\n",
      "Generation 46 | Episode 180 | Avg Reward 0.0\n",
      "Generation 46 | Episode 200 | Avg Reward 0.1\n",
      "Generation 46 Average Reward: 0.1\n",
      "Generation: 47\n",
      "Generation 47 | Episode 20 | Avg Reward 0.1\n",
      "Generation 47 | Episode 40 | Avg Reward 0.1\n",
      "Generation 47 | Episode 60 | Avg Reward 0.1\n",
      "Generation 47 | Episode 80 | Avg Reward 0.1\n",
      "Generation 47 | Episode 100 | Avg Reward 0.1\n",
      "Generation 47 | Episode 120 | Avg Reward 0.1\n",
      "Generation 47 | Episode 140 | Avg Reward 0.1\n",
      "Generation 47 | Episode 160 | Avg Reward 0.1\n",
      "Generation 47 | Episode 180 | Avg Reward 0.1\n",
      "Generation 47 | Episode 200 | Avg Reward 0.1\n",
      "Generation 47 Average Reward: 0.1\n",
      "Generation: 48\n",
      "Generation 48 | Episode 20 | Avg Reward 0.1\n",
      "Generation 48 | Episode 40 | Avg Reward 0.1\n",
      "Generation 48 | Episode 60 | Avg Reward 0.2\n",
      "Generation 48 | Episode 80 | Avg Reward 0.1\n",
      "Generation 48 | Episode 100 | Avg Reward 0.0\n",
      "Generation 48 | Episode 120 | Avg Reward 0.1\n",
      "Generation 48 | Episode 140 | Avg Reward 0.1\n",
      "Generation 48 | Episode 160 | Avg Reward 0.1\n",
      "Generation 48 | Episode 180 | Avg Reward 0.1\n",
      "Generation 48 | Episode 200 | Avg Reward 0.1\n",
      "Generation 48 Average Reward: 0.1\n",
      "Generation: 49\n",
      "Generation 49 | Episode 20 | Avg Reward 0.1\n",
      "Generation 49 | Episode 40 | Avg Reward 0.0\n",
      "Generation 49 | Episode 60 | Avg Reward 0.1\n",
      "Generation 49 | Episode 80 | Avg Reward 0.1\n",
      "Generation 49 | Episode 100 | Avg Reward 0.1\n",
      "Generation 49 | Episode 120 | Avg Reward 0.0\n",
      "Generation 49 | Episode 140 | Avg Reward 0.2\n",
      "Generation 49 | Episode 160 | Avg Reward 0.1\n",
      "Generation 49 | Episode 180 | Avg Reward 0.2\n",
      "Generation 49 | Episode 200 | Avg Reward 0.0\n",
      "Generation 49 Average Reward: 0.1\n",
      "Generation: 50\n",
      "Generation 50 | Episode 20 | Avg Reward 0.2\n",
      "Generation 50 | Episode 40 | Avg Reward 0.1\n",
      "Generation 50 | Episode 60 | Avg Reward 0.1\n",
      "Generation 50 | Episode 80 | Avg Reward 0.1\n",
      "Generation 50 | Episode 100 | Avg Reward 0.2\n",
      "Generation 50 | Episode 120 | Avg Reward 0.0\n",
      "Generation 50 | Episode 140 | Avg Reward 0.2\n",
      "Generation 50 | Episode 160 | Avg Reward 0.1\n",
      "Generation 50 | Episode 180 | Avg Reward 0.1\n",
      "Generation 50 | Episode 200 | Avg Reward 0.1\n",
      "Generation 50 Average Reward: 0.1\n",
      "Thread 0x170dc7000 may have been prematurely finalized\n",
      "[Physics::Module] Cleanup current backned.\n",
      "[Physics::Module] Id: 0xf2b8ea05\n",
      "Thread 0x16fb6f000 may have been prematurely finalized\n",
      "Thread 0x16fbfb000 may have been prematurely finalized\n",
      "Thread 0x16fd13000 may have been prematurely finalized\n",
      "Thread 0x16fc87000 may have been prematurely finalized\n"
     ]
    }
   ],
   "source": [
    "sns.set()\n",
    "\n",
    "class ActorCriticNN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,action_dim))\n",
    "\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1))\n",
    "\n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "\n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value\n",
    "\n",
    "#trainer\n",
    "class PPOTrainer():\n",
    "    def __init__(self, actor_critic, ppo_clip_val = 0.2, target_kl_div = 0.01, max_policy_train_iters = 80, value_train_iters=80, \n",
    "                policy_lr = 3e-4, value_lr = 1e-2, entropy_coeff = 0.1):\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_val = ppo_clip_val\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.value_train_iters = value_train_iters\n",
    "        self.entropy_coeff = entropy_coeff\n",
    "    \n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr = policy_lr)\n",
    "    \n",
    "        value_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr = value_lr)\n",
    "\n",
    "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "        \"\"\"\n",
    "        Train the policy using PPO with the given observations, actions, old log probabilities, and GAEs.\n",
    "        \"\"\"\n",
    "        # Forward pass through the policy network\n",
    "        new_logits = self.ac.policy(obs)  # Output from policy network\n",
    "        new_logits = Categorical(logits=new_logits)\n",
    "\n",
    "        acts = acts.squeeze(1)\n",
    "        \n",
    "        # Ensure actions are reshaped to 1D if necessary\n",
    "        acts = acts.reshape(-1)  # Flatten actions to a 1D tensor\n",
    "        \n",
    "        # Ensure batch sizes match\n",
    "        batch_size = new_logits.logits.size(0)\n",
    "        if acts.size(0) != batch_size:\n",
    "            #print(f\"Warning: Mismatch in batch size: logits size {batch_size}, actions size {acts.size(0)}\")\n",
    "            acts = acts[:batch_size]  # Slice actions to match logits batch size\n",
    "        \n",
    "        # Compute the log probabilities of the actions\n",
    "        new_log_probs = new_logits.log_prob(acts)  # This requires acts to be a 1D tensor, not 2D\n",
    "        \n",
    "        # Compute the policy ratio\n",
    "        policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        clipped_ratio = policy_ratio.clamp(1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = -torch.min(policy_ratio * gaes, clipped_ratio * gaes).mean()\n",
    "        \n",
    "        # Add entropy bonus to encourage exploration\n",
    "        #entropy_loss = -torch.mean(new_logits.entropy())\n",
    "        #loss = loss - self.entropy_coeff * entropy_loss\n",
    "\n",
    "        # Backpropagate and optimize (assuming optimizer is defined)\n",
    "        self.policy_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        kl_div = (old_log_probs - new_log_probs).mean()\n",
    "\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "\n",
    "        for _ in range (self.value_train_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "    \n",
    "            values = self.ac.value(obs)\n",
    "            value_loss = (returns-values) ** 2\n",
    "            value_loss = value_loss.mean()\n",
    "    \n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()\n",
    "\n",
    "def create_population(pop_size, state_dim, action_dim):\n",
    "    return [ActorCriticNN(state_dim, action_dim) for _ in range(pop_size)]\n",
    "\n",
    "\"\"\"\n",
    "When initializing PPOTrainer, you're passing the individual's hyperparameters (policy_lr, value_lr, etc.) \n",
    "to ensure the evaluation process reflects the effect of those hyperparameters.\n",
    "You're sampling actions from the model and accumulating rewards to calculate the fitness score.\n",
    "\"\"\"\n",
    "\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "\n",
    "def evaluate_population(population, env, behavior_name):\n",
    "    \"\"\"\n",
    "    Evaluate each individual in the population within the Unity environment.\n",
    "    \n",
    "    Parameters:\n",
    "        population (list): List of policies (or models) to evaluate.\n",
    "        env (UnityEnvironment): Unity environment object.\n",
    "        behavior_name (str): Name of the behavior being evaluated.\n",
    "        \n",
    "    Returns:\n",
    "        fitness_scores (list): List of fitness scores (rewards) for each individual.\n",
    "    \"\"\"\n",
    "    fitness_scores = []\n",
    "\n",
    "    for policy in population:\n",
    "        # Reset the environment and initialize reward for the episode\n",
    "        env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Get the current observation and decision steps\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "            if len(decision_steps) > 0:\n",
    "                # Extract observation data for the agent\n",
    "                obs = decision_steps.obs[0][0]  # Get the first observation from the agent (assuming single agent)\n",
    "\n",
    "                # Convert obs to a PyTorch tensor with dtype float32\n",
    "                obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "                # Ensure the tensor is correctly shaped for the model (batch dimension)\n",
    "                if len(obs_tensor.shape) == 1:  # if obs is 1D, add a batch dimension\n",
    "                    obs_tensor = obs_tensor.unsqueeze(0)\n",
    "\n",
    "                # Get the action from the policy model\n",
    "                logits, _ = policy(obs_tensor)  # Assuming 'policy' method returns both logits and value\n",
    "\n",
    "                # Convert logits into a categorical distribution and sample an action\n",
    "                action_dist = Categorical(logits=logits)\n",
    "                action = action_dist.sample().item()  # Get the action as a scalar\n",
    "\n",
    "                # Ensure action is a 2D array with shape (1, 2) for continuous action space\n",
    "                action = np.array([[action, action]])  # Example to make action a 2D array\n",
    "\n",
    "                # Convert the action to the correct Unity format\n",
    "                action_tuple = ActionTuple(continuous=action)\n",
    "\n",
    "                # Set the action and step the environment\n",
    "                env.set_actions(behavior_name, action_tuple)\n",
    "                env.step()\n",
    "\n",
    "            # Collect rewards from decision and terminal steps\n",
    "            for agent_id in decision_steps:\n",
    "                total_reward += decision_steps[agent_id].reward\n",
    "            \n",
    "            for agent_id in terminal_steps:\n",
    "                total_reward += terminal_steps[agent_id].reward\n",
    "                done = True  # End of episode if there are terminal steps\n",
    "\n",
    "        # Append the total reward as the fitness score for the policy\n",
    "        fitness_scores.append(total_reward)\n",
    "\n",
    "    return fitness_scores\n",
    "\n",
    "\n",
    "\n",
    "def select_parents(population, fitness_scores, num_parents):\n",
    "   # Get indices that would sort the fitness scores in descending order\n",
    "    sorted_indices = np.argsort(fitness_scores)[::-1]  # Reverse for descending order\n",
    "    \n",
    "    # Select the best parents based on the sorted indices\n",
    "    parents = np.array(population)[sorted_indices][:num_parents]\n",
    "    return parents.tolist()\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    child = ActorCriticNN(state_dim, action_dim)\n",
    "    \n",
    "    # Crossover the weights of the model in a way that keeps useful features\n",
    "    for param1, param2, param_child in zip(parent1.parameters(), parent2.parameters(), child.parameters()):\n",
    "        param_child.data = (param1.data + param2.data) / 2  # Averaging weights of the parents\n",
    "    \n",
    "    return child\n",
    "\n",
    "\n",
    "def mutate(individual, mutation_rate=0.01):\n",
    "    for param in individual.parameters():\n",
    "        if random.random() < mutation_rate:\n",
    "            param.data += torch.randn(param.size()) * 0.1  # Add small random noise\n",
    "\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Return discounted rewards based on the given rewards and the gamma param.\n",
    "    \"\"\"\n",
    "\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma = 0.99, decay = 0.97):\n",
    "    \"\"\"\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/abs/1506.02438\n",
    "    \"\"\"\n",
    "\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
    "\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "    return np.array(gaes[::-1])\n",
    "\n",
    "def unity_rollout(model, env, name, max_steps=1000, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Perform a rollout of the model in the Unity environment and collect rewards, \n",
    "    actions, values, and log probabilities.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The trained policy model.\n",
    "        env (UnityEnvironment): The Unity environment object.\n",
    "        name (str): The behavior name for the agent.\n",
    "        max_steps (int): Maximum number of steps in an episode.\n",
    "\n",
    "    Returns:\n",
    "        train_data (tuple): Collected training data (observations, actions, rewards, values, log_probs).\n",
    "        reward (float): Total reward for the episode.\n",
    "    \"\"\"\n",
    "    train_data = [[], [], [], [], []]  # To store observations, actions, rewards, values, log_probs\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    # Reset environment at the start\n",
    "    env.reset()\n",
    "\n",
    "    while not done and step < max_steps:\n",
    "        decision_steps, terminal_steps = env.get_steps(name)\n",
    "\n",
    "        if len(decision_steps) > 0:\n",
    "            # Get observation from the environment\n",
    "            obs = decision_steps.obs[0][0]  # Assuming single agent\n",
    "            \n",
    "            # Convert the observation to a tensor\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "            # Get the action and value from the policy model\n",
    "            action_logits, value = model(obs_tensor)\n",
    "\n",
    "            # Ensure the action is a 2D tensor (1, 2) for continuous action space\n",
    "            if action_logits.shape != (1, 2):\n",
    "                action_logits = action_logits.reshape(1, 2)  # Reshape to (1, 2) if necessary\n",
    "\n",
    "            # Create action distribution and sample an action\n",
    "            act_dist = Categorical(logits=action_logits)\n",
    "            action = act_dist.sample()\n",
    "            act_log_prob = act_dist.log_prob(action).item()\n",
    "\n",
    "            # Ensure action has the correct shape (1, 2) for continuous action\n",
    "            if action.shape == (1,):  # Check if action is a single value\n",
    "                action = action.unsqueeze(0)  # Reshape to (1, 2) if it's a scalar\n",
    "                action = action.expand(1, 2)  # Duplicate the value to match the shape (1, 2)\n",
    "\n",
    "            # Store the observation, action, reward, value, and log probability\n",
    "            train_data[0].append(obs)  # Observation\n",
    "            train_data[1].append(action.detach().numpy())  # Action\n",
    "            train_data[2].append(decision_steps[0].reward)  # Reward (for decision steps)\n",
    "            train_data[3].append(value.item())  # Value from the model\n",
    "            train_data[4].append(act_log_prob)  # Log probability of the action\n",
    "\n",
    "            # Set the action and step the environment\n",
    "            action_tuple = ActionTuple()\n",
    "            action_tuple.add_continuous(action.detach().numpy())  # Detach the tensor before converting to numpy\n",
    "            env.set_actions(name, action_tuple)\n",
    "            env.step()\n",
    "\n",
    "            # Collect rewards\n",
    "            for agent_id in decision_steps:\n",
    "                total_reward += decision_steps[agent_id].reward\n",
    "\n",
    "            # Check if the episode has ended (by terminal steps)\n",
    "            if len(terminal_steps) > 0:\n",
    "                for agent_id in terminal_steps:\n",
    "                    total_reward += terminal_steps[agent_id].reward\n",
    "                    # Store at the end of the episode\n",
    "                    train_data[0].append(obs)\n",
    "                    train_data[1].append(action.detach().numpy())\n",
    "                    train_data[2].append(terminal_steps[agent_id].reward)\n",
    "                    train_data[3].append(value.item())  # Final value prediction\n",
    "                    train_data[4].append(act_log_prob)  # Final log probability\n",
    "                done = True\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "\n",
    "    # Calculate GAEs using the stored rewards and values\n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3], gamma, lam)\n",
    "\n",
    "    return train_data, total_reward\n",
    "\n",
    "worker_id = 0\n",
    "show_visuals = False;\n",
    "while worker_id <= 64:\n",
    "    try:\n",
    "        env = UnityEnvironment(file_name=\"../Evo_Project\", worker_id=worker_id, no_graphics=not show_visuals)\n",
    "        env.reset()\n",
    "        break\n",
    "    except UnityWorkerInUseException:\n",
    "        worker_id += 1\n",
    "\n",
    "# Keep top N best individuals\n",
    "def elitism(population, fitness_scores, num_best=5):\n",
    "    sorted_indices = np.argsort(fitness_scores)[::-1]  # Descending order of fitness scores\n",
    "    best_individuals = [population[i] for i in sorted_indices[:num_best]]\n",
    "    return best_individuals\n",
    "\n",
    "def adaptive_mutation_rate(generation, max_generations, initial_rate=0.1, min_rate=0.01):\n",
    "    # Linearly decay mutation rate from initial_rate to min_rate over generations\n",
    "    rate = initial_rate - (initial_rate - min_rate) * (generation / max_generations)\n",
    "    return rate\n",
    "\n",
    "def evolutionary_process(population, fitness_scores, num_parents, pop_size):\n",
    "    \n",
    "    # Create next generation using selected parents\n",
    "    next_generation = []\n",
    "    for i in range(pop_size):\n",
    "        parent1, parent2 = np.random.choice(parents, 2, replace=False)  # Select parents from the elite\n",
    "        child = crossover(parent1, parent2)\n",
    "        mutate(child)  # Apply mutation\n",
    "        next_generation.append(child)\n",
    "\n",
    "    return next_generation\n",
    "\n",
    "behavior_name = list(env.behavior_specs.keys())[0]\n",
    "spec = env.behavior_specs[behavior_name]\n",
    "state_dim = sum([obs.shape[0] for obs in spec.observation_specs])  # Total observation dimensions\n",
    "action_spec = spec.action_spec\n",
    "action_dim = action_spec.discrete_size if action_spec.is_discrete() else action_spec.continuous_size\n",
    "\n",
    "# Define the environment and parameters\n",
    "# state_dim = env.observation_space.shape[0]\n",
    "# action_dim = env.action_space.n\n",
    "n_episodes = 200\n",
    "print_freq = 20\n",
    "pop_size = 50\n",
    "num_parents = 10\n",
    "num_generations = 50\n",
    "\n",
    "\n",
    "# Initialize the population\n",
    "population = create_population(pop_size, state_dim, action_dim)\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    \n",
    "        # Evaluate the current population\n",
    "        fitness_scores = evaluate_population(population, env, behavior_name)\n",
    "\n",
    "        # Elitism: select the top 5 individuals based on fitness\n",
    "        parents = elitism(population, fitness_scores, num_best=5)\n",
    "\n",
    "        # Select the best parents based on fitness scores\n",
    "        parents = select_parents(population, fitness_scores, num_parents)\n",
    "\n",
    "        # Apply the evolutionary process to generate the next generation\n",
    "        next_generation = evolutionary_process(parents, fitness_scores, num_parents, pop_size)\n",
    "\n",
    "        print(f\"Generation: {generation + 1}\")\n",
    "\n",
    "        # Fine-tune the best policy using PPO\n",
    "        best_policy = parents[0]  # Select the best policy for further training with PPO\n",
    "        ppo =  ppo = PPOTrainer(best_policy, policy_lr=3e-4, value_lr=1e-3, target_kl_div=0.02, max_policy_train_iters=80, value_train_iters=80)\n",
    "\n",
    "        ep_rewards = []\n",
    "        for episode_idx in range(n_episodes):\n",
    "            # Perform rollout\n",
    "            train_data, reward = unity_rollout(best_policy, env, behavior_name)\n",
    "            ep_rewards.append(reward)\n",
    "\n",
    "            # Permute the indices\n",
    "            permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "\n",
    "            # Convert the train_data[0] (observations) and train_data[1] (actions) to torch tensors\n",
    "            obs = torch.tensor(np.array(train_data[0])[permute_idxs], dtype=torch.float32)\n",
    "            act = torch.tensor(np.array(train_data[1])[permute_idxs], dtype=torch.float32)\n",
    "            gaes = torch.tensor(np.array(train_data[3])[permute_idxs], dtype=torch.float32)  # If using GAEs, ensure it's in train_data[3]\n",
    "\n",
    "            act_log_probs = torch.tensor(train_data[4][permute_idxs], dtype=torch.float32)\n",
    "\n",
    "            # Value Data\n",
    "            returns = discount_rewards(train_data[2])[permute_idxs]\n",
    "            returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "            # Train Policy\n",
    "            ppo.train_policy(obs, act, act_log_probs, gaes)\n",
    "            ppo.train_value(obs, returns)\n",
    "\n",
    "            # Print average reward every 'print_freq' episodes\n",
    "            if (episode_idx + 1) % print_freq == 0:\n",
    "                avg_reward = np.mean(ep_rewards[-print_freq:])  # Calculate the average of the last 'print_freq' rewards\n",
    "                print(f'Generation {generation + 1} | Episode {episode_idx + 1} | Avg Reward {avg_reward:.1f}')\n",
    "\n",
    "        # Calculate and print the overall average reward for this generation\n",
    "        generation_avg_reward = np.mean(ep_rewards)  # Calculate average for all episodes in this generation\n",
    "        print(f\"Generation {generation + 1} Average Reward: {generation_avg_reward:.1f}\")\n",
    "\n",
    "        # Set the population for the next generation\n",
    "        population = next_generation\n",
    "\n",
    "        # Reset the environment for the next generation\n",
    "        env.reset()\n",
    "\n",
    "    # After training finishes\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f39294-f0c4-4436-babd-bfc58e4c3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a Replay Buffer Below and modified hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6442d64-ebf1-48e8-8fed-da14b221a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "class ActorCriticNN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,action_dim))\n",
    "\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1))\n",
    "\n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "\n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value\n",
    "\n",
    "#trainer\n",
    "class PPOTrainer():\n",
    "    def __init__(self, actor_critic, ppo_clip_val, target_kl_div, policy_lr, value_lr, max_policy_train_iters = 80, \n",
    "                 value_train_iters = 80, entropy_coeff = 0.01):\n",
    "        self.entropy_coeff = entropy_coeff\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_val = ppo_clip_val\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.value_train_iters = value_train_iters\n",
    "    \n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr = policy_lr)\n",
    "    \n",
    "        value_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr = value_lr)\n",
    "\n",
    "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "        \"\"\"\n",
    "        Train the policy using PPO with the given observations, actions, old log probabilities, and GAEs.\n",
    "        \"\"\"\n",
    "        # Forward pass through the policy network\n",
    "        new_logits = self.ac.policy(obs)  # Output from policy network\n",
    "        new_logits = Categorical(logits=new_logits)\n",
    "\n",
    "        acts = acts.squeeze(1)\n",
    "        \n",
    "        # Ensure actions are reshaped to 1D if necessary\n",
    "        acts = acts.reshape(-1)  # Flatten actions to a 1D tensor\n",
    "        \n",
    "        # Ensure batch sizes match\n",
    "        batch_size = new_logits.logits.size(0)\n",
    "        if acts.size(0) != batch_size:\n",
    "            #print(f\"Warning: Mismatch in batch size: logits size {batch_size}, actions size {acts.size(0)}\")\n",
    "            acts = acts[:batch_size]  # Slice actions to match logits batch size\n",
    "        \n",
    "        # Compute the log probabilities of the actions\n",
    "        new_log_probs = new_logits.log_prob(acts)  # This requires acts to be a 1D tensor, not 2D\n",
    "        \n",
    "        # Compute the policy ratio\n",
    "        policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        clipped_ratio = policy_ratio.clamp(1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = -torch.min(policy_ratio * gaes, clipped_ratio * gaes).mean()\n",
    "        \n",
    "        # Add entropy bonus to encourage exploration\n",
    "        #entropy_loss = -torch.mean(new_logits.entropy())\n",
    "        #loss = loss - self.entropy_coeff * entropy_loss\n",
    "\n",
    "        # Backpropagate and optimize (assuming optimizer is defined)\n",
    "        self.policy_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        kl_div = (old_log_probs - new_log_probs).mean()\n",
    "\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "\n",
    "        for _ in range (self.value_train_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "    \n",
    "            values = self.ac.value(obs)\n",
    "            value_loss = (returns-values) ** 2\n",
    "            value_loss = value_loss.mean()\n",
    "    \n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()\n",
    "\n",
    "class ActorCriticChromosome:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        # Actor-Critic neural network\n",
    "        self.model = ActorCriticNN(state_dim, action_dim)\n",
    "\n",
    "        #Hyperparameters\n",
    "        self.policy_lr = random.uniform(1e-5, 1e-2)  # Expanded range\n",
    "        self.value_lr = random.uniform(1e-5, 1e-2)  # Expanded range\n",
    "        self.ppo_clip_val = random.uniform(0.1, 0.4)  # Expanded range\n",
    "        self.target_kl_div = random.uniform(0.001, 0.05)\n",
    "\n",
    "def create_population(pop_size, state_dim, action_dim):\n",
    "    return [ActorCriticChromosome(state_dim, action_dim) for _ in range(pop_size)]\n",
    "\n",
    "\"\"\"\n",
    "When initializing PPOTrainer, you're passing the individual's hyperparameters (policy_lr, value_lr, etc.) \n",
    "to ensure the evaluation process reflects the effect of those hyperparameters.\n",
    "You're sampling actions from the model and accumulating rewards to calculate the fitness score.\n",
    "\"\"\"\n",
    "\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "\n",
    "def evaluate_population(population, env, behavior_name):\n",
    "    \"\"\"\n",
    "    Evaluate each individual in the population within the Unity environment.\n",
    "    \n",
    "    Parameters:\n",
    "        population (list): List of policies (or models) to evaluate.\n",
    "        env (UnityEnvironment): Unity environment object.\n",
    "        behavior_name (str): Name of the behavior being evaluated.\n",
    "        \n",
    "    Returns:\n",
    "        fitness_scores (list): List of fitness scores (rewards) for each individual.\n",
    "    \"\"\"\n",
    "    fitness_scores = []\n",
    "\n",
    "    for policy in population:\n",
    "        # Reset the environment and initialize reward for the episode\n",
    "        env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Get the current observation and decision steps\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "            if len(decision_steps) > 0:\n",
    "                # Extract observation data for the agent\n",
    "                obs = decision_steps.obs[0][0]  # Get the first observation from the agent (assuming single agent)\n",
    "\n",
    "                # Convert obs to a PyTorch tensor with dtype float32\n",
    "                obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "                # Ensure the tensor is correctly shaped for the model (batch dimension)\n",
    "                if len(obs_tensor.shape) == 1:  # if obs is 1D, add a batch dimension\n",
    "                    obs_tensor = obs_tensor.unsqueeze(0)\n",
    "\n",
    "                # Get the action from the policy model\n",
    "                logits, _ = policy.model(obs_tensor)  # Assuming 'policy' method returns both logits and value\n",
    "\n",
    "                # Convert logits into a categorical distribution and sample an action\n",
    "                action_dist = Categorical(logits=logits)\n",
    "                action = action_dist.sample().item()  # Get the action as a scalar\n",
    "\n",
    "                # Ensure action is a 2D array with shape (1, 2) for continuous action space\n",
    "                action = np.array([[action, action]])  # Example to make action a 2D array\n",
    "\n",
    "                # Convert the action to the correct Unity format\n",
    "                action_tuple = ActionTuple(continuous=action)\n",
    "\n",
    "                # Set the action and step the environment\n",
    "                env.set_actions(behavior_name, action_tuple)\n",
    "                env.step()\n",
    "\n",
    "            # Collect rewards from decision and terminal steps\n",
    "            for agent_id in decision_steps:\n",
    "                total_reward += decision_steps[agent_id].reward\n",
    "            \n",
    "            for agent_id in terminal_steps:\n",
    "                total_reward += terminal_steps[agent_id].reward\n",
    "                done = True  # End of episode if there are terminal steps\n",
    "\n",
    "        # Append the total reward as the fitness score for the policy\n",
    "        fitness_scores.append(total_reward)\n",
    "\n",
    "    return fitness_scores\n",
    "\n",
    "\n",
    "\n",
    "def select_parents(population, fitness_scores, num_parents):\n",
    "   # Get indices that would sort the fitness scores in descending order\n",
    "    sorted_indices = np.argsort(fitness_scores)[::-1]  # Reverse for descending order\n",
    "    \n",
    "    # Select the best parents based on the sorted indices\n",
    "    parents = np.array(population)[sorted_indices][:num_parents]\n",
    "    return parents.tolist()\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    child = ActorCriticChromosome(state_dim, action_dim)\n",
    "    \n",
    "    # Crossover the weights of the model in a way that keeps useful features\n",
    "    for param1, param2, param_child in zip(parent1.model.parameters(), parent2.model.parameters(), child.model.parameters()):\n",
    "        param_child.data = (param1.data + param2.data) / 2  # Averaging weights of the parents\n",
    "\n",
    "    # Crossover the hyperparameters (keeping them in a range that allows for continued learning)\n",
    "    child.policy_lr = (parent1.policy_lr + parent2.policy_lr) / 2\n",
    "    child.value_lr = (parent1.value_lr + parent2.value_lr) / 2\n",
    "    child.ppo_clip_val = (parent1.ppo_clip_val + parent2.ppo_clip_val) / 2\n",
    "    child.target_kl_div = (parent1.target_kl_div + parent2.target_kl_div) / 2\n",
    "    \n",
    "    return child\n",
    "\n",
    "\n",
    "def mutate(individual, mutation_rate=0.01):\n",
    "    for param in individual.model.parameters():\n",
    "        if random.random() < mutation_rate:\n",
    "            param.data += torch.randn(param.size()) * 0.1  # Add small random noise\n",
    "    \n",
    "    # Mutate hyperparameters with greater changes\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.policy_lr += random.uniform(-5e-4, 5e-4)  # Much larger mutation range\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.value_lr += random.uniform(-5e-4, 5e-4)  # Increased mutation range\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.ppo_clip_val += random.uniform(-0.1, 0.1)  # Increased mutation range\n",
    "    if random.random() < mutation_rate:\n",
    "        individual.target_kl_div += random.uniform(-0.01, 0.01)  # Increased mutation range\n",
    "\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Return discounted rewards based on the given rewards and the gamma param.\n",
    "    \"\"\"\n",
    "\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma = 0.99, decay = 0.97):\n",
    "    \"\"\"\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/abs/1506.02438\n",
    "    \"\"\"\n",
    "\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
    "\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "    return np.array(gaes[::-1])\n",
    "\n",
    "def unity_rollout(model, env, name, max_steps=1000, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Perform a rollout of the model in the Unity environment and collect rewards, \n",
    "    actions, values, and log probabilities.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The trained policy model.\n",
    "        env (UnityEnvironment): The Unity environment object.\n",
    "        name (str): The behavior name for the agent.\n",
    "        max_steps (int): Maximum number of steps in an episode.\n",
    "\n",
    "    Returns:\n",
    "        train_data (tuple): Collected training data (observations, actions, rewards, values, log_probs).\n",
    "        reward (float): Total reward for the episode.\n",
    "    \"\"\"\n",
    "    train_data = [[], [], [], [], []]  # To store observations, actions, rewards, values, log_probs\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    # Reset environment at the start\n",
    "    env.reset()\n",
    "\n",
    "    while not done and step < max_steps:\n",
    "        decision_steps, terminal_steps = env.get_steps(name)\n",
    "\n",
    "        if len(decision_steps) > 0:\n",
    "            # Get observation from the environment\n",
    "            obs = decision_steps.obs[0][0]  # Assuming single agent\n",
    "            \n",
    "            # Convert the observation to a tensor\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "            # Get the action and value from the policy model\n",
    "            action_logits, value = model(obs_tensor)\n",
    "\n",
    "            # Ensure the action is a 2D tensor (1, 2) for continuous action space\n",
    "            if action_logits.shape != (1, 2):\n",
    "                action_logits = action_logits.reshape(1, 2)  # Reshape to (1, 2) if necessary\n",
    "\n",
    "            # Create action distribution and sample an action\n",
    "            act_dist = Categorical(logits=action_logits)\n",
    "            action = act_dist.sample()\n",
    "            act_log_prob = act_dist.log_prob(action).item()\n",
    "\n",
    "            # Ensure action has the correct shape (1, 2) for continuous action\n",
    "            if action.shape == (1,):  # Check if action is a single value\n",
    "                action = action.unsqueeze(0)  # Reshape to (1, 2) if it's a scalar\n",
    "                action = action.expand(1, 2)  # Duplicate the value to match the shape (1, 2)\n",
    "\n",
    "            # Store the observation, action, reward, value, and log probability\n",
    "            train_data[0].append(obs)  # Observation\n",
    "            train_data[1].append(action.detach().numpy())  # Action\n",
    "            train_data[2].append(decision_steps[0].reward)  # Reward (for decision steps)\n",
    "            train_data[3].append(value.item())  # Value from the model\n",
    "            train_data[4].append(act_log_prob)  # Log probability of the action\n",
    "\n",
    "            # Set the action and step the environment\n",
    "            action_tuple = ActionTuple()\n",
    "            action_tuple.add_continuous(action.detach().numpy())  # Detach the tensor before converting to numpy\n",
    "            env.set_actions(name, action_tuple)\n",
    "            env.step()\n",
    "\n",
    "            # Collect rewards\n",
    "            for agent_id in decision_steps:\n",
    "                total_reward += decision_steps[agent_id].reward\n",
    "\n",
    "            # Check if the episode has ended (by terminal steps)\n",
    "            if len(terminal_steps) > 0:\n",
    "                for agent_id in terminal_steps:\n",
    "                    total_reward += terminal_steps[agent_id].reward\n",
    "                    # Store at the end of the episode\n",
    "                    train_data[0].append(obs)\n",
    "                    train_data[1].append(action.detach().numpy())\n",
    "                    train_data[2].append(terminal_steps[agent_id].reward)\n",
    "                    train_data[3].append(value.item())  # Final value prediction\n",
    "                    train_data[4].append(act_log_prob)  # Final log probability\n",
    "                done = True\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "\n",
    "    # Calculate GAEs using the stored rewards and values\n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3], gamma, lam)\n",
    "\n",
    "    return train_data, total_reward\n",
    "\n",
    "worker_id = 0\n",
    "show_visuals = False;\n",
    "while worker_id <= 64:\n",
    "    try:\n",
    "        env = UnityEnvironment(file_name=\"../Evo_Project\", worker_id=worker_id, no_graphics=not show_visuals)\n",
    "        env.reset()\n",
    "        break\n",
    "    except UnityWorkerInUseException:\n",
    "        worker_id += 1\n",
    "\n",
    "# Keep top N best individuals\n",
    "def elitism(population, fitness_scores, num_best=5):\n",
    "    sorted_indices = np.argsort(fitness_scores)[::-1]  # Descending order of fitness scores\n",
    "    best_individuals = [population[i] for i in sorted_indices[:num_best]]\n",
    "    return best_individuals\n",
    "\n",
    "def adaptive_mutation_rate(generation, max_generations, initial_rate=0.1, min_rate=0.01):\n",
    "    # Linearly decay mutation rate from initial_rate to min_rate over generations\n",
    "    rate = initial_rate - (initial_rate - min_rate) * (generation / max_generations)\n",
    "    return rate\n",
    "\n",
    "def evolutionary_process(population, fitness_scores, num_parents, pop_size):\n",
    "    \n",
    "    # Create next generation using selected parents\n",
    "    next_generation = []\n",
    "    for i in range(pop_size):\n",
    "        parent1, parent2 = np.random.choice(parents, 2, replace=False)  # Select parents from the elite\n",
    "        child = crossover(parent1, parent2)\n",
    "        mutate(child)  # Apply mutation\n",
    "        next_generation.append(child)\n",
    "\n",
    "    return next_generation\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, num_steps=1, gamma=0.99):\n",
    "        self.capacity = int(capacity)\n",
    "        self.num_steps = int(num_steps)\n",
    "        self.gamma = gamma\n",
    "        self.buffer = deque(maxlen=self.capacity)\n",
    "        self.n_step_buffer = deque(maxlen=self.num_steps)\n",
    "\n",
    "    def add(self, transition):\n",
    "        assert len(transition) == 6  # (state, action, reward, next_state, terminated, truncated)\n",
    "\n",
    "        if self.num_steps == 1:\n",
    "            state, action, reward, next_state, terminated, truncated = transition\n",
    "            self.buffer.append((state, action, reward, next_state, terminated))\n",
    "        else:\n",
    "            self.n_step_buffer.append(transition)\n",
    "            _, _, _, final_state, final_terminated, final_truncated = transition\n",
    "            n_step_reward = 0\n",
    "\n",
    "            for _, _, reward, _, _, _ in reversed(self.n_step_buffer):\n",
    "                n_step_reward = (n_step_reward * self.gamma) + reward\n",
    "            state, action, _, _, _, _ = self.n_step_buffer[0]\n",
    "\n",
    "            if len(self.n_step_buffer) == self.num_steps:\n",
    "                self.buffer.append((state, action, n_step_reward, final_state, final_terminated))\n",
    "\n",
    "            if final_terminated or final_truncated:\n",
    "                self.n_step_buffer.clear()\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "behavior_name = list(env.behavior_specs.keys())[0]\n",
    "spec = env.behavior_specs[behavior_name]\n",
    "state_dim = sum([obs.shape[0] for obs in spec.observation_specs])  # Total observation dimensions\n",
    "action_spec = spec.action_spec\n",
    "action_dim = action_spec.discrete_size if action_spec.is_discrete() else action_spec.continuous_size\n",
    "\n",
    "# Define the environment and parameters\n",
    "# state_dim = env.observation_space.shape[0]\n",
    "# action_dim = env.action_space.n\n",
    "n_episodes = 200\n",
    "print_freq = 20\n",
    "pop_size = 50\n",
    "num_parents = 10\n",
    "num_generations = 50\n",
    "\n",
    "replay_buffer = ReplayBuffer(capacity=10000, num_steps=3, gamma=0.99)\n",
    "\n",
    "\n",
    "# Initialize the population\n",
    "population = create_population(pop_size, state_dim, action_dim)\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    \n",
    "        # Evaluate the current population\n",
    "        fitness_scores = evaluate_population(population, env, behavior_name)\n",
    "\n",
    "        # Elitism: select the top 5 individuals based on fitness\n",
    "        parents = elitism(population, fitness_scores, num_best=5)\n",
    "\n",
    "        # Select the best parents based on fitness scores\n",
    "        parents = select_parents(population, fitness_scores, num_parents)\n",
    "\n",
    "        # Apply the evolutionary process to generate the next generation\n",
    "        next_generation = evolutionary_process(parents, fitness_scores, num_parents, pop_size)\n",
    "\n",
    "        print(f\"Generation: {generation + 1}\")\n",
    "\n",
    "        # Fine-tune the best policy using PPO\n",
    "        best_policy = parents[0]  # Select the best policy for further training with PPO\n",
    "        ppo = PPOTrainer(best_policy.model, ppo_clip_val=best_policy.ppo_clip_val, target_kl_div=best_policy.target_kl_div,\n",
    "                         policy_lr=best_policy.policy_lr, value_lr=best_policy.value_lr)\n",
    "\n",
    "        ep_rewards = []\n",
    "        for episode_idx in range(n_episodes):\n",
    "            # Perform rollout\n",
    "            train_data, reward = unity_rollout(best_policy.model, env, behavior_name)\n",
    "            ep_rewards.append(reward)\n",
    "\n",
    "            for t in range(len(train_data[0])):\n",
    "                state = train_data[0][t]\n",
    "                action = train_data[1][t]\n",
    "                reward = train_data[2][t]\n",
    "                next_state = train_data[3][t]\n",
    "                done = train_data[4][t]\n",
    "                replay_buffer.add((state, action, reward, next_state, done, False))\n",
    "\n",
    "            ep_rewards.append(reward)\n",
    "\n",
    "            # Sample a batch from the buffer for training\n",
    "            if len(replay_buffer) > 64:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(64)\n",
    "\n",
    "                # Convert data to torch tensors\n",
    "                states = torch.tensor(states, dtype=torch.float32)\n",
    "                actions = torch.tensor(actions, dtype=torch.float32)\n",
    "                rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "                next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "                dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "                # Calculate returns and advantages (if needed)\n",
    "                returns = discount_rewards(rewards)\n",
    "                gaes = compute_gaes(rewards, next_states, dones)\n",
    "\n",
    "                # Train PPO with the sampled batch\n",
    "                ppo.train_policy(states, actions, gaes)\n",
    "                ppo.train_value(states, returns)\n",
    "\n",
    "            # Print average reward every 'print_freq' episodes\n",
    "            if (episode_idx + 1) % print_freq == 0:\n",
    "                avg_reward = np.mean(ep_rewards[-print_freq:])\n",
    "                print(f'Generation {generation + 1} | Episode {episode_idx + 1} | Avg Reward {avg_reward:.1f}')\n",
    "\n",
    "        # Calculate overall average reward for the generation\n",
    "        generation_avg_reward = np.mean(ep_rewards)\n",
    "        print(f\"Generation {generation + 1} Average Reward: {generation_avg_reward:.1f}\")\n",
    "\n",
    "        # Set population for next generation\n",
    "        population = next_generation\n",
    "\n",
    "        # Reset environment for the next generation\n",
    "        env.reset()\n",
    "\n",
    "    # After training finishes\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
